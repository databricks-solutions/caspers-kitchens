{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Choose the Best Model for Your Agent\n",
    "\n",
    "Databricks provides native access to a wide range of major AI model families, including ChatGPT, Claude, Gemini, Llama, and more. MLflow on Databricks supports sophisticated model evaluation and comparison workflows, including trace-aware, human-aligned agentic evaluation.\n",
    "\n",
    "In this notebook, we will explore some of the ways Databricks and MLflow empower you to rigorously iterate on the quality of your agent and to choose the best model for your use case. In particular, we will show how to use a trace-aware agentic judge and a human-aligned template-based judge to evaluate and compare different models.\n",
    "\n",
    "**Scenario:** You've built a prototype complaint triage agent for Casper's Kitchens, a ghost kitchen network. You want to know which AI model should power your production agent.\n",
    "\n",
    "**What you'll do:**\n",
    "1. Define a prototype agent for customer complaint triage\n",
    "2. Create an evaluation dataset with diverse complaint scenarios\n",
    "3. Evaluate different models using agentic- and template-based scorers\n",
    "4. Review results and add human feedback via MLflow UI\n",
    "5. Align the judge with human feedback to improve accuracy\n",
    "6. Re-evaluate and visualize results\n",
    "7. Register the winning model configuration\n",
    "\n",
    "**Prerequisites:** This demo assumes UC tools have been created by running `stages/complaint_agent.ipynb` and that you have run `stages/raw_data.ipynb` and `stages/lakeflow.ipynb` to start the raw data stream. See the [README](../../../README.md) for details on setting up the Casper's environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "## 1. Setup & Prerequisites\n",
    "\n",
    "Install required packages and verify that UC tools exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow[databricks] langgraph==0.3.4 databricks-langchain plotly dspy\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "if not CATALOG:\n",
    "    raise ValueError(\"Please provide a CATALOG name in the widget above\")\n",
    "print(f\"Using catalog: {CATALOG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify UC tools exist\n",
    "print(\"Checking for required UC functions...\")\n",
    "\n",
    "required_functions = ['get_order_overview', 'get_order_timing', 'get_location_timings']\n",
    "\n",
    "for func_name in required_functions:\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE FUNCTION {CATALOG}.ai.{func_name}\").collect()\n",
    "    except Exception:\n",
    "        raise RuntimeError(\"UC tools missing. Run ../../stages/complaint_agent.ipynb first to create these tools.\")\n",
    "\n",
    "print(\"\\n\u2705 All required UC functions found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow experiment\n",
    "\n",
    "import mlflow\n",
    "\n",
    "experiment_name = f\"/Shared/{CATALOG}_model_comparison\"\n",
    "experiment = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment.experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "## 2. Define Agent\n",
    "\n",
    "The cell below defines a version of the complaint agent used in the [complaint_agent.ipynb](../../stages/complaint_agent.ipynb) notebook. The agent can call on a set of Unity Catalog functions enabling it to retrieve order details, delivery timing, and location timings.\n",
    "\n",
    "In this scenario, imagine that we are earlier in the prototyping phase and we want to quickly iterate on the agent's design. We are making fundamental design choices like which models to use and how to structure the agent's system prompt.\n",
    "\n",
    "This version of the complaint agent takes `model_endpoint` as a parameter, enabling us to easily swap between different models for comparison. Beyond that, the specific implementation details are largely unimportant for the purposes of this demo: you can use the same general evaluation and comparison principles regardless of the underlying agent implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-agent-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Annotated, Any, Optional, Sequence, TypedDict, Literal, cast\n",
    "from uuid import uuid4\n",
    "\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "# Set up UC function client\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "# Define response schema (same as production)\n",
    "class ComplaintResponse(TypedDict):\n",
    "    order_id: str\n",
    "    complaint_category: Literal[\"delivery_delay\", \"missing_items\", \"food_quality\", \"service_issue\", \"billing\", \"other\"]\n",
    "    decision: Literal[\"suggest_credit\", \"escalate\"]\n",
    "    credit_amount: Optional[float]\n",
    "    confidence: Optional[Literal[\"high\", \"medium\", \"low\"]]\n",
    "    priority: Optional[Literal[\"standard\", \"urgent\"]]\n",
    "    rationale: str\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# System prompt (same as production)\n",
    "SYSTEM_PROMPT = \"\"\"You are the Complaint Triage Agent for Casper's Kitchens, a ghost kitchen network. Your job is to analyze customer complaints and recommend actions for internal customer service staff.\n",
    "\n",
    "PROCESS:\n",
    "1. Extract the order_id from the complaint text\n",
    "2. Call get_order_overview(order_id) to get order details and items\n",
    "3. Call get_order_timing(order_id) to get delivery timing\n",
    "4. For delivery delay complaints, call get_location_timings(location) to get percentile benchmarks\n",
    "5. Analyze the data and make a recommendation\n",
    "\n",
    "DECISION FRAMEWORK:\n",
    "\n",
    "SUGGEST_CREDIT - Use when you can make a data-backed recommendation:\n",
    "\n",
    "Delivery delays (high confidence when data-backed):\n",
    "- Compare actual delivery time to location percentiles\n",
    "- >P75 but <P90: Suggest 15% of order total\n",
    "- >P90 but <P99: Suggest 25% of order total  \n",
    "- >P99: Suggest 50% of order total\n",
    "- On-time delivery: Suggest $0 credit (low confidence - might be other issues)\n",
    "\n",
    "Missing items:\n",
    "- Parse items_json from get_order_overview to find actual item prices\n",
    "- Use real item costs when available for accurate refund\n",
    "- Check if claimed missing item actually makes sense for this order\n",
    "- If item plausibly missing but can't verify price: estimate $8-12 per item (medium confidence)\n",
    "- If no evidence of missing items: $0 credit (low confidence)\n",
    "\n",
    "Food quality issues:\n",
    "- If specific details provided (cold, soggy, wrong preparation): $10-15 (medium confidence)\n",
    "- If vague (\"bad\", \"gross\"): escalate with priority=\"standard\" instead\n",
    "- If food quality complaint but order shows delivery delay >P90: consider combined credit\n",
    "\n",
    "ESCALATE - Use when human judgment is needed:\n",
    "- priority=\"standard\": Vague complaints, missing data, edge cases, billing issues, service complaints\n",
    "- priority=\"urgent\": Legal threats, health/safety concerns, suspected fraud, abusive language\n",
    "\n",
    "OUTPUT RULES:\n",
    "- For suggest_credit: Include credit_amount (can be $0) and confidence, set priority to null\n",
    "- For escalate: Include priority, set credit_amount and confidence to null\n",
    "- Always include a data-focused rationale citing specific numbers (delivery times, percentiles, order details, item verification)\n",
    "- Rationale is for internal staff - be factual, not apologetic\n",
    "\n",
    "RATIONALE GUIDELINES:\n",
    "- Rationale should clearly articulate the justification for the decision\n",
    "- All evidence used to support the decision should be cited in the rationale. Rationale MUST cite evidence.\n",
    "- Rationale should be detailed and specific, but no longer than 150 words\n",
    "- Rationale should clearly justify the decision, credit amount (if applicable), confidence level, and priority (if applicable)\n",
    "- If a refund is suggested, the rationale should clearly justify the credit amount, based on the evidence provided.\n",
    "\n",
    "CONFIDENCE GUIDELINES:\n",
    "- high: Strong data support (delivery >P90, item prices from order data, clear verification)\n",
    "- medium: Reasonable inference (delivery P75-P90, estimated item costs, plausible but unverified)\n",
    "- low: Weak or contradictory evidence ($0 credits when data doesn't support claim, can't verify complaint details)\n",
    "\n",
    "IMPORTANT NOTES:\n",
    "- If order_id not found or data unavailable: escalate with priority=\"standard\"\n",
    "- Round credit amounts to nearest $0.50\n",
    "- When in doubt between two options, prefer escalate with priority=\"standard\"\n",
    "- A suggest_credit with $0 and low confidence is valid when complaint seems unfounded\n",
    "\"\"\"\n",
    "\n",
    "RESPONSE_FIELDS = {\"order_id\", \"complaint_category\", \"decision\", \"rationale\"}\n",
    "\n",
    "def parse_structured_response(obj) -> ComplaintResponse:\n",
    "    \"\"\"Parse ComplaintResponse from AIMessage or dict\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        candidate = obj\n",
    "    else:\n",
    "        parsed = obj.additional_kwargs.get(\"parsed_structured_output\")\n",
    "        if isinstance(parsed, dict):\n",
    "            candidate = parsed\n",
    "        else:\n",
    "            content = obj.content\n",
    "            if isinstance(content, str):\n",
    "                raw = content\n",
    "            elif isinstance(content, list):\n",
    "                raw = \"\".join(part.get(\"text\", \"\") if isinstance(part, dict) else str(part) for part in content)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported message content type\")\n",
    "            candidate = json.loads(raw)\n",
    "\n",
    "    missing = RESPONSE_FIELDS.difference(candidate.keys())\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required fields: {sorted(missing)}\")\n",
    "\n",
    "    decision = candidate.get(\"decision\")\n",
    "    if decision == \"suggest_credit\":\n",
    "        if candidate.get(\"credit_amount\") is None:\n",
    "            raise ValueError(\"suggest_credit requires credit_amount\")\n",
    "        if candidate.get(\"confidence\") is None:\n",
    "            raise ValueError(\"suggest_credit requires confidence\")\n",
    "        if candidate.get(\"priority\") is not None:\n",
    "            candidate[\"priority\"] = None\n",
    "    elif decision == \"escalate\":\n",
    "        if candidate.get(\"priority\") is None:\n",
    "            raise ValueError(\"escalate requires priority\")\n",
    "        if candidate.get(\"credit_amount\") is not None:\n",
    "            candidate[\"credit_amount\"] = None\n",
    "        if candidate.get(\"confidence\") is not None:\n",
    "            candidate[\"confidence\"] = None\n",
    "\n",
    "    return cast(ComplaintResponse, candidate)\n",
    "\n",
    "\n",
    "class ComplaintsAgentCore:\n",
    "    \"\"\"Lightweight complaint agent for model comparison (no ResponsesAgent wrapper)\"\"\"\n",
    "    \n",
    "    def __init__(self, model_endpoint: str, catalog: str):\n",
    "        self.model_endpoint = model_endpoint\n",
    "        self.catalog = catalog\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = ChatDatabricks(endpoint=model_endpoint)\n",
    "        \n",
    "        # Load UC tools\n",
    "        uc_tool_names = [\n",
    "            f\"{catalog}.ai.get_order_overview\",\n",
    "            f\"{catalog}.ai.get_order_timing\",\n",
    "            f\"{catalog}.ai.get_location_timings\",\n",
    "        ]\n",
    "        uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "        self.tools = uc_toolkit.tools\n",
    "        \n",
    "        # Build agent\n",
    "        self.agent = self._create_agent()\n",
    "    \n",
    "    def _create_agent(self):\n",
    "        \"\"\"Create LangGraph workflow\"\"\"\n",
    "        tool_model = self.llm.bind_tools(self.tools, tool_choice=\"auto\")\n",
    "        structured_model = tool_model.with_structured_output(ComplaintResponse)\n",
    "\n",
    "        def should_continue(state: AgentState):\n",
    "            messages = state[\"messages\"]\n",
    "            last_message = messages[-1]\n",
    "            if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "                return \"continue\"\n",
    "            return \"end\"\n",
    "\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] + state[\"messages\"]\n",
    "        )\n",
    "\n",
    "        tool_runnable = preprocessor | tool_model\n",
    "        structured_runnable = preprocessor | structured_model\n",
    "\n",
    "        def call_model(state: AgentState, config: RunnableConfig):\n",
    "            response = tool_runnable.invoke(state, config)\n",
    "            if not isinstance(response, AIMessage):\n",
    "                raise ValueError(f\"Expected AIMessage, received {type(response)}\")\n",
    "\n",
    "            if response.tool_calls:\n",
    "                return {\"messages\": [response]}\n",
    "\n",
    "            try:\n",
    "                parsed = parse_structured_response(response)\n",
    "            except (json.JSONDecodeError, ValueError):\n",
    "                structured = structured_runnable.invoke(state, config)\n",
    "                parsed = parse_structured_response(structured)\n",
    "\n",
    "            structured_message = AIMessage(\n",
    "                id=response.id or str(uuid4()),\n",
    "                content=json.dumps(parsed),\n",
    "                additional_kwargs={\"parsed_structured_output\": parsed},\n",
    "            )\n",
    "            return {\"messages\": [structured_message]}\n",
    "\n",
    "        workflow = StateGraph(AgentState)\n",
    "        workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "        workflow.add_node(\"tools\", ToolNode(self.tools))\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"agent\",\n",
    "            should_continue,\n",
    "            {\"continue\": \"tools\", \"end\": END},\n",
    "        )\n",
    "        workflow.add_edge(\"tools\", \"agent\")\n",
    "        return workflow.compile()\n",
    "    \n",
    "    def invoke(self, complaint: str) -> dict:\n",
    "        \"\"\"Process a complaint and return structured response\"\"\"\n",
    "        result = self.agent.invoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": complaint}]\n",
    "        })\n",
    "        \n",
    "        # Extract final response\n",
    "        final_message = result[\"messages\"][-1]\n",
    "        return parse_structured_response(final_message)\n",
    "\n",
    "print(\"\u2705 ComplaintsAgentCore class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-agent-header",
   "metadata": {},
   "source": [
    "### Test the Agent\n",
    "\n",
    "Before creating the full evaluation dataset, let's validate that the agent works correctly with a single test case. We will:\n",
    "1. Retrieve a real order ID from the `all_events` table\n",
    "2. Query the agent with a delivery delay complaint and the order ID\n",
    "\n",
    "We will also enable MLflow autologging so we can review the agent's execution trace in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample order ID for testing\n",
    "sample_order = spark.sql(f\"\"\"\n",
    "    SELECT order_id \n",
    "    FROM {CATALOG}.lakeflow.all_events \n",
    "    WHERE event_type='delivered'\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0]['order_id']\n",
    "\n",
    "print(f\"Using test order ID: {sample_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Enable autologging for LangChain\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Create agent instance with a test model\n",
    "test_agent = ComplaintsAgentCore(\n",
    "    model_endpoint=\"databricks-gpt-5\",\n",
    "    catalog=CATALOG\n",
    ")\n",
    "\n",
    "# Test with a delivery delay complaint\n",
    "test_complaint = f\"My order took forever to arrive! Order ID: {sample_order}\"\n",
    "\n",
    "result = test_agent.invoke(test_complaint)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-agent-output",
   "metadata": {},
   "source": [
    "The agent successfully analyzed the complaint and returned a structured response. For this particular delivery delay complaint, the agent:\n",
    "- Categorized it as `delivery_delay`\n",
    "- Suggested a credit of $1.00 (15% of order total, rounded)\n",
    "- Assigned `medium` confidence\n",
    "- Provided detailed rationale citing specific delivery time (31.38 min), location percentiles (P50: 26.13, P75: 31.05), and calculation logic\n",
    "\n",
    "This demonstrates the agent's ability to retrieve order data, compare against benchmarks, and make data-driven credit recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "## 3. Create Evaluation Dataset\n",
    "\n",
    "Now we will generate 15 diverse complaint scenarios to thoroughly test model performance across different situations:\n",
    "- Delivery delays (with varying severity)\n",
    "- Missing items (verifiable vs suspicious)\n",
    "- Food quality complaints (specific vs vague)\n",
    "- Service issues (should escalate)\n",
    "- Health/safety concerns (urgent escalation)\n",
    "- Edge cases (missing order ID, invalid format)\n",
    "\n",
    "A diverse dataset helps reveal model strengths and weaknesses across the decision space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-sample-orders",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get real order IDs for realistic eval scenarios\n",
    "all_order_ids = [\n",
    "    row['order_id'] for row in spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT order_id \n",
    "        FROM {CATALOG}.lakeflow.all_events \n",
    "        WHERE event_type='delivered'\n",
    "        LIMIT 15\n",
    "    \"\"\").collect()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-eval-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Create 15 diverse complaint scenarios (exactly one per order ID)\n",
    "templates = [\n",
    "    \"My order took forever to arrive! Order ID: {oid}\",\n",
    "    \"Order {oid} arrived late and cold\",\n",
    "    \"My falafel was completely soggy and inedible. Order: {oid}\",\n",
    "    \"The gyro meat was overcooked and dry. Order: {oid}\",\n",
    "    \"Everything tasted bad. Order {oid}\",\n",
    "    \"My entire falafel bowl was missing from the order! Order: {oid}\",\n",
    "    \"No drinks in my order {oid}\",\n",
    "    \"Your driver was extremely rude to me. Order: {oid}\",\n",
    "    \"This food made me sick, possible food poisoning. Order: {oid}\",\n",
    "    \"Order {oid} was late AND missing items AND cold!\",\n",
    "    \"The packaging was torn open when it arrived. Order: {oid}\",\n",
    "    \"The fries were completely soggy by the time they got here. Order: {oid}\",\n",
    "    \"I received someone else\u2019s order by mistake. Order: {oid}\",\n",
    "    \"The sauce containers leaked all over the bag. Order: {oid}\",\n",
    "    \"The order was marked delivered but never showed up. Order: {oid}\",\n",
    "]\n",
    "\n",
    "# Map 1:1 (use each of the 15 IDs exactly once)\n",
    "complaints = [tmpl.format(oid=all_order_ids[i]) for i, tmpl in enumerate(templates)]\n",
    "\n",
    "# Format for mlflow.genai.evaluate\n",
    "eval_data = [{\"inputs\": {\"complaint\": c}} for c in complaints]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "## 4. Define [MLflow Scorers](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/)\n",
    "\n",
    "We use two complementary LLM judges with no overlap:\n",
    "\n",
    "1. **Evidence Groundedness (Agent-as-a-Judge with `{{ trace }}`)**: Inspects execution traces to verify decisions align with tool outputs. Uses MCP tools (GetSpan, ListSpans, etc.) to examine what data was actually returned by tool calls and whether the agent's rationale matches that evidence. Shows off agentic evaluation capabilities. **Not aligned** (alignment not yet supported for trace-aware judges).\n",
    "\n",
    "2. **Rationale Sufficiency (Template-based Judge)**: Evaluates if a human could understand the decision logic from the rationale alone. Uses `{{ inputs }}` and `{{ outputs }}` templates to assess clarity, completeness, and logical flow. **Will be aligned** with human feedback using SIMBA optimization.\n",
    "\n",
    "Both of these scorers use the `mlflow.genai.judges.make_judge` function to create [template-based scorers](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/make-judge/). There are several other ways to define scorers in MLflow, including a variety of [pre-defined scorers](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/predefined/), [guideline-based LLM scorers](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/guidelines/), and [code-based scorers](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/custom/).\n",
    "\n",
    "Note that, just like we can use different models for our agent, we can also use different models for our scorers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-scorers",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.judges import make_judge\n",
    "from mlflow.genai.scorers import Guidelines\n",
    "\n",
    "# Scorer 1: Evidence Groundedness (Agent-as-a-Judge, Binary Pass/Fail)\n",
    "# Uses {{ trace }} to enable agentic evaluation with tool access\n",
    "evidence_groundedness_judge = make_judge(\n",
    "    name=\"evidence_groundedness\",\n",
    "    instructions=\"\"\"\n",
    "Evaluate whether the agent's decision is grounded in evidence from the execution {{ trace }}.\n",
    "\n",
    "Investigation checklist:\n",
    "1. Find spans where tools were called (get_order_overview, get_order_timing, get_location_timings)\n",
    "2. Extract the actual outputs returned by these tool calls\n",
    "3. Compare tool outputs against claims made in the agent's rationale\n",
    "4. Verify that credit amounts or escalation decisions match the tool data\n",
    "\n",
    "For delivery complaints, check:\n",
    "- Does the rationale's delivery time match what get_order_timing returned?\n",
    "- Does the rationale's percentile comparison match get_location_timings output?\n",
    "- Is the credit calculation based on the actual order total from get_order_overview?\n",
    "\n",
    "For missing item complaints, check:\n",
    "- Does get_order_overview show the items mentioned in the rationale?\n",
    "- Is the credit amount based on actual item prices from the tool output?\n",
    "\n",
    "Customer complaint: {{ inputs }}\n",
    "Agent's final output: {{ outputs }}\n",
    "\n",
    "Rate as PASS if rationale claims match tool outputs, FAIL if there are contradictions or unsupported claims.\n",
    "\"\"\",\n",
    "    model=\"databricks:/databricks-claude-sonnet-4-5\"\n",
    ")\n",
    "\n",
    "# Scorer 2: Rationale Sufficiency (Template Judge, Pass/Fail)\n",
    "# This judge will be aligned with human feedback\n",
    "rationale_sufficiency_judge = make_judge(\n",
    "    name=\"rationale_sufficiency\",\n",
    "    instructions=\"\"\"\n",
    "Evaluate whether the agent's rationale is sufficient to explain and justify the decision.\n",
    "\n",
    "Customer complaint: {{ inputs }}\n",
    "Agent's output: {{ outputs }}\n",
    "\n",
    "Check if a human reading the rationale can clearly understand:\n",
    "1. What decision was made (suggest_credit or escalate)\n",
    "2. Why that decision was appropriate for this complaint\n",
    "3. How any credit amount was determined (if applicable)\n",
    "\n",
    "For credit decisions:\n",
    "- Does the rationale cite specific numbers (delivery time, percentiles, dollar amounts)?\n",
    "- Is there a clear logical connection between the evidence mentioned and the credit amount?\n",
    "- Would a human understand why this amount is fair?\n",
    "\n",
    "For escalations:\n",
    "- Does the rationale explain why escalation is needed?\n",
    "- Is the priority level (standard/urgent) justified?\n",
    "- Would a human reviewer know what to investigate?\n",
    "\n",
    "Rate as PASS if the rationale is clear, complete, and logically connects evidence to decision.\n",
    "Rate as FAIL if the rationale is vague, missing key information, or logic is unclear.\n",
    "\"\"\",\n",
    "    model=\"databricks:/databricks-claude-sonnet-4-5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "## 5. Run Baseline Evaluation (Single Model)\n",
    "\n",
    "We will start by running an initial evaluation with one model to validate our eval setup and generate traces. Note that we run the evaluation in two phases:\n",
    "1. Run the agent to generate traces\n",
    "2. Retrieve the traces and evaluate them with the scorers\n",
    "\n",
    "We can then use the traces and initial evaluation results to align the `rationale_sufficiency` judge with human feedback.\n",
    "\n",
    "There are different approaches you can take to running evaluations, including passing a prediction function to `mlflow.genai.evaluate`. Working with a pre-generated trace dataset is simple and flexible and allows you to iterate on your judges without re-running the agent.\n",
    "\n",
    "We will tag the traces with the baseline model name so we can easily retrieve them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one model for initial evaluation and judge alignment\n",
    "initial_model_name = \"llama-3-3-70b\"\n",
    "initial_model_endpoint = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "baseline_tag = f\"baseline_{initial_model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-initial-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Enable autologging for detailed trace capture\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "print(f\"Running {initial_model_name} on test cases to generate traces...\\n\")\n",
    "\n",
    "# Create agent instance\n",
    "agent = ComplaintsAgentCore(model_endpoint=initial_model_endpoint, catalog=CATALOG)\n",
    "\n",
    "# Enable autologging to capture traces\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Step 1: Run agent to generate traces (tag them for later retrieval)\n",
    "with mlflow.start_run(run_name=baseline_tag) as run:\n",
    "    experiment_id = run.info.experiment_id\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    # Invoke agent on each complaint to generate traces\n",
    "    for row in eval_data:\n",
    "        complaint = row['inputs']['complaint']\n",
    "        result = agent.invoke(complaint)\n",
    "        \n",
    "        # Tag the trace for later retrieval\n",
    "        trace_id = mlflow.get_last_active_trace_id()\n",
    "        mlflow.set_trace_tag(trace_id, \"eval_group\", baseline_tag)\n",
    "        mlflow.set_trace_tag(trace_id, \"model\", initial_model_name)\n",
    "\n",
    "print(f\"\u2705 Generated {len(eval_data)} traces. Run ID: {run_id}\")\n",
    "\n",
    "# Step 2: Retrieve traces for evaluation\n",
    "baseline_traces = mlflow.search_traces(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=f\"tags.eval_group = '{baseline_tag}'\",\n",
    "    max_results=15\n",
    ")\n",
    "\n",
    "\n",
    "# Step 3: Evaluate the traces\n",
    "print(f\"\\nEvaluating traces with scorers...\")\n",
    "baseline_result = mlflow.genai.evaluate(\n",
    "    data=baseline_traces,\n",
    "    scorers=[evidence_groundedness_judge, rationale_sufficiency_judge]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-eval-output",
   "metadata": {},
   "source": [
    "In our run, the baseline evaluation:\n",
    "- Generated 15 traces tagged with `baseline_llama-3-3-70b`\n",
    "- Ran both judges (evidence_groundedness and rationale_sufficiency) on all traces\n",
    "- Completed successfully with results viewable in the MLflow UI Evaluation tab\n",
    "\n",
    "You can now review these traces and add human feedback to improve judge accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "## 6. Provide Human Assessments for Judge Alignment\n",
    "\n",
    "Now it's time to add human feedback to improve the judge's accuracy. To do so, navigate to the Evaluation tab in the MLflow UI and find the traces under the run generated above. You can add your feedback by clicking into each trace, then clicking the \"+\" next to the `rationale_sufficiency` scorer. Fill in your feedback for each trace (even if you agree with the initial assessment) along with your rationale, if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "## 7. Judge Alignment\n",
    "\n",
    "After collecting human feedback, we can align the judge to better match human judgment. The SIMBA optimizer analyzes disagreements between the judge and human assessments, then generates improved instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "align-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.judges.optimizers import SIMBAAlignmentOptimizer\n",
    "\n",
    "baseline_traces = mlflow.search_traces(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=f\"tags.eval_group = '{baseline_tag}'\",\n",
    "    max_results=15,\n",
    "    return_type=\"list\"\n",
    ")\n",
    "\n",
    "\n",
    "# Get traces with human feedback for alignment\n",
    "traces_for_alignment = baseline_traces\n",
    "\n",
    "# Align the rationale_sufficiency judge with human feedback\n",
    "# Note: We align rationale_sufficiency (not evidence_groundedness) because alignment\n",
    "# is not yet supported for trace-aware judges using {{ trace }} \n",
    "## (source: https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/alignment/#quick-start-align-your-first-judge)\n",
    "\n",
    "\n",
    "aligned_judge = rationale_sufficiency_judge.align(traces_for_alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judge-alignment-output",
   "metadata": {},
   "source": [
    "During alignment, the SIMBA optimizer analyzed disagreements between the initial judge and human assessments, then generated improved instructions. The aligned judge now includes specific guidance learned from human feedback:\n",
    "\n",
    "**For missing item complaints:**\n",
    "- Focus solely on the specifics of that complaint\n",
    "- Clearly state what item is missing\n",
    "- Provide the item's price from the order\n",
    "- Explain how the credit amount is calculated based on that price\n",
    "- Avoid discussing irrelevant factors like delivery time unless they directly impact the decision\n",
    "\n",
    "**For all complaints:**\n",
    "- Ensure all statements are consistent and logically support the decision\n",
    "- Clarify why a credit is appropriate in the context of the complaint\n",
    "- Connect evidence explicitly to decisions\n",
    "\n",
    "The alignment process took approximately 2-3 minutes. The resulting aligned judge is better calibrated to human judgment and more focused on relevant complaint details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "## 8. Model Comparison with Aligned Judge\n",
    "\n",
    "Now that the judge is aligned with human feedback, compare multiple models to find the best one for your use case. Check out the list of models in the Databricks Model Serving tab to see the available models.\n",
    "\n",
    "Suppose we are interested in comparing the following models:\n",
    "- **databricks-claude-sonnet-4-5**\n",
    "- **databricks-gpt-5-mini**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-aligned-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models_to_compare = {\n",
    "    \"Claude Sonnet 4.5\": \"databricks-claude-sonnet-4-5\",\n",
    "    \"GPT-5 mini\": \"databricks-gpt-5-mini\"\n",
    "}\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "for model_name, endpoint in models_to_compare.items():\n",
    "    print(f\"\\nRunning {model_name} to generate traces...\")\n",
    "    \n",
    "    agent = ComplaintsAgentCore(model_endpoint=endpoint, catalog=CATALOG)\n",
    "    \n",
    "    # Step 1: Generate traces with tags\n",
    "    comparison_tag = f\"comparison_{model_name}\"\n",
    "    with mlflow.start_run(run_name=comparison_tag) as run:\n",
    "        # Invoke agent on each complaint to generate traces\n",
    "        for row in eval_data:\n",
    "            complaint = row['inputs']['complaint']\n",
    "            result = agent.invoke(complaint)\n",
    "            \n",
    "            # Tag the trace for later retrieval\n",
    "            trace_id = mlflow.get_last_active_trace_id()\n",
    "            mlflow.set_trace_tag(trace_id, \"eval_group\", comparison_tag)\n",
    "            mlflow.set_trace_tag(trace_id, \"model\", model_name)\n",
    "    \n",
    "    print(f\"  Generated {len(eval_data)} traces for {model_name}\")\n",
    "    \n",
    "    # Step 2: Retrieve traces\n",
    "    traces = mlflow.search_traces(\n",
    "        experiment_ids=[experiment_id],\n",
    "        filter_string=f\"tags.eval_group = '{comparison_tag}'\",\n",
    "        max_results=15\n",
    "    )\n",
    "    \n",
    "    # Step 3: Evaluate traces\n",
    "    print(f\"  Evaluating {len(traces)} traces with aligned judge...\")\n",
    "    result = mlflow.genai.evaluate(\n",
    "        data=traces,\n",
    "        scorers=[aligned_judge, rationale_sufficiency_judge]\n",
    "    )\n",
    "    comparison_results[model_name] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-comparison-output",
   "metadata": {},
   "source": [
    "For each model in the comparison:\n",
    "- Generated 15 new traces (one per complaint scenario)\n",
    "- Tagged traces for easy retrieval and organization\n",
    "- Evaluated with both the aligned rationale_sufficiency judge and the trace-aware evidence_groundedness judge\n",
    "- Results are available in MLflow UI for detailed analysis\n",
    "\n",
    "The comparison enables side-by-side evaluation of model performance on accuracy, rationale quality, and latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9-header",
   "metadata": {},
   "source": [
    "## 9. Results Analysis\n",
    "\n",
    "Compare the models to identify the best one for your use case:\n",
    "- Which model has the highest evidence groundedness (decisions align with tool data)?\n",
    "- Which model has the most sufficient rationales (clear logic from rationale to decision)?\n",
    "- What are the latency trade-offs between models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-comparison-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "# Add baseline result for reference\n",
    "comparison_data.append({\n",
    "    'model': f\"{initial_model_name} (baseline)\",\n",
    "    'evidence_groundedness': baseline_result.metrics.get('evidence_groundedness/percentage', 0),\n",
    "    'rationale_sufficiency': baseline_result.metrics.get('rationale_sufficiency/percentage', 0),\n",
    "    'latency_p50_ms': baseline_result.metrics.get('latency/p50', 0) * 1000 if baseline_result.metrics.get('latency/p50') else 0,\n",
    "})\n",
    "\n",
    "# Add comparison results\n",
    "for model_name, result in comparison_results.items():\n",
    "    comparison_data.append({\n",
    "        'model': model_name,\n",
    "        'evidence_groundedness': result.metrics.get('evidence_groundedness/percentage', 0),\n",
    "        'rationale_sufficiency': result.metrics.get('rationale_sufficiency/percentage', 0),\n",
    "        'latency_p50_ms': result.metrics.get('latency/p50', 0) * 1000 if result.metrics.get('latency/p50') else 0,\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNote: Baseline used initial judge; other models used aligned judge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Built a parameterized complaint triage agent that uses UC tools.\n",
    "- Generated a concise eval set and ran a baseline to create traces.\n",
    "- Evaluated with two judges: evidence_groundedness (trace-aware, unaligned) and rationale_sufficiency (template judge, aligned via SIMBA with human feedback).\n",
    "- Compared multiple serving endpoints using the aligned judge and reviewed accuracy and latency trade-offs.\n",
    "\n",
    "**Key takeaways:**\n",
    "1. Databricks lets you use the models you prefer and provides first-class tools to build agents with UC functions, capture traces, and evaluate them rigorously with MLflow.\n",
    "2. Trace-aware judging verifies decisions against actual tool outputs; template judges can be aligned with human feedback for better agreement.\n",
    "3. Choose the endpoint that best balances quality, latency, and cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}