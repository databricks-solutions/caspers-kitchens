{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Complaint Agent: Model Comparison & Selection\n",
    "\n",
    "This notebook demonstrates how to compare different LLM models for an agentic application using MLflow's evaluation and judge alignment capabilities.\n",
    "\n",
    "**Scenario:** You've built a prototype complaint triage agent but aren't sure which model to use. Databricks is model-agnostic, so you can easily test models from different providers.\n",
    "\n",
    "**What you'll do:**\n",
    "1. Define a lightweight agent for inner-loop development (no deployment required)\n",
    "2. Create an evaluation dataset with diverse complaint scenarios\n",
    "3. Evaluate 3 different models using Agent-as-a-Judge and Guidelines scorers\n",
    "4. Review results and add human feedback via MLflow UI\n",
    "5. Align the judge with human feedback to improve accuracy\n",
    "6. Re-evaluate and visualize results\n",
    "7. Register the winning model configuration\n",
    "\n",
    "**Prerequisites:** This demo assumes UC tools have been created by running `stages/complaint_agent.ipynb` first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "## 1. Setup & Prerequisites\n",
    "\n",
    "Install required packages and verify that UC tools exist. This is inner-loop development, so no deployment or ResponsesAgent wrapper needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow[databricks] langgraph==0.3.4 databricks-langchain plotly\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-widgets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create widget for catalog name (will be populated by bundle parameter)\n",
    "dbutils.widgets.text(\"CATALOG\", \"dlcaspers\", \"Catalog Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "if not CATALOG:\n",
    "    raise ValueError(\"Please provide a CATALOG name in the widget above\")\n",
    "print(f\"Using catalog: {CATALOG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify UC tools exist\n",
    "print(\"Checking for required UC functions...\")\n",
    "\n",
    "required_functions = ['get_order_overview', 'get_order_timing', 'get_location_timings']\n",
    "missing_functions = []\n",
    "\n",
    "for func_name in required_functions:\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE FUNCTION {CATALOG}.ai.{func_name}\").collect()\n",
    "        print(f\"✅ {func_name} found\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {func_name} not found\")\n",
    "        missing_functions.append(func_name)\n",
    "\n",
    "if missing_functions:\n",
    "    raise RuntimeError(\n",
    "        f\"Missing UC functions: {', '.join(missing_functions)}. \"\n",
    "        f\"Run ../../stages/complaint_agent.ipynb first to create these tools.\"\n",
    "    )\n",
    "\n",
    "print(\"\\n✅ All required UC functions found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Create experiment for model comparison evaluations\n",
    "experiment_name = f\"/Shared/{CATALOG}_model_comparison\"\n",
    "experiment = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "print(f\"✅ Using experiment: {experiment_name}\")\n",
    "print(f\"   Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "## 2. Define Inner-Loop Agent\n",
    "\n",
    "Create a lightweight version of the complaint agent that takes `model_endpoint` as a parameter. This version:\n",
    "- Uses the same UC tools and system prompt as the production version\n",
    "- Returns results as a dict (no ResponsesAgent wrapper)\n",
    "- Can be quickly instantiated with different models for comparison\n",
    "\n",
    "This is perfect for rapid iteration during model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-agent-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Annotated, Any, Optional, Sequence, TypedDict, Literal, cast\n",
    "from uuid import uuid4\n",
    "\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "# Set up UC function client\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "# Define response schema (same as production)\n",
    "class ComplaintResponse(TypedDict):\n",
    "    order_id: str\n",
    "    complaint_category: Literal[\"delivery_delay\", \"missing_items\", \"food_quality\", \"service_issue\", \"billing\", \"other\"]\n",
    "    decision: Literal[\"suggest_credit\", \"escalate\"]\n",
    "    credit_amount: Optional[float]\n",
    "    confidence: Optional[Literal[\"high\", \"medium\", \"low\"]]\n",
    "    priority: Optional[Literal[\"standard\", \"urgent\"]]\n",
    "    rationale: str\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# System prompt (same as production)\n",
    "SYSTEM_PROMPT = \"\"\"You are the Complaint Triage Agent for Casper's Kitchens, a ghost kitchen network. Your job is to analyze customer complaints and recommend actions for internal customer service staff.\n",
    "\n",
    "PROCESS:\n",
    "1. Extract the order_id from the complaint text\n",
    "2. Call get_order_overview(order_id) to get order details and items\n",
    "3. Call get_order_timing(order_id) to get delivery timing\n",
    "4. For delivery delay complaints, call get_location_timings(location) to get percentile benchmarks\n",
    "5. Analyze the data and make a recommendation\n",
    "\n",
    "DECISION FRAMEWORK:\n",
    "\n",
    "SUGGEST_CREDIT - Use when you can make a data-backed recommendation:\n",
    "\n",
    "Delivery delays (high confidence when data-backed):\n",
    "- Compare actual delivery time to location percentiles\n",
    "- >P75 but <P90: Suggest 15% of order total\n",
    "- >P90 but <P99: Suggest 25% of order total  \n",
    "- >P99: Suggest 50% of order total\n",
    "- On-time delivery: Suggest $0 credit (low confidence - might be other issues)\n",
    "\n",
    "Missing items:\n",
    "- Parse items_json from get_order_overview to find actual item prices\n",
    "- Use real item costs when available for accurate refund\n",
    "- Check if claimed missing item actually makes sense for this order\n",
    "- If item plausibly missing but can't verify price: estimate $8-12 per item (medium confidence)\n",
    "- If no evidence of missing items: $0 credit (low confidence)\n",
    "\n",
    "Food quality issues:\n",
    "- If specific details provided (cold, soggy, wrong preparation): $10-15 (medium confidence)\n",
    "- If vague (\"bad\", \"gross\"): escalate with priority=\"standard\" instead\n",
    "- If food quality complaint but order shows delivery delay >P90: consider combined credit\n",
    "\n",
    "ESCALATE - Use when human judgment is needed:\n",
    "- priority=\"standard\": Vague complaints, missing data, edge cases, billing issues, service complaints\n",
    "- priority=\"urgent\": Legal threats, health/safety concerns, suspected fraud, abusive language\n",
    "\n",
    "OUTPUT RULES:\n",
    "- For suggest_credit: Include credit_amount (can be $0) and confidence, set priority to null\n",
    "- For escalate: Include priority, set credit_amount and confidence to null\n",
    "- Always include a data-focused rationale citing specific numbers (delivery times, percentiles, order details, item verification)\n",
    "- Rationale is for internal staff - be factual, not apologetic\n",
    "\n",
    "RATIONALE GUIDELINES:\n",
    "- Rationale should clearly articulate the justification for the decision\n",
    "- All evidence used to support the decision should be cited in the rationale. Rationale MUST cite evidence.\n",
    "- Rationale should be detailed and specific, but no longer than 150 words\n",
    "- Rationale should clearly justify the decision, credit amount (if applicable), confidence level, and priority (if applicable)\n",
    "- If a refund is suggested, the rationale should clearly justify the credit amount, based on the evidence provided.\n",
    "\n",
    "CONFIDENCE GUIDELINES:\n",
    "- high: Strong data support (delivery >P90, item prices from order data, clear verification)\n",
    "- medium: Reasonable inference (delivery P75-P90, estimated item costs, plausible but unverified)\n",
    "- low: Weak or contradictory evidence ($0 credits when data doesn't support claim, can't verify complaint details)\n",
    "\n",
    "IMPORTANT NOTES:\n",
    "- If order_id not found or data unavailable: escalate with priority=\"standard\"\n",
    "- Round credit amounts to nearest $0.50\n",
    "- When in doubt between two options, prefer escalate with priority=\"standard\"\n",
    "- A suggest_credit with $0 and low confidence is valid when complaint seems unfounded\n",
    "\"\"\"\n",
    "\n",
    "RESPONSE_FIELDS = {\"order_id\", \"complaint_category\", \"decision\", \"rationale\"}\n",
    "\n",
    "def parse_structured_response(obj) -> ComplaintResponse:\n",
    "    \"\"\"Parse ComplaintResponse from AIMessage or dict\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        candidate = obj\n",
    "    else:\n",
    "        parsed = obj.additional_kwargs.get(\"parsed_structured_output\")\n",
    "        if isinstance(parsed, dict):\n",
    "            candidate = parsed\n",
    "        else:\n",
    "            content = obj.content\n",
    "            if isinstance(content, str):\n",
    "                raw = content\n",
    "            elif isinstance(content, list):\n",
    "                raw = \"\".join(part.get(\"text\", \"\") if isinstance(part, dict) else str(part) for part in content)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported message content type\")\n",
    "            candidate = json.loads(raw)\n",
    "\n",
    "    missing = RESPONSE_FIELDS.difference(candidate.keys())\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required fields: {sorted(missing)}\")\n",
    "\n",
    "    decision = candidate.get(\"decision\")\n",
    "    if decision == \"suggest_credit\":\n",
    "        if candidate.get(\"credit_amount\") is None:\n",
    "            raise ValueError(\"suggest_credit requires credit_amount\")\n",
    "        if candidate.get(\"confidence\") is None:\n",
    "            raise ValueError(\"suggest_credit requires confidence\")\n",
    "        if candidate.get(\"priority\") is not None:\n",
    "            candidate[\"priority\"] = None\n",
    "    elif decision == \"escalate\":\n",
    "        if candidate.get(\"priority\") is None:\n",
    "            raise ValueError(\"escalate requires priority\")\n",
    "        if candidate.get(\"credit_amount\") is not None:\n",
    "            candidate[\"credit_amount\"] = None\n",
    "        if candidate.get(\"confidence\") is not None:\n",
    "            candidate[\"confidence\"] = None\n",
    "\n",
    "    return cast(ComplaintResponse, candidate)\n",
    "\n",
    "\n",
    "class ComplaintsAgentCore:\n",
    "    \"\"\"Lightweight complaint agent for model comparison (no ResponsesAgent wrapper)\"\"\"\n",
    "    \n",
    "    def __init__(self, model_endpoint: str, catalog: str):\n",
    "        self.model_endpoint = model_endpoint\n",
    "        self.catalog = catalog\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = ChatDatabricks(endpoint=model_endpoint)\n",
    "        \n",
    "        # Load UC tools\n",
    "        uc_tool_names = [\n",
    "            f\"{catalog}.ai.get_order_overview\",\n",
    "            f\"{catalog}.ai.get_order_timing\",\n",
    "            f\"{catalog}.ai.get_location_timings\",\n",
    "        ]\n",
    "        uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "        self.tools = uc_toolkit.tools\n",
    "        \n",
    "        # Build agent\n",
    "        self.agent = self._create_agent()\n",
    "    \n",
    "    def _create_agent(self):\n",
    "        \"\"\"Create LangGraph workflow\"\"\"\n",
    "        tool_model = self.llm.bind_tools(self.tools, tool_choice=\"auto\")\n",
    "        structured_model = tool_model.with_structured_output(ComplaintResponse)\n",
    "\n",
    "        def should_continue(state: AgentState):\n",
    "            messages = state[\"messages\"]\n",
    "            last_message = messages[-1]\n",
    "            if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "                return \"continue\"\n",
    "            return \"end\"\n",
    "\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] + state[\"messages\"]\n",
    "        )\n",
    "\n",
    "        tool_runnable = preprocessor | tool_model\n",
    "        structured_runnable = preprocessor | structured_model\n",
    "\n",
    "        def call_model(state: AgentState, config: RunnableConfig):\n",
    "            response = tool_runnable.invoke(state, config)\n",
    "            if not isinstance(response, AIMessage):\n",
    "                raise ValueError(f\"Expected AIMessage, received {type(response)}\")\n",
    "\n",
    "            if response.tool_calls:\n",
    "                return {\"messages\": [response]}\n",
    "\n",
    "            try:\n",
    "                parsed = parse_structured_response(response)\n",
    "            except (json.JSONDecodeError, ValueError):\n",
    "                structured = structured_runnable.invoke(state, config)\n",
    "                parsed = parse_structured_response(structured)\n",
    "\n",
    "            structured_message = AIMessage(\n",
    "                id=response.id or str(uuid4()),\n",
    "                content=json.dumps(parsed),\n",
    "                additional_kwargs={\"parsed_structured_output\": parsed},\n",
    "            )\n",
    "            return {\"messages\": [structured_message]}\n",
    "\n",
    "        workflow = StateGraph(AgentState)\n",
    "        workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "        workflow.add_node(\"tools\", ToolNode(self.tools))\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"agent\",\n",
    "            should_continue,\n",
    "            {\"continue\": \"tools\", \"end\": END},\n",
    "        )\n",
    "        workflow.add_edge(\"tools\", \"agent\")\n",
    "        return workflow.compile()\n",
    "    \n",
    "    def invoke(self, complaint: str) -> dict:\n",
    "        \"\"\"Process a complaint and return structured response\"\"\"\n",
    "        result = self.agent.invoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": complaint}]\n",
    "        })\n",
    "        \n",
    "        # Extract final response\n",
    "        final_message = result[\"messages\"][-1]\n",
    "        return parse_structured_response(final_message)\n",
    "\n",
    "print(\"✅ ComplaintsAgentCore class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-agent-header",
   "metadata": {},
   "source": [
    "### Test the Agent\n",
    "\n",
    "Before creating the full evaluation dataset, let's validate that the agent works correctly with a single test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample order ID for testing\n",
    "sample_order = spark.sql(f\"\"\"\n",
    "    SELECT order_id \n",
    "    FROM {CATALOG}.lakeflow.all_events \n",
    "    WHERE event_type='delivered'\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0]['order_id']\n",
    "\n",
    "print(f\"Using test order ID: {sample_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Enable autologging for LangChain\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Create agent instance with a test model\n",
    "test_agent = ComplaintsAgentCore(\n",
    "    model_endpoint=\"databricks-llama-4-maverick\",\n",
    "    catalog=CATALOG\n",
    ")\n",
    "\n",
    "# Test with a delivery delay complaint\n",
    "test_complaint = f\"My order took forever to arrive! Order ID: {sample_order}\"\n",
    "print(f\"\\nTest complaint: {test_complaint}\")\n",
    "print(\"\\nProcessing...\")\n",
    "\n",
    "result = test_agent.invoke(test_complaint)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGENT RESPONSE\")\n",
    "print(\"=\"*80)\n",
    "import json\n",
    "print(json.dumps(result, indent=2))\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✅ Agent test successful! Ready to proceed with evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "## 3. Create Evaluation Dataset\n",
    "\n",
    "Generate ~20 diverse complaint scenarios to thoroughly test model performance across different situations:\n",
    "- Delivery delays (with varying severity)\n",
    "- Missing items (verifiable vs suspicious)\n",
    "- Food quality complaints (specific vs vague)\n",
    "- Service issues (should escalate)\n",
    "- Health/safety concerns (urgent escalation)\n",
    "- Edge cases (missing order ID, invalid format)\n",
    "\n",
    "A diverse dataset helps reveal model strengths and weaknesses across the decision space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-sample-orders",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get real order IDs for realistic eval scenarios\n",
    "all_order_ids = [\n",
    "    row['order_id'] for row in spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT order_id \n",
    "        FROM {CATALOG}.lakeflow.all_events \n",
    "        WHERE event_type='delivered'\n",
    "        LIMIT 30\n",
    "    \"\"\").collect()\n",
    "]\n",
    "\n",
    "print(f\"✅ Retrieved {len(all_order_ids)} sample order IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-eval-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Create diverse complaint scenarios\n",
    "complaints = []\n",
    "\n",
    "# Delivery delays (8 examples)\n",
    "for oid in all_order_ids[:4]:\n",
    "    complaints.extend([\n",
    "        f\"My order took forever to arrive! Order ID: {oid}\",\n",
    "        f\"Order {oid} arrived late and cold\",\n",
    "    ])\n",
    "\n",
    "# Food quality - specific (4 examples)\n",
    "for oid in all_order_ids[4:6]:\n",
    "    complaints.extend([\n",
    "        f\"My falafel was completely soggy and inedible. Order: {oid}\",\n",
    "        f\"The gyro meat was overcooked and dry. Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# Food quality - vague (should escalate) (2 examples)\n",
    "for oid in all_order_ids[6:8]:\n",
    "    complaints.append(f\"Everything tasted bad. Order {oid}\")\n",
    "\n",
    "# Missing items (4 examples)\n",
    "for oid in all_order_ids[8:10]:\n",
    "    complaints.extend([\n",
    "        f\"My entire falafel bowl was missing from the order! Order: {oid}\",\n",
    "        f\"No drinks in my order {oid}\",\n",
    "    ])\n",
    "\n",
    "# Service issues (should escalate) (3 examples)\n",
    "for oid in all_order_ids[10:13]:\n",
    "    complaints.append(f\"Your driver was extremely rude to me. Order: {oid}\")\n",
    "\n",
    "# Health/safety (urgent escalation) (2 examples)\n",
    "for oid in all_order_ids[13:15]:\n",
    "    complaints.append(f\"This food made me sick, possible food poisoning. Order: {oid}\")\n",
    "\n",
    "# Multiple issues (2 examples)\n",
    "for oid in all_order_ids[15:17]:\n",
    "    complaints.append(f\"Order {oid} was late AND missing items AND cold!\")\n",
    "\n",
    "# Edge cases (3 examples)\n",
    "complaints.extend([\n",
    "    \"My order was really late and the food was cold!\",  # Missing order ID\n",
    "    \"Order ABC123 never arrived\",  # Invalid order ID\n",
    "    f\"Not satisfied with order {all_order_ids[17]}\",  # Vague\n",
    "])\n",
    "\n",
    "# Ensure exactly 20 examples\n",
    "complaints = complaints[:20]\n",
    "\n",
    "# Format for mlflow.genai.evaluate\n",
    "# predict_fn takes 'inputs' parameter, so data format is {\"inputs\": <value>}\n",
    "eval_data = [{\"inputs\": {\"complaint\": c} } for c in complaints]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "## 4. Define Scorers\n",
    "\n",
    "We use two complementary scorers with no overlap:\n",
    "\n",
    "1. **Evidence Groundedness (Agent-as-a-Judge, Pass/Fail)**: Inspects execution traces to verify the final decision aligns with evidence actually gathered from tool calls. Uses trace inspection to check if the agent's conclusions match the data it retrieved. This scorer will be aligned with human feedback.\n",
    "\n",
    "2. **Rationale Sufficiency (Guidelines, Pass/Fail)**: Validates output structure only—can a human follow the logic from rationale to decision? Checks self-consistency without needing trace data. Rule-based and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-scorers",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.judges import make_judge\n",
    "from mlflow.genai.scorers import Guidelines\n",
    "\n",
    "# Scorer 1: Evidence Groundedness (Agent-as-a-Judge, Binary Pass/Fail)\n",
    "# Uses {{ trace }} to enable agentic evaluation with tool access\n",
    "evidence_groundedness_judge = make_judge(\n",
    "    name=\"evidence_groundedness\",\n",
    "    instructions=\"\"\"\n",
    "Evaluate whether the agent's decision is grounded in evidence from the execution {{ trace }}.\n",
    "\n",
    "Investigation checklist:\n",
    "1. Find spans where tools were called (get_order_overview, get_order_timing, get_location_timings)\n",
    "2. Extract the actual outputs returned by these tool calls\n",
    "3. Compare tool outputs against claims made in the agent's rationale\n",
    "4. Verify that credit amounts or escalation decisions match the tool data\n",
    "\n",
    "For delivery complaints, check:\n",
    "- Does the rationale's delivery time match what get_order_timing returned?\n",
    "- Does the rationale's percentile comparison match get_location_timings output?\n",
    "- Is the credit calculation based on the actual order total from get_order_overview?\n",
    "\n",
    "For missing item complaints, check:\n",
    "- Does get_order_overview show the items mentioned in the rationale?\n",
    "- Is the credit amount based on actual item prices from the tool output?\n",
    "\n",
    "Customer complaint: {{ inputs }}\n",
    "Agent's final output: {{ outputs }}\n",
    "\n",
    "Rate as PASS if rationale claims match tool outputs, FAIL if there are contradictions or unsupported claims.\n",
    "\"\"\",\n",
    "    model=\"databricks:/databricks-claude-sonnet-4-5\"\n",
    ")\n",
    "\n",
    "# Scorer 2: Rationale Sufficiency (Guidelines, Pass/Fail)\n",
    "rationale_sufficiency = Guidelines(\n",
    "    name=\"rationale_sufficiency\",\n",
    "    guidelines=[\n",
    "        \"If decision='suggest_credit', rationale must contain numerical justification (dollar amount, percentile, or item value)\",\n",
    "        \"If decision='suggest_credit', credit_amount must be present and > 0\",\n",
    "        \"If decision='escalate', rationale must explain why (e.g., health/safety, missing data, service issue)\",\n",
    "        \"If decision='escalate' and priority='urgent', rationale must mention health, safety, or time-sensitive concern\",\n",
    "        \"Rationale must be at least 20 characters long\",\n",
    "        \"Decision must be either 'suggest_credit' or 'escalate'\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"✅ Scorers defined:\")\n",
    "print(\"   - evidence_groundedness (Agent-as-a-Judge, inspects traces, will be aligned)\")\n",
    "print(\"   - rationale_sufficiency (Guidelines, output-only validation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "## 5. Run Initial Evaluation (Single Model)\n",
    "\n",
    "Run an initial evaluation with one model to validate your eval setup and generate traces for judge alignment.\n",
    "\n",
    "This creates the baseline traces that you'll review and use to align the judge before comparing multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one model for initial evaluation and judge alignment\n",
    "initial_model_name = \"llama-3-3-70b\"\n",
    "initial_model_endpoint = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "\n",
    "print(f\"Initial model for baseline evaluation: {initial_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-initial-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Enable autologging for detailed trace capture\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "print(f\"Running {initial_model_name} on 20 test cases to generate traces...\\n\")\n",
    "\n",
    "# Create agent instance\n",
    "agent = ComplaintsAgentCore(model_endpoint=initial_model_endpoint, catalog=CATALOG)\n",
    "\n",
    "# Enable autologging to capture traces\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Step 1: Run agent to generate traces (tag them for later retrieval)\n",
    "baseline_tag = f\"baseline_{initial_model_name}\"\n",
    "with mlflow.start_run(run_name=baseline_tag) as run:\n",
    "    experiment_id = run.info.experiment_id\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    # Invoke agent on each complaint to generate traces\n",
    "    for row in eval_data.itertuples():\n",
    "        complaint = row.inputs['complaint']\n",
    "        result = agent.invoke(complaint)\n",
    "        \n",
    "        # Tag the trace for later retrieval\n",
    "        trace_id = mlflow.get_last_active_trace_id()\n",
    "        mlflow.set_trace_tag(trace_id, \"eval_group\", baseline_tag)\n",
    "        mlflow.set_trace_tag(trace_id, \"model\", initial_model_name)\n",
    "\n",
    "print(f\"✅ Generated {len(eval_data)} traces. Run ID: {run_id}\")\n",
    "\n",
    "# Step 2: Retrieve traces for evaluation\n",
    "print(f\"\\nRetrieving traces for evaluation...\")\n",
    "baseline_traces = mlflow.search_traces(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=f\"tags.eval_group = '{baseline_tag}'\",\n",
    "    max_results=20\n",
    ")\n",
    "\n",
    "print(f\"Retrieved {len(baseline_traces)} traces\")\n",
    "\n",
    "# Step 3: Evaluate the traces\n",
    "print(f\"\\nEvaluating traces with scorers...\")\n",
    "baseline_result = mlflow.genai.evaluate(\n",
    "    data=baseline_traces,\n",
    "    scorers=[evidence_groundedness_judge, rationale_sufficiency]\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Baseline evaluation complete:\")\n",
    "print(f\"   Evidence Groundedness: {baseline_result.metrics.get('evidence_groundedness/percentage', 'N/A'):.1f}% pass\")\n",
    "print(f\"   Rationale Sufficiency: {baseline_result.metrics.get('rationale_sufficiency/percentage', 'N/A'):.1f}% pass\")\n",
    "print(f\"\\nExperiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "## 6. Human Review Instructions\n",
    "\n",
    "Now it's time to add human feedback to improve the judge's accuracy. The MLflow UI makes this easy:\n",
    "\n",
    "### How to Add Feedback:\n",
    "\n",
    "1. **Open MLflow UI**: Navigate to the experiment (link printed below)\n",
    "2. **Review traces**: Click into evaluation runs to see individual predictions\n",
    "3. **Add feedback**: For each trace, you can rate the `decision_quality` assessment\n",
    "   - ✅ Mark as \"good\" if the agent made the right decision\n",
    "   - ❌ Mark as \"bad\" if the agent made the wrong decision\n",
    "   - Add comments explaining your reasoning\n",
    "\n",
    "### What to Look For:\n",
    "- Did the agent call the right tools for the complaint type?\n",
    "- Is the credit amount justified by the data retrieved?\n",
    "- Are escalations appropriate for the severity?\n",
    "- Does the rationale cite specific numbers from tool outputs?\n",
    "\n",
    "### Tips:\n",
    "- Focus on cases where the judge scored 2-4 (borderline/uncertain cases)\n",
    "- You need at least 10 feedback entries for alignment to work well\n",
    "- Look for patterns where the judge disagreed with your assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print-review-instructions",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "# Get workspace URL\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "experiment_path = urllib.parse.quote(experiment_name)\n",
    "\n",
    "mlflow_ui_url = f\"https://{workspace_url}/#mlflow/experiments/{experiment_id}\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"👤 HUMAN REVIEW REQUIRED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n1. Open MLflow UI: {mlflow_ui_url}\")\n",
    "print(\"\\n2. Review evaluation traces and add feedback:\")\n",
    "print(\"   - Click into individual runs to see predictions\")\n",
    "print(\"   - For each trace, rate whether the agent's decision was correct\")\n",
    "print(\"   - Add comments explaining your reasoning\")\n",
    "print(\"\\n3. Focus on:\")\n",
    "print(\"   - Tool usage (did it call the right functions?)\")\n",
    "print(\"   - Data justification (does rationale cite specific numbers?)\")\n",
    "print(\"   - Escalation appropriateness (correct priority level?)\")\n",
    "print(\"   - Credit accuracy (reasonable amount based on data?)\")\n",
    "print(\"\\n4. Aim for at least 10-15 feedback entries for good alignment\")\n",
    "print(\"\\n5. Once done, proceed to the next cell for judge alignment\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "## 7. Judge Alignment\n",
    "\n",
    "After collecting human feedback, we can align the judge to better match human judgment. The SIMBA optimizer analyzes disagreements between the judge and human assessments, then generates improved instructions.\n",
    "\n",
    "This typically reduces false positives/negatives by 30-50% compared to the initial judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "align-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.optimizers import SIMBAAlignmentOptimizer\n",
    "\n",
    "# Get traces with human feedback for alignment\n",
    "traces_for_alignment = mlflow.search_traces(\n",
    "    experiment_ids=[experiment_id], max_results=15, return_type=\"list\"\n",
    ")\n",
    "\n",
    "# Align the judge using human corrections (minimum 10 traces recommended)\n",
    "if len(traces_for_alignment) >= 10:\n",
    "    optimizer = SIMBAAlignmentOptimizer(model=\"anthropic:/claude-opus-4-1-20250805\")\n",
    "    \n",
    "    # Run alignment - shows minimal progress by default:\n",
    "    # INFO: Starting SIMBA optimization with 15 examples (set logging to DEBUG for detailed output)\n",
    "    # INFO: SIMBA optimization completed\n",
    "    aligned_judge = evidence_groundedness_judge.align(optimizer, traces_for_alignment)\n",
    "    \n",
    "    # Register the aligned judge\n",
    "    aligned_judge.register(experiment_id=experiment_id)\n",
    "    print(\"✅ Judge aligned successfully with human feedback\")\n",
    "else:\n",
    "    print(f\"Need at least 10 traces for alignment, have {len(traces_for_alignment)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "register-aligned-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register aligned judge for reuse\n",
    "aligned_judge.register(name=f\"{CATALOG}.ai.evidence_groundedness_judge_aligned\")\n",
    "\n",
    "print(f\"✅ Aligned judge registered to UC: {CATALOG}.ai.evidence_groundedness_judge_aligned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "## 8. Model Comparison with Aligned Judge\n",
    "\n",
    "Now that the judge is aligned with human feedback, compare multiple models to find the best one for your use case.\n",
    "\n",
    "We'll evaluate three different models:\n",
    "- **GPT OSS 20B**: Cost-effective open model\n",
    "- **Claude 3.7 Sonnet**: Strong reasoning capabilities\n",
    "- **DBRX Instruct**: Databricks' own model (commented out for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-aligned-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models_to_compare = {\n",
    "    \"gpt-oss\": \"databricks-gpt-oss-20b\",\n",
    "    \"sonnet\": \"databricks-claude-3-7-sonnet\"\n",
    "    # \"dbrx-instruct\": \"databricks-dbrx-instruct\"\n",
    "}\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "for model_name, endpoint in models_to_compare.items():\n",
    "    print(f\"\\nRunning {model_name} to generate traces...\")\n",
    "    \n",
    "    agent = ComplaintsAgentCore(model_endpoint=endpoint, catalog=CATALOG)\n",
    "    \n",
    "    # Step 1: Generate traces with tags\n",
    "    comparison_tag = f\"comparison_{model_name}\"\n",
    "    with mlflow.start_run(run_name=comparison_tag) as run:\n",
    "        # Invoke agent on each complaint to generate traces\n",
    "        for row in eval_data.itertuples():\n",
    "            complaint = row.inputs['complaint']\n",
    "            result = agent.invoke(complaint)\n",
    "            \n",
    "            # Tag the trace for later retrieval\n",
    "            trace_id = mlflow.get_last_active_trace_id()\n",
    "            mlflow.set_trace_tag(trace_id, \"eval_group\", comparison_tag)\n",
    "            mlflow.set_trace_tag(trace_id, \"model\", model_name)\n",
    "    \n",
    "    print(f\"  Generated {len(eval_data)} traces for {model_name}\")\n",
    "    \n",
    "    # Step 2: Retrieve traces\n",
    "    traces = mlflow.search_traces(\n",
    "        experiment_ids=[experiment_id],\n",
    "        filter_string=f\"tags.eval_group = '{comparison_tag}'\",\n",
    "        max_results=20\n",
    "    )\n",
    "    \n",
    "    # Step 3: Evaluate traces\n",
    "    print(f\"  Evaluating {len(traces)} traces with aligned judge...\")\n",
    "    result = mlflow.genai.evaluate(\n",
    "        data=traces,\n",
    "        scorers=[aligned_judge, rationale_sufficiency]\n",
    "    )\n",
    "    comparison_results[model_name] = result\n",
    "    \n",
    "    print(f\"  ✅ {model_name}: evidence_groundedness={result.metrics.get('evidence_groundedness/percentage', 'N/A'):.1f}% pass, \"\n",
    "          f\"rationale_sufficiency={result.metrics.get('rationale_sufficiency/percentage', 'N/A'):.1f}% pass\")\n",
    "\n",
    "print(\"\\n✅ Model comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9-header",
   "metadata": {},
   "source": [
    "## 9. Results Analysis\n",
    "\n",
    "Compare the models to identify the best one for your use case:\n",
    "- Which model has the highest evidence groundedness (decisions align with tool data)?\n",
    "- Which model has the most sufficient rationales (clear logic from rationale to decision)?\n",
    "- What are the latency trade-offs between models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-comparison-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "# Add baseline result for reference\n",
    "comparison_data.append({\n",
    "    'model': f\"{initial_model_name} (baseline)\",\n",
    "    'evidence_groundedness': baseline_result.metrics.get('evidence_groundedness/percentage', 0),\n",
    "    'rationale_sufficiency': baseline_result.metrics.get('rationale_sufficiency/percentage', 0),\n",
    "    'latency_p50_ms': baseline_result.metrics.get('latency/p50', 0) * 1000 if baseline_result.metrics.get('latency/p50') else 0,\n",
    "})\n",
    "\n",
    "# Add comparison results\n",
    "for model_name, result in comparison_results.items():\n",
    "    comparison_data.append({\n",
    "        'model': model_name,\n",
    "        'evidence_groundedness': result.metrics.get('evidence_groundedness/percentage', 0),\n",
    "        'rationale_sufficiency': result.metrics.get('rationale_sufficiency/percentage', 0),\n",
    "        'latency_p50_ms': result.metrics.get('latency/p50', 0) * 1000 if result.metrics.get('latency/p50') else 0,\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNote: Baseline used initial judge; other models used aligned judge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create visualization with multiple charts\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Evidence Groundedness (% Pass)',\n",
    "        'Rationale Sufficiency (% Pass)',\n",
    "        'Latency Comparison (P50)',\n",
    "        'Average Pass Rate (%)'\n",
    "    ),\n",
    "    specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "           [{'type': 'bar'}, {'type': 'bar'}]]\n",
    ")\n",
    "\n",
    "# Chart 1: Evidence Groundedness\n",
    "fig.add_trace(\n",
    "    go.Bar(x=comparison_df['model'], y=comparison_df['evidence_groundedness'],\n",
    "           marker_color='darkblue', showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Chart 2: Rationale Sufficiency\n",
    "fig.add_trace(\n",
    "    go.Bar(x=comparison_df['model'], y=comparison_df['rationale_sufficiency'],\n",
    "           marker_color='green', showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Chart 3: Latency\n",
    "fig.add_trace(\n",
    "    go.Bar(x=comparison_df['model'], y=comparison_df['latency_p50_ms'],\n",
    "           marker_color='orange', showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Chart 4: Average pass rate\n",
    "# Both scorers return 0-100 percentages, so just average them\n",
    "comparison_df['average_pass_rate'] = (\n",
    "    (comparison_df['evidence_groundedness'] + comparison_df['rationale_sufficiency']) / 2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=comparison_df['model'], y=comparison_df['average_pass_rate'],\n",
    "           marker_color='purple', showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"Model Comparison Dashboard\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Score (1-5)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Latency (ms)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Combined Score\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n📊 Visualization complete. Review charts above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10-header",
   "metadata": {},
   "source": [
    "## 10. Register Winner & Next Steps\n",
    "\n",
    "Identify the best-performing model and log its configuration for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identify-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify winner based on combined score\n",
    "winner_idx = comparison_df['average_pass_rate'].idxmax()\n",
    "winner_name = comparison_df.loc[winner_idx, 'model']\n",
    "winner_endpoint = models_to_test[winner_name]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🏆 WINNING MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel: {winner_name}\")\n",
    "print(f\"Endpoint: {winner_endpoint}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  - Evidence Groundedness: {comparison_df.loc[winner_idx, 'evidence_groundedness']:.1f}% pass\")\n",
    "print(f\"  - Rationale Sufficiency: {comparison_df.loc[winner_idx, 'rationale_sufficiency']:.1f}% pass\")\n",
    "print(f\"  - Latency P50: {comparison_df.loc[winner_idx, 'latency_p50_ms']:.0f}ms\")\n",
    "print(f\"  - Average Pass Rate: {comparison_df.loc[winner_idx, 'average_pass_rate']:.1f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "log-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log winning configuration\n",
    "with mlflow.start_run(run_name=f\"winning_config_{winner_name}\") as run:\n",
    "    mlflow.log_param(\"winning_model\", winner_name)\n",
    "    mlflow.log_param(\"model_endpoint\", winner_endpoint)\n",
    "    mlflow.log_param(\"catalog\", CATALOG)\n",
    "    \n",
    "    # Log all metrics\n",
    "    mlflow.log_metric(\"evidence_groundedness\", comparison_df.loc[winner_idx, 'evidence_groundedness'])\n",
    "    mlflow.log_metric(\"rationale_sufficiency\", comparison_df.loc[winner_idx, 'rationale_sufficiency'])\n",
    "    mlflow.log_metric(\"latency_p50_ms\", comparison_df.loc[winner_idx, 'latency_p50_ms'])\n",
    "    mlflow.log_metric(\"average_pass_rate\", comparison_df.loc[winner_idx, 'average_pass_rate'])\n",
    "    \n",
    "    # Save comparison df as artifact\n",
    "    comparison_df.to_csv(\"model_comparison.csv\", index=False)\n",
    "    mlflow.log_artifact(\"model_comparison.csv\")\n",
    "    \n",
    "    winning_run_id = run.info.run_id\n",
    "\n",
    "print(f\"\\n✅ Winning configuration logged to MLflow (run_id: {winning_run_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "next-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📋 NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n1. Use the winning model in your deployment:\")\n",
    "print(f\"   - Model: {winner_name}\")\n",
    "print(f\"   - Endpoint: {winner_endpoint}\")\n",
    "print(f\"\\n2. Deploy to production:\")\n",
    "print(f\"   - Open: ../../stages/complaint_agent.ipynb\")\n",
    "print(f\"   - Update LLM_MODEL widget to: {winner_endpoint}\")\n",
    "print(f\"   - Run full deployment workflow (log, register, deploy to Model Serving)\")\n",
    "print(f\"\\n3. Continue monitoring:\")\n",
    "print(f\"   - Use the aligned judge ({CATALOG}.ai.evidence_groundedness_judge_aligned)\")\n",
    "print(f\"   - Monitor production traffic with same scorers\")\n",
    "print(f\"   - Collect more human feedback to further improve the judge\")\n",
    "print(f\"\\n4. Iterate:\")\n",
    "print(f\"   - As new models become available, re-run this comparison notebook\")\n",
    "print(f\"   - Update evaluation dataset with new complaint patterns\")\n",
    "print(f\"   - Refine scorers based on production learnings\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n🎯 You've successfully compared {len(models_to_test)} models and identified the best one!\")\n",
    "print(f\"   The {winner_name} model achieved an average pass rate of {comparison_df.loc[winner_idx, 'average_pass_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ✅ Lightweight agent definition for rapid iteration (no deployment needed)\n",
    "- ✅ Systematic model comparison using MLflow evaluation\n",
    "- ✅ Agent-as-a-Judge for sophisticated trace-based evaluation\n",
    "- ✅ Human-in-the-loop feedback collection via MLflow UI\n",
    "- ✅ Judge alignment to improve evaluation accuracy\n",
    "- ✅ Clear visualization of model trade-offs\n",
    "- ✅ Structured decision-making for model selection\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. Inner-loop development (without deployment) enables fast model comparison\n",
    "2. Agent-as-a-Judge can inspect execution traces for deeper evaluation\n",
    "3. Human feedback significantly improves judge accuracy (30-50% improvement typical)\n",
    "4. Different models have different trade-offs (accuracy vs latency vs cost)\n",
    "5. Once you've selected a model, deployment is straightforward using the stages notebook\n",
    "\n",
    "**What's Next:**\n",
    "Deploy your winning model using the full production workflow in `../../stages/complaint_agent.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
