{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Choose the Best Model for Your Agent\n",
    "\n",
    "Databricks provides native access to a wide range of major AI model families, including ChatGPT, Claude, Gemini, Llama, and more. MLflow on Databricks supports sophisticated model evaluation and comparison workflows, including trace-aware, human-aligned agentic evaluation.\n",
    "\n",
    "In this notebook, we will explore some of the ways Databricks and MLflow empower you to rigorously iterate on the quality of your agent and to choose the best model for your use case. In particular, we will show how to use a trace-aware agentic judge and a human-aligned template-based judge to evaluate and compare different models.\n",
    "\n",
    "**Scenario:** You've built a prototype complaint triage agent for Casper's Kitchens, a ghost kitchen network. You want to know which AI model should power your production agent.\n",
    "\n",
    "**What you'll do:**\n",
    "1. Define a prototype agent for customer complaint triage\n",
    "2. Create an evaluation dataset with diverse complaint scenarios\n",
    "3. Evaluate different models using agentic- and template-based scorers\n",
    "4. Review results and add human feedback via MLflow UI\n",
    "5. Align the judge with human feedback to improve accuracy\n",
    "6. Re-evaluate and visualize results\n",
    "7. Register the winning model configuration\n",
    "\n",
    "**Prerequisites:** This demo assumes UC tools have been created by running `stages/complaint_agent.ipynb` and that you have run `stages/raw_data.ipynb` and `stages/lakeflow.ipynb` to start the raw data stream. See the [README](../../../README.md) for details on setting up the Casper's environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "## 1. Setup & Prerequisites\n",
    "\n",
    "Install required packages and verify that UC tools exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {},
   "outputs": [],
   "source": "%pip install -U -qqqq mlflow[databricks] dspy-ai unitycatalog-openai[databricks] pydantic\n%restart_python"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-catalog",
   "metadata": {},
   "outputs": [],
   "source": "# Create widget for catalog parameter\ndbutils.widgets.text(\"CATALOG\", \"caspersdev\", \"UC Catalog\")\n\nCATALOG = dbutils.widgets.get(\"CATALOG\")\nif not CATALOG:\n    raise ValueError(\"Please provide a CATALOG name in the widget above\")\nprint(f\"Using catalog: {CATALOG}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify UC tools exist\n",
    "print(\"Checking for required UC functions...\")\n",
    "\n",
    "required_functions = ['get_order_overview', 'get_order_timing', 'get_location_timings']\n",
    "\n",
    "for func_name in required_functions:\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE FUNCTION {CATALOG}.ai.{func_name}\").collect()\n",
    "    except Exception:\n",
    "        raise RuntimeError(\"UC tools missing. Run ../../stages/complaint_agent.ipynb first to create these tools.\")\n",
    "\n",
    "print(\"\\n✅ All required UC functions found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow experiment\n",
    "\n",
    "import mlflow\n",
    "\n",
    "experiment_name = f\"/Shared/{CATALOG}_model_comparison\"\n",
    "experiment = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment.experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": "## 2. Define Agent\n\nThe cell below defines a DSPy version of the complaint agent used in the [complaint_agent.ipynb](../../stages/complaint_agent.ipynb) notebook. The agent uses DSPy's ReAct module for automatic tool orchestration and can call Unity Catalog functions to retrieve order details, delivery timing, and location timings.\n\nIn this scenario, imagine that we are earlier in the prototyping phase and we want to quickly iterate on the agent's design. We are making fundamental design choices like which models to use and how to structure the agent's prompt.\n\nThis version of the complaint agent takes `model_endpoint` as a parameter, enabling us to easily swap between different models for comparison. Beyond that, the specific implementation details are largely unimportant for the purposes of this demo: you can use the same general evaluation and comparison principles regardless of the underlying agent implementation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-agent-class",
   "metadata": {},
   "outputs": [],
   "source": "from typing import Optional, Literal\nfrom pydantic import BaseModel, Field, field_validator, ValidationError\n\nimport dspy\nfrom unitycatalog.ai.core.base import get_uc_function_client\n\n# Initialize UC function client\nuc_client = get_uc_function_client()\n\n# Define response schema (same as production)\nclass ComplaintResponse(BaseModel):\n    \"\"\"Structured output for complaint triage decisions.\"\"\"\n    order_id: str\n    complaint_category: Literal[\"delivery_delay\", \"missing_items\", \"food_quality\", \"service_issue\", \"billing\", \"other\"] = Field(\n        description=\"Exactly ONE primary complaint category\"\n    )\n    decision: Literal[\"suggest_credit\", \"escalate\"]\n    credit_amount: Optional[float] = None\n    confidence: Optional[Literal[\"high\", \"medium\", \"low\"]] = None\n    priority: Optional[Literal[\"standard\", \"urgent\"]] = None\n    rationale: str\n    \n    @field_validator('complaint_category', mode='before')\n    @classmethod\n    def parse_category(cls, v):\n        \"\"\"Extract first valid category if multiple provided.\"\"\"\n        if not isinstance(v, str):\n            return v\n            \n        valid_categories = [\"delivery_delay\", \"missing_items\", \"food_quality\", \"service_issue\", \"billing\", \"other\"]\n        v_lower = v.lower().strip()\n        \n        # Exact match\n        if v_lower in valid_categories:\n            return v_lower\n        \n        # Find first valid category in string\n        for cat in valid_categories:\n            if cat in v_lower:\n                return cat\n        \n        return \"other\"\n    \n    @field_validator('confidence', mode='before')\n    @classmethod\n    def parse_confidence(cls, v):\n        \"\"\"Ensure valid confidence value.\"\"\"\n        if v is None or (isinstance(v, str) and v.lower() == \"null\"):\n            return None\n        if isinstance(v, str):\n            v_lower = v.lower().strip()\n            if v_lower in [\"high\", \"medium\", \"low\"]:\n                return v_lower\n            return \"medium\"\n        return v\n    \n    @field_validator('priority', mode='before')\n    @classmethod\n    def parse_priority(cls, v):\n        \"\"\"Ensure valid priority value.\"\"\"\n        if v is None or (isinstance(v, str) and v.lower() == \"null\"):\n            return None\n        if isinstance(v, str):\n            v_lower = v.lower().strip()\n            if v_lower in [\"standard\", \"urgent\"]:\n                return v_lower\n            return \"standard\"\n        return v\n\n\nclass ComplaintTriage(dspy.Signature):\n    \"\"\"Analyze customer complaints for Casper's Kitchens and recommend triage actions.\n    \n    Process:\n    1. Extract order_id from complaint\n    2. Use get_order_overview(order_id) for order details and items\n    3. Use get_order_timing(order_id) for delivery timing\n    4. For delays, use get_location_timings(location) for percentile benchmarks\n    5. Make data-backed decision\n    \n    Decision Framework:\n    \n    SUGGEST_CREDIT (with credit_amount and confidence):\n    - Delivery delays: Compare actual delivery time to location percentiles\n      * <P75: Suggest $0 credit (low confidence - on-time or minimal delay)\n      * P75-P99: Suggest 15% of order total (medium to high confidence)\n      * >P99: Suggest 25% of order total (high confidence)\n    - Missing items: Use actual item prices from order data when available\n      * Verify claimed item exists in order (affects confidence)\n      * Use real costs from order data, or estimate $8-12 per item if unavailable\n    - Food quality: 20-40% of order total based on severity\n      * Minor issues (slightly cold, minor preparation issue): 20% (medium confidence)\n      * Major issues (completely inedible, wrong preparation, health concern): 40% (high confidence)\n      * Vague complaints (\"bad\", \"gross\"): escalate instead\n    \n    ESCALATE (with priority):\n    - priority=\"standard\": Vague complaints, missing data, billing issues, service complaints\n    - priority=\"urgent\": Legal threats, health/safety concerns, suspected fraud, abusive language\n    \n    Output Requirements:\n    - For suggest_credit: credit_amount is REQUIRED and must be a number (can be 0.0 if no credit warranted), confidence is REQUIRED, priority must be null\n    - For escalate: priority is REQUIRED, credit_amount and confidence must be null\n    - complaint_category: Choose EXACTLY ONE category (the primary one)\n    - Rationale must cite specific evidence (delivery times, percentiles, item verification, order total)\n    - Rationale should be detailed but under 150 words\n    - Round credit amounts to nearest $0.50\n    - Confidence: high (strong data), medium (reasonable inference), low (weak/contradictory)\n    \"\"\"\n    \n    complaint: str = dspy.InputField(desc=\"Customer complaint text\")\n    order_id: str = dspy.OutputField(desc=\"Extracted order ID\")\n    complaint_category: str = dspy.OutputField(desc=\"EXACTLY ONE category: delivery_delay, missing_items, food_quality, service_issue, billing, or other\")\n    decision: str = dspy.OutputField(desc=\"EXACTLY ONE: suggest_credit or escalate\")\n    credit_amount: str = dspy.OutputField(desc=\"If suggest_credit: MUST be a number (e.g., 0.0, 10.5). If escalate: null\")\n    confidence: str = dspy.OutputField(desc=\"If suggest_credit: EXACTLY ONE of high, medium, low. If escalate: null\")\n    priority: str = dspy.OutputField(desc=\"If escalate: EXACTLY ONE of standard or urgent. If suggest_credit: null\")\n    rationale: str = dspy.OutputField(desc=\"Data-focused justification citing specific evidence\")\n\n\n# Unity Catalog tool wrappers\ndef get_order_overview(order_id: str) -> str:\n    \"\"\"Get order details including items, location, and customer info.\"\"\"\n    result = uc_client.execute_function(\n        f\"{CATALOG}.ai.get_order_overview\",\n        {\"oid\": order_id}\n    )\n    return str(result.value)\n\n\ndef get_order_timing(order_id: str) -> str:\n    \"\"\"Get timing information for a specific order.\"\"\"\n    result = uc_client.execute_function(\n        f\"{CATALOG}.ai.get_order_timing\",\n        {\"oid\": order_id}\n    )\n    return str(result.value)\n\n\ndef get_location_timings(location: str) -> str:\n    \"\"\"Get delivery time percentiles for a specific location.\"\"\"\n    result = uc_client.execute_function(\n        f\"{CATALOG}.ai.get_location_timings\",\n        {\"loc\": location}\n    )\n    return str(result.value)\n\n\nclass ComplaintTriageModule(dspy.Module):\n    \"\"\"DSPy module for complaint triage with tool calling.\"\"\"\n    \n    def __init__(self, model_endpoint: str):\n        super().__init__()\n        # Configure DSPy with the specified model endpoint\n        lm = dspy.LM(f'databricks/{model_endpoint}', max_tokens=2000)\n        dspy.configure(lm=lm)\n        \n        self.react = dspy.ReAct(\n            signature=ComplaintTriage,\n            tools=[get_order_overview, get_order_timing, get_location_timings],\n            max_iters=10\n        )\n    \n    def forward(self, complaint: str, max_retries: int = 2) -> ComplaintResponse:\n        \"\"\"Process complaint and return structured triage decision with retry on validation failure.\"\"\"\n        \n        for attempt in range(max_retries + 1):\n            try:\n                result = self.react(complaint=complaint)\n                \n                # Parse credit_amount\n                credit_amount = None\n                if result.credit_amount and result.credit_amount.lower() != \"null\":\n                    try:\n                        credit_amount = float(result.credit_amount)\n                    except (ValueError, TypeError):\n                        if result.decision == \"suggest_credit\":\n                            raise ValidationError(\"suggest_credit requires valid numeric credit_amount\")\n                \n                # Validate business rules before Pydantic construction\n                if result.decision == \"suggest_credit\" and credit_amount is None:\n                    raise ValidationError(\"suggest_credit requires credit_amount to be a number (can be 0.0)\")\n                \n                # Construct Pydantic model - field validators run here\n                return ComplaintResponse(\n                    order_id=result.order_id,\n                    complaint_category=result.complaint_category,\n                    decision=result.decision,\n                    credit_amount=credit_amount,\n                    confidence=result.confidence,\n                    priority=result.priority,\n                    rationale=result.rationale\n                )\n                \n            except (ValidationError, ValueError) as e:\n                if attempt < max_retries:\n                    # Retry - DSPy will regenerate with potentially different output\n                    continue\n                else:\n                    # Final attempt failed - re-raise\n                    raise\n\n\nclass ComplaintsAgentCore:\n    \"\"\"Lightweight complaint agent for model comparison using DSPy\"\"\"\n    \n    def __init__(self, model_endpoint: str, catalog: str):\n        self.model_endpoint = model_endpoint\n        self.catalog = catalog\n        global CATALOG\n        CATALOG = catalog\n        \n        # Build DSPy agent\n        self.agent = ComplaintTriageModule(model_endpoint=model_endpoint)\n    \n    def invoke(self, complaint: str) -> dict:\n        \"\"\"Process a complaint and return structured response\"\"\"\n        result = self.agent(complaint=complaint)\n        return result.model_dump()\n\nprint(\"✅ ComplaintsAgentCore class defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "test-agent-header",
   "metadata": {},
   "source": [
    "### Test the Agent\n",
    "\n",
    "Before creating the full evaluation dataset, let's validate that the agent works correctly with a single test case. We will:\n",
    "1. Retrieve a real order ID from the `all_events` table\n",
    "2. Query the agent with a delivery delay complaint and the order ID\n",
    "\n",
    "We will also enable MLflow autologging so we can review the agent's execution trace in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample order ID for testing\n",
    "sample_order = spark.sql(f\"\"\"\n",
    "    SELECT order_id \n",
    "    FROM {CATALOG}.lakeflow.all_events \n",
    "    WHERE event_type='delivered'\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0]['order_id']\n",
    "\n",
    "print(f\"Using test order ID: {sample_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-test",
   "metadata": {},
   "outputs": [],
   "source": "import mlflow\n\n# Enable autologging for DSPy\nmlflow.dspy.autolog()\n\n# Create agent instance with a test model\ntest_agent = ComplaintsAgentCore(\n    model_endpoint=\"databricks-gpt-5\",\n    catalog=CATALOG\n)\n\n# Test with a delivery delay complaint\ntest_complaint = f\"My order took forever to arrive! Order ID: {sample_order}\"\n\nresult = test_agent.invoke(test_complaint)\n\nprint(result)"
  },
  {
   "cell_type": "markdown",
   "id": "test-agent-output",
   "metadata": {},
   "source": "The DSPy agent successfully analyzed the complaint and returned a structured response. For this particular delivery delay complaint, the agent:\n- Categorized it as `delivery_delay`\n- Made a data-driven decision using the ReAct workflow to call UC tools\n- Provided detailed rationale citing specific evidence from tool outputs\n\nThis demonstrates DSPy's ability to automatically orchestrate tool calls, retrieve order data, compare against benchmarks, and make data-driven credit recommendations using the ReAct pattern."
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "## 3. Create Evaluation Dataset\n",
    "\n",
    "Now we will generate 15 diverse complaint scenarios to thoroughly test model performance across different situations:\n",
    "- Delivery delays (with varying severity)\n",
    "- Missing items (verifiable vs suspicious)\n",
    "- Food quality complaints (specific vs vague)\n",
    "- Service issues (should escalate)\n",
    "- Health/safety concerns (urgent escalation)\n",
    "- Edge cases (missing order ID, invalid format)\n",
    "\n",
    "A diverse dataset helps reveal model strengths and weaknesses across the decision space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-sample-orders",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get real order IDs for realistic eval scenarios\n",
    "all_order_ids = [\n",
    "    row['order_id'] for row in spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT order_id \n",
    "        FROM {CATALOG}.lakeflow.all_events \n",
    "        WHERE event_type='delivered'\n",
    "        LIMIT 15\n",
    "    \"\"\").collect()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-eval-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 15 diverse complaint scenarios (exactly one per order ID)\n",
    "templates = [\n",
    "    \"My order took forever to arrive! Order ID: {oid}\",\n",
    "    \"Order {oid} arrived late and cold\",\n",
    "    \"My falafel was completely soggy and inedible. Order: {oid}\",\n",
    "    \"The gyro meat was overcooked and dry. Order: {oid}\",\n",
    "    \"Everything tasted bad. Order {oid}\",\n",
    "    \"My entire falafel bowl was missing from the order! Order: {oid}\",\n",
    "    \"No drinks in my order {oid}\",\n",
    "    \"Your driver was extremely rude to me. Order: {oid}\",\n",
    "    \"This food made me sick, possible food poisoning. Order: {oid}\",\n",
    "    \"Order {oid} was late AND missing items AND cold!\",\n",
    "    \"The packaging was torn open when it arrived. Order: {oid}\",\n",
    "    \"The fries were completely soggy by the time they got here. Order: {oid}\",\n",
    "    \"I received someone else’s order by mistake. Order: {oid}\",\n",
    "    \"The sauce containers leaked all over the bag. Order: {oid}\",\n",
    "    \"The order was marked delivered but never showed up. Order: {oid}\",\n",
    "]\n",
    "\n",
    "# Map 1:1 (use each of the 15 IDs exactly once)\n",
    "complaints = [tmpl.format(oid=all_order_ids[i]) for i, tmpl in enumerate(templates)]\n",
    "\n",
    "# Format for mlflow.genai.evaluate\n",
    "eval_data = [{\"inputs\": {\"complaint\": c}} for c in complaints]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "## 4. Define [MLflow Scorers](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/)\n",
    "\n",
    "We use two complementary LLM judges with no overlap:\n",
    "\n",
    "1. **Evidence Groundedness (Agent-as-a-Judge with `{{ trace }}`)**: Inspects execution traces to verify decisions align with tool outputs. Uses MCP tools (GetSpan, ListSpans, etc.) to examine what data was actually returned by tool calls and whether the agent's rationale matches that evidence. Shows off agentic evaluation capabilities. **Not aligned** (alignment not yet supported for trace-aware judges).\n",
    "\n",
    "2. **Rationale Sufficiency (Template-based Judge)**: Evaluates if a human could understand the decision logic from the rationale alone. Uses `{{ inputs }}` and `{{ outputs }}` templates to assess clarity, completeness, and logical flow. **Will be aligned** with human feedback using SIMBA optimization.\n",
    "\n",
    "Both of these scorers use the `mlflow.genai.judges.make_judge` function to create [template-based scorers](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/make-judge/). There are several other ways to define scorers in MLflow, including a variety of [pre-defined scorers](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/predefined/), [guideline-based LLM scorers](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/guidelines/), and [code-based scorers](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/custom/).\n",
    "\n",
    "Note that, just like we can use different models for our agent, we can also use different models for our scorers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-scorers",
   "metadata": {},
   "outputs": [],
   "source": "from mlflow.genai.judges import make_judge\n\n# Scorer 1: Evidence Groundedness (Agent-as-a-Judge, Binary Pass/Fail)\n# Uses {{ trace }} to enable agentic evaluation with tool access\nevidence_groundedness_judge = make_judge(\n    name=\"evidence_groundedness\",\n    instructions=\"\"\"\nEvaluate whether the agent's decision is grounded in evidence from the execution {{ trace }}.\n\nInvestigation checklist:\n1. Find spans where tools were called (get_order_overview, get_order_timing, get_location_timings)\n2. Extract the actual outputs returned by these tool calls\n3. Compare tool outputs against claims made in the agent's rationale\n4. Verify that credit amounts or escalation decisions match the tool data\n\nFor delivery complaints, check:\n- Does the rationale's delivery time match what get_order_timing returned?\n- Does the rationale's percentile comparison match get_location_timings output?\n- Is the credit calculation based on the actual order total from get_order_overview?\n\nFor missing item complaints, check:\n- Does get_order_overview show the items mentioned in the rationale?\n- Is the credit amount based on actual item prices from the tool output?\n\nCustomer complaint: {{ inputs }}\nAgent's final output: {{ outputs }}\n\nRate as PASS if rationale claims match tool outputs, FAIL if there are contradictions or unsupported claims.\n\"\"\",\n    model=\"databricks:/databricks-gpt-5-mini\"\n)\n\n# Scorer 2: Rationale Sufficiency (Template Judge, Pass/Fail)\n# This judge will be aligned with human feedback\nrationale_sufficiency_judge = make_judge(\n    name=\"rationale_sufficiency\",\n    instructions=\"\"\"\nEvaluate whether the agent's rationale is sufficient to explain and justify the decision.\n\nCustomer complaint: {{ inputs }}\nAgent's output: {{ outputs }}\n\nCheck if a human reading the rationale can clearly understand:\n1. What decision was made (suggest_credit or escalate)\n2. Why that decision was appropriate for this complaint\n3. How any credit amount was determined (if applicable)\n\nFor credit decisions:\n- Does the rationale cite specific numbers (delivery time, percentiles, dollar amounts)?\n- Is there a clear logical connection between the evidence mentioned and the credit amount?\n- Would a human understand why this amount is fair?\n\nFor escalations:\n- Does the rationale explain why escalation is needed?\n- Is the priority level (standard/urgent) justified?\n- Would a human reviewer know what to investigate?\n\nRate as PASS if the rationale is clear, complete, and logically connects evidence to decision.\nRate as FAIL if the rationale is vague, missing key information, or logic is unclear.\n\"\"\",\n    model=\"databricks:/databricks-gemini-2-5-pro\"\n)"
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "## 5. Run Baseline Evaluation (Single Model)\n",
    "\n",
    "We will start by running an initial evaluation with one model to validate our eval setup and generate traces. Note that we run the evaluation in two phases:\n",
    "1. Run the agent to generate traces\n",
    "2. Retrieve the traces and evaluate them with the scorers\n",
    "\n",
    "We can then use the traces and initial evaluation results to align the `rationale_sufficiency` judge with human feedback.\n",
    "\n",
    "There are different approaches you can take to running evaluations, including passing a prediction function to `mlflow.genai.evaluate`. Working with a pre-generated trace dataset is simple and flexible and allows you to iterate on your judges without re-running the agent.\n",
    "\n",
    "We will tag the traces with the baseline model name so we can easily retrieve them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one model for initial evaluation and judge alignment\n",
    "initial_model_name = \"llama-3-3-70b\"\n",
    "initial_model_endpoint = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "baseline_tag = f\"baseline_{initial_model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-initial-eval",
   "metadata": {},
   "outputs": [],
   "source": "import mlflow\n\n# Enable autologging for detailed trace capture\nmlflow.dspy.autolog()\n\nprint(f\"Running {initial_model_name} on test cases to generate traces...\\n\")\n\n# Create agent instance\nagent = ComplaintsAgentCore(model_endpoint=initial_model_endpoint, catalog=CATALOG)\n\n# Step 1: Run agent to generate traces (tag them for later retrieval)\nwith mlflow.start_run(run_name=baseline_tag) as run:\n    experiment_id = run.info.experiment_id\n    run_id = run.info.run_id\n    \n    # Invoke agent on each complaint to generate traces\n    for row in eval_data:\n        complaint = row['inputs']['complaint']\n        result = agent.invoke(complaint)\n        \n        # Tag the trace for later retrieval\n        trace_id = mlflow.get_last_active_trace_id()\n        mlflow.set_trace_tag(trace_id, \"eval_group\", baseline_tag)\n        mlflow.set_trace_tag(trace_id, \"model\", initial_model_name)\n\nprint(f\"✅ Generated {len(eval_data)} traces. Run ID: {run_id}\")\n\n# Step 2: Retrieve traces for evaluation\nbaseline_traces = mlflow.search_traces(\n    experiment_ids=[experiment_id],\n    filter_string=f\"tags.eval_group = '{baseline_tag}'\",\n    max_results=15\n)\n\n\n# Step 3: Evaluate the traces\nprint(f\"\\nEvaluating traces with scorers...\")\nbaseline_result = mlflow.genai.evaluate(\n    data=baseline_traces,\n    scorers=[evidence_groundedness_judge, rationale_sufficiency_judge]\n)"
  },
  {
   "cell_type": "markdown",
   "id": "baseline-eval-output",
   "metadata": {},
   "source": [
    "In our run, the baseline evaluation:\n",
    "- Generated 15 traces tagged with `baseline_llama-3-3-70b`\n",
    "- Ran both judges (evidence_groundedness and rationale_sufficiency) on all traces\n",
    "- Completed successfully with results viewable in the MLflow UI Evaluation tab\n",
    "\n",
    "You can now review these traces and add human feedback to improve judge accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "## 6. Provide Human Assessments for Judge Alignment\n",
    "\n",
    "Now it's time to add human feedback to improve the judge's accuracy. To do so, navigate to the Evaluation tab in the MLflow UI and find the traces under the run generated above. You can add your feedback by clicking into each trace, then clicking the \"+\" next to the `rationale_sufficiency` scorer. Fill in your feedback for each trace (even if you agree with the initial assessment) along with your rationale, if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "## 7. Judge Alignment\n",
    "\n",
    "After collecting human feedback, we can align the judge to better match human judgment. The SIMBA optimizer analyzes disagreements between the judge and human assessments, then generates improved instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "align-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_traces = mlflow.search_traces(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=f\"tags.eval_group = '{baseline_tag}'\",\n",
    "    max_results=15,\n",
    "    return_type=\"list\"\n",
    ")\n",
    "\n",
    "\n",
    "# Get traces with human feedback for alignment\n",
    "traces_for_alignment = baseline_traces\n",
    "\n",
    "# Align the rationale_sufficiency judge with human feedback\n",
    "# Note: We align rationale_sufficiency (not evidence_groundedness) because alignment\n",
    "# is not yet supported for trace-aware judges using {{ trace }} \n",
    "## (source: https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/alignment/#quick-start-align-your-first-judge)\n",
    "\n",
    "\n",
    "aligned_judge = rationale_sufficiency_judge.align(traces_for_alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judge-alignment-output",
   "metadata": {},
   "source": [
    "During alignment, the SIMBA optimizer analyzed disagreements between the initial judge and human assessments, then generated improved instructions. The aligned judge now includes specific guidance learned from human feedback:\n",
    "\n",
    "**For missing item complaints:**\n",
    "- Focus solely on the specifics of that complaint\n",
    "- Clearly state what item is missing\n",
    "- Provide the item's price from the order\n",
    "- Explain how the credit amount is calculated based on that price\n",
    "- Avoid discussing irrelevant factors like delivery time unless they directly impact the decision\n",
    "\n",
    "**For all complaints:**\n",
    "- Ensure all statements are consistent and logically support the decision\n",
    "- Clarify why a credit is appropriate in the context of the complaint\n",
    "- Connect evidence explicitly to decisions\n",
    "\n",
    "The alignment process took approximately 2-3 minutes. The resulting aligned judge is better calibrated to human judgment and more focused on relevant complaint details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "## 8. Model Comparison with Aligned Judge\n",
    "\n",
    "Now that the judge is aligned with human feedback, compare multiple models to find the best one for your use case. Check out the list of models in the Databricks Model Serving tab to see the available models.\n",
    "\n",
    "Suppose we are interested in comparing the following models:\n",
    "- **databricks-claude-sonnet-4-5**\n",
    "- **databricks-gpt-5-mini**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-aligned-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models_to_compare = {\n",
    "    \"Claude Sonnet 4.5\": \"databricks-claude-sonnet-4-5\",\n",
    "    \"GPT-5 mini\": \"databricks-gpt-5-mini\"\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, endpoint in models_to_compare.items():\n",
    "    print(f\"\\nRunning {model_name} to generate traces...\")\n",
    "    \n",
    "    agent = ComplaintsAgentCore(model_endpoint=endpoint, catalog=CATALOG)\n",
    "    \n",
    "    # Step 1: Generate traces with tags\n",
    "    comparison_tag = f\"comparison_{model_name}\"\n",
    "    with mlflow.start_run(run_name=comparison_tag) as run:\n",
    "        # Invoke agent on each complaint to generate traces\n",
    "        for row in eval_data:\n",
    "            complaint = row['inputs']['complaint']\n",
    "            result = agent.invoke(complaint)\n",
    "            \n",
    "            # Tag the trace for later retrieval\n",
    "            trace_id = mlflow.get_last_active_trace_id()\n",
    "            mlflow.set_trace_tag(trace_id, \"eval_group\", comparison_tag)\n",
    "            mlflow.set_trace_tag(trace_id, \"model\", model_name)\n",
    "    \n",
    "    print(f\"  Generated {len(eval_data)} traces for {model_name}\")\n",
    "    \n",
    "    # Step 2: Retrieve traces\n",
    "    traces = mlflow.search_traces(\n",
    "        experiment_ids=[experiment_id],\n",
    "        filter_string=f\"tags.eval_group = '{comparison_tag}'\",\n",
    "        max_results=15\n",
    "    )\n",
    "    \n",
    "    # Step 3: Evaluate traces\n",
    "    print(f\"  Evaluating {len(traces)} traces with aligned judge...\")\n",
    "    result = mlflow.genai.evaluate(\n",
    "        data=traces,\n",
    "        scorers=[aligned_judge, evidence_groundedness_judge]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-comparison-output",
   "metadata": {},
   "source": [
    "For each model in the comparison:\n",
    "- Generated 15 new traces (one per complaint scenario)\n",
    "- Tagged traces for easy retrieval and organization\n",
    "- Evaluated with both the aligned rationale_sufficiency judge and the trace-aware evidence_groundedness judge\n",
    "- Results are available in MLflow UI for detailed analysis\n",
    "\n",
    "The comparison enables side-by-side evaluation of model performance on accuracy, rationale quality, and latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\n- Built a parameterized complaint triage agent using DSPy's ReAct module with UC tools.\n- Generated a concise eval set and ran a baseline to create traces.\n- Evaluated with two judges: evidence_groundedness (trace-aware, unaligned) and rationale_sufficiency (template judge, aligned via SIMBA with human feedback).\n- Compared multiple serving endpoints using the aligned judge and reviewed accuracy and latency trade-offs.\n\n**Key takeaways:**\n1. Databricks lets you use the models you prefer and provides first-class tools to build agents with UC functions, capture traces, and evaluate them rigorously with MLflow.\n2. DSPy's ReAct module provides automatic tool orchestration and structured outputs for building production-ready agents.\n3. Trace-aware judging verifies decisions against actual tool outputs; template judges can be aligned with human feedback for better agreement.\n4. Choose the endpoint that best balances quality, latency, and cost"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}