{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Chunking Strategies for Code RAG\n",
    "\n",
    "You've built a RAG system over your codebase. How do you know if your chunking strategy is working well? And how would you measure if a different approach would work better?\n",
    "\n",
    "This notebook demonstrates a systematic approach to evaluating chunking strategies using MLflow's genai evaluation framework. We compare three approaches:\n",
    "\n",
    "1. **Naive**: Fixed-size character chunks with overlap\n",
    "2. **Language-aware**: LangChain's `RecursiveCharacterTextSplitter.from_language()` with language-specific separators  \n",
    "3. **AST-based**: Tree-sitter parsing via the `astchunk` library with metadata headers showing file path and class/function hierarchy\n",
    "\n",
    "The methodology matters more than the specific results. Once you have a repeatable evaluation setup, you can apply it to any RAG decision: chunking strategy, embedding model, k value, reranking approach, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "source": "%pip install -U -qqq .\n%restart_python",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Literal\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lancedb\n",
    "import mlflow\n",
    "from mlflow.entities import Document, SpanType, Feedback\n",
    "from mlflow.genai.scorers import scorer  # Custom scorer decorator\n",
    "\n",
    "# DeepEval scorers (RAG-specific metrics)\n",
    "from mlflow.genai.scorers.deepeval import AnswerRelevancy, Faithfulness\n",
    "\n",
    "# MLflow built-in scorers\n",
    "from mlflow.genai.scorers import RetrievalSufficiency, Correctness\n",
    "\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from astchunk import ASTChunkBuilder\n",
    "\n",
    "# Configuration\n",
    "# NOTE: Must resolve() FIRST, then get parents - Path(\".\").parent returns \".\" not \"..\"\n",
    "CODEBASE_ROOT = Path(\".\").resolve().parent.parent  # /caspers-kitchens/\n",
    "EMBEDDING_MODEL = \"databricks-gte-large-en\"\n",
    "LLM_MODEL = \"databricks-gemini-3-flash\"\n",
    "\n",
    "JUDGE_MODEL = \"databricks:/databricks-claude-opus-4-5\"  # Format for scorers\n",
    "JUDGE_MODEL_NAME = \"databricks-claude-opus-4-5\"  # For direct API calls\n",
    "K_CHUNKS = 10\n",
    "\n",
    "print(f\"Codebase root: {CODEBASE_ROOT}\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(f\"Judge model: {JUDGE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Databricks SDK's `WorkspaceClient` for authentication, which provides an OpenAI-compatible client for calling Foundation Model APIs. This handles token refresh automatically and works both in Databricks notebooks (automatic auth) and locally (via config profile or environment variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up Databricks authentication and MLflow tracking\nfrom databricks.sdk import WorkspaceClient\n\n# =============================================================================\n# CONFIGURATION - Auto-detects environment\n# =============================================================================\nON_DATABRICKS = \"DATABRICKS_RUNTIME_VERSION\" in os.environ\nDATABRICKS_PROFILE = None  # None = auto-detect (notebook auth on DBX, env vars locally)\nUSE_DATABRICKS_MLFLOW = ON_DATABRICKS  # Log to Databricks workspace when running there\n# =============================================================================\n\n# Initialize Databricks client\nif DATABRICKS_PROFILE:\n    os.environ[\"DATABRICKS_CONFIG_PROFILE\"] = DATABRICKS_PROFILE\n    w = WorkspaceClient(profile=DATABRICKS_PROFILE)\nelse:\n    w = WorkspaceClient()\n\n# Set DATABRICKS_HOST for MLflow to resolve \"databricks:/model-name\" endpoints\nos.environ[\"DATABRICKS_HOST\"] = w.config.host\n\n# Create a personal access token for MLflow scorers (2 hour lifetime)\n# MLflow's internal HTTP client for \"databricks:/model-name\" endpoints requires\n# DATABRICKS_TOKEN in the environment, even when using OAuth profiles.\ntoken_response = w.tokens.create(\n    comment=\"MLflow chunking evaluation\",\n    lifetime_seconds=7200  # 2 hours\n)\nos.environ[\"DATABRICKS_TOKEN\"] = token_response.token_value\nprint(f\"Created Databricks token (expires in 2 hours)\")\nprint(f\"Databricks host: {w.config.host}\")\n\n# Disable OpenAI autolog to prevent traces from custom scorer calls\nmlflow.openai.autolog(disable=True)\n\n# Configure MLflow tracking location\nif USE_DATABRICKS_MLFLOW:\n    mlflow.set_tracking_uri(\"databricks\")\n    mlflow.set_experiment(\"/Shared/chunking-strategy-comparison\")\n    print(\"MLflow tracking: Databricks workspace\")\nelse:\n    # Local MLflow server (run `mlflow server` or `mlflow ui` first)\n    mlflow.set_tracking_uri(\"http://localhost:5000\")\n    mlflow.set_experiment(\"chunking-strategy-comparison\")\n    print(\"MLflow tracking: http://localhost:5000\")\n\n# Get OpenAI-compatible client for model serving\nclient = w.serving_endpoints.get_open_ai_client()\n\n# Rate limit MLflow evaluation to avoid 429 errors\nos.environ[\"MLFLOW_GENAI_EVAL_MAX_WORKERS\"] = \"2\"\nos.environ[\"MLFLOW_GENAI_EVAL_MAX_SCORER_WORKERS\"] = \"2\"\nprint(\"MLflow rate limiting configured\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Codebase Files\n",
    "\n",
    "We index Python, Markdown, YAML, and Jupyter notebook files while excluding generated artifacts, dependencies, and documentation that would add noise without helping answer code questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File patterns to include\n",
    "INCLUDE_EXTENSIONS = {\".py\", \".md\", \".yaml\", \".yml\", \".ipynb\"}\n",
    "\n",
    "# Directories/files to exclude (glob patterns)\n",
    "EXCLUDE_PATTERNS = [\n",
    "    \"docs/vendor/*\",\n",
    "    \".dbx-runs/*\",\n",
    "    \".beads/*\",\n",
    "    \".git/*\",\n",
    "    \"__pycache__/*\",\n",
    "    \"demos/*\",\n",
    "    \".databricks/*\",\n",
    "    \".venv/*\",\n",
    "    \"*.egg-info/*\",\n",
    "    \".pytest_cache/*\",\n",
    "    \"*/.terraform/*\",\n",
    "    \"node_modules/*\",\n",
    "    \"dist/*\",\n",
    "    \"build/*\",\n",
    "    \"dbx_execution/*\",\n",
    "    \"dbx_ai_docs/*\",\n",
    "    \".claude/*\",\n",
    "    \"CLAUDE.md\",\n",
    "    \"*/CLAUDE.md\",\n",
    "]\n",
    "\n",
    "import fnmatch\n",
    "\n",
    "def should_exclude(file_path: Path, root: Path) -> bool:\n",
    "    \"\"\"Check if a file should be excluded based on glob patterns.\"\"\"\n",
    "    rel_path = str(file_path.relative_to(root))\n",
    "    for pattern in EXCLUDE_PATTERNS:\n",
    "        if fnmatch.fnmatch(rel_path, pattern):\n",
    "            return True\n",
    "        # Also check each path component for directory patterns\n",
    "        parts = rel_path.split('/')\n",
    "        for i, part in enumerate(parts):\n",
    "            partial = '/'.join(parts[:i+1])\n",
    "            if fnmatch.fnmatch(partial, pattern.rstrip('/*')):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def discover_files(root: Path) -> list[Path]:\n",
    "    \"\"\"Discover all files to process.\"\"\"\n",
    "    files = []\n",
    "    for ext in INCLUDE_EXTENSIONS:\n",
    "        for file_path in root.rglob(f\"*{ext}\"):\n",
    "            if not should_exclude(file_path, root):\n",
    "                files.append(file_path)\n",
    "    return sorted(files)\n",
    "\n",
    "# Discover files\n",
    "files = discover_files(CODEBASE_ROOT)\n",
    "print(f\"Found {len(files)} files to process\")\n",
    "\n",
    "# Show breakdown by extension\n",
    "ext_counts = {}\n",
    "for f in files:\n",
    "    ext = f.suffix.lower()\n",
    "    ext_counts[ext] = ext_counts.get(ext, 0) + 1\n",
    "print(\"\\nFiles by extension:\")\n",
    "for ext, count in sorted(ext_counts.items()):\n",
    "    print(f\"  {ext}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebooks pose a challenge for code chunking since their JSON structure doesn't parse well. We convert them to Python format with cell markers (`# %% [code]`), which lets all three chunking strategies process notebook code the same way they handle regular Python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert notebooks to .py format for better chunking\n",
    "import nbformat\n",
    "\n",
    "def notebook_to_python(content: str) -> str:\n",
    "    \"\"\"Convert .ipynb to .py format with cell markers.\n",
    "    \n",
    "    - Code cells: # %% [code] followed by code\n",
    "    - Markdown cells: # %% [markdown] with each line commented\n",
    "    \n",
    "    This allows AST chunking to parse notebook code cells properly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nb = nbformat.reads(content, as_version=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse notebook: {e}\")\n",
    "        return content  # Return raw content if parsing fails\n",
    "    \n",
    "    lines = []\n",
    "    for cell in nb.cells:\n",
    "        source = cell.source.strip()\n",
    "        if not source:\n",
    "            continue\n",
    "            \n",
    "        if cell.cell_type == 'markdown':\n",
    "            lines.append('# %% [markdown]')\n",
    "            for line in source.split('\\n'):\n",
    "                lines.append(f'# {line}')\n",
    "        else:\n",
    "            lines.append('# %% [code]')\n",
    "            lines.append(source)\n",
    "        lines.append('')  # Blank line between cells\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "# Load file contents, converting notebooks to .py format\n",
    "file_contents = {}\n",
    "for file_path in files:\n",
    "    try:\n",
    "        content = file_path.read_text(encoding=\"utf-8\")\n",
    "        if content.strip():\n",
    "            # Convert notebooks to Python format\n",
    "            if file_path.suffix.lower() == '.ipynb':\n",
    "                content = notebook_to_python(content)\n",
    "            file_contents[file_path] = content\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read {file_path}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(file_contents)} files with content\")\n",
    "print(f\"  (Notebooks converted to .py format with cell markers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement Chunking Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare three chunking strategies that represent increasing levels of code awareness:\n",
    "\n",
    "1. **Naive** splits text at fixed character intervals, ignoring code structure entirely\n",
    "2. **Language-aware** uses heuristic separators (class/function boundaries) but enforces strict size limits\n",
    "3. **AST-based** parses code into a syntax tree and chunks at semantic boundaries, with metadata headers\n",
    "\n",
    "All strategies use the same target chunk size (1000 chars) and overlap (200 chars) for fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Naive Chunking (Baseline)\n",
    "\n",
    "The naive approach treats code as plain text, splitting at fixed character intervals with overlap. This is the simplest strategy but often cuts through function names, string literals, or logical blocks, losing semantic context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "overlap = 200\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"A single chunk with content and metadata.\"\"\"\n",
    "    content: str\n",
    "    file_path: str\n",
    "    strategy: str\n",
    "    chunk_index: int\n",
    "    file_type: str\n",
    "    char_start: int = 0\n",
    "    char_end: int = 0\n",
    "\n",
    "def chunk_naive(content: str, file_path: str, chunk_size: int = chunk_size, overlap: int = overlap) -> list[Chunk]:\n",
    "    \"\"\"Fixed-size character chunks with no language awareness.\n",
    "    \n",
    "    Uses same chunk_size and overlap as middleground for fair comparison.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    file_type = Path(file_path).suffix.lower().lstrip(\".\")\n",
    "    \n",
    "    for i in range(0, len(content), chunk_size - overlap):\n",
    "        chunk_text = content[i:i + chunk_size]\n",
    "        if chunk_text.strip():\n",
    "            chunks.append(Chunk(\n",
    "                content=chunk_text,\n",
    "                file_path=file_path,\n",
    "                strategy=\"naive\",\n",
    "                chunk_index=len(chunks),\n",
    "                file_type=file_type,\n",
    "                char_start=i,\n",
    "                char_end=min(i + chunk_size, len(content))\n",
    "            ))\n",
    "    return chunks\n",
    "\n",
    "# Test naive chunking\n",
    "test_file = list(file_contents.keys())[0]\n",
    "test_chunks = chunk_naive(file_contents[test_file], str(test_file.relative_to(CODEBASE_ROOT)))\n",
    "print(f\"Naive chunking test: {len(test_chunks)} chunks from {test_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Middleground Chunking (LangChain Language-Aware)\n",
    "\n",
    "LangChain's `RecursiveCharacterTextSplitter.from_language()` uses language-specific separators (like `\\nclass ` and `\\ndef ` for Python) to prefer splitting at logical boundaries. Unlike naive chunking, it tries to keep functions and classes intact. However, it still enforces strict size limits, so large functions get split mid-body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language-aware splitters using LangChain's from_language()\n",
    "\n",
    "chunk_size = 1000\n",
    "overlap = 200\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    ")\n",
    "\n",
    "markdown_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    ")\n",
    "\n",
    "# Generic splitter for YAML and other formats\n",
    "generic_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "def chunk_middleground(content: str, file_path: str) -> list[Chunk]:\n",
    "    \"\"\"Language-aware chunking using LangChain's from_language() splitters.\"\"\"\n",
    "    chunks = []\n",
    "    suffix = Path(file_path).suffix.lower()\n",
    "    \n",
    "    # Notebooks are pre-converted to .py format, treat as Python\n",
    "    if suffix in {\".py\", \".ipynb\"}:\n",
    "        split_texts = python_splitter.split_text(content)\n",
    "        file_type = \"python\" if suffix == \".py\" else \"notebook\"\n",
    "    elif suffix == \".md\":\n",
    "        split_texts = markdown_splitter.split_text(content)\n",
    "        file_type = \"markdown\"\n",
    "    elif suffix in {\".yaml\", \".yml\"}:\n",
    "        split_texts = generic_splitter.split_text(content)\n",
    "        file_type = \"yaml\"\n",
    "    else:\n",
    "        split_texts = generic_splitter.split_text(content)\n",
    "        file_type = \"unknown\"\n",
    "    \n",
    "    for i, text in enumerate(split_texts):\n",
    "        if text.strip():\n",
    "            chunks.append(Chunk(\n",
    "                content=text,\n",
    "                file_path=file_path,\n",
    "                strategy=\"middleground\",\n",
    "                chunk_index=i,\n",
    "                file_type=file_type,\n",
    "            ))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 AST-Based Chunking (ASTChunk)\n",
    "\n",
    "AST chunking uses tree-sitter to parse code into a syntax tree, then chunks at semantic boundaries (complete functions, classes, or statement blocks). Unlike the LangChain approach which uses regex-based heuristics, AST parsing understands actual code structure.\n",
    "\n",
    "The key difference is **chunk expansion**: each chunk gets a metadata header prepended showing the file path and class/function hierarchy. This context becomes part of the embedding, helping retrieval match queries like \"UCState.clear_all()\" to the right chunk even if those exact terms don't appear in the chunk body.\n",
    "\n",
    "```python\n",
    "'''\n",
    "utils/calculator.py\n",
    "class Calculator:\n",
    "'''\n",
    "    def add(self, a: int, b: int) -> int:\n",
    "        return a + b\n",
    "```\n",
    "\n",
    "AST chunking may also exceed the target chunk size when necessary to keep a complete function together, trading uniform chunk sizes for semantic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language mapping for ASTChunk\n",
    "# NOTE: .ipynb files are converted to .py format in cell-6, so treat them as Python\n",
    "AST_SUPPORTED = {\".py\": \"python\", \".ipynb\": \"python\", \".ts\": \"typescript\", \".tsx\": \"typescript\", \".js\": \"javascript\"}\n",
    "\n",
    "# AST chunking configuration\n",
    "AST_CHUNK_SIZE = 1000\n",
    "AST_OVERLAP = 1           # Number of AST nodes to overlap between chunks\n",
    "AST_EXPANSION = True      # Add filepath + class/function path headers to chunks\n",
    "\n",
    "def chunk_ast(content: str, file_path: str, max_chunk_size: int = AST_CHUNK_SIZE) -> list[Chunk]:\n",
    "    \"\"\"AST-aware chunking using tree-sitter.\n",
    "    \n",
    "    Uses chunk_overlap for bidirectional AST node overlap between chunks.\n",
    "    Uses chunk_expansion=True to prepend metadata headers with filepath and \n",
    "    ancestor path (class/function hierarchy) to each chunk for better retrieval.\n",
    "    \n",
    "    NOTE: Notebooks are pre-converted to .py format, so they get full AST treatment.\n",
    "    \"\"\"\n",
    "    suffix = Path(file_path).suffix.lower()\n",
    "    language = AST_SUPPORTED.get(suffix)\n",
    "    \n",
    "    if language is None:\n",
    "        # Fall back to middleground for unsupported languages\n",
    "        chunks = chunk_middleground(content, file_path)\n",
    "        for c in chunks:\n",
    "            c.strategy = \"ast_fallback\"\n",
    "        return chunks\n",
    "    \n",
    "    try:\n",
    "        builder = ASTChunkBuilder(\n",
    "            max_chunk_size=max_chunk_size,\n",
    "            language=language,\n",
    "            metadata_template=\"default\",\n",
    "        )\n",
    "        \n",
    "        # Pass overlap, expansion, and filepath metadata to chunkify()\n",
    "        raw_chunks = builder.chunkify(\n",
    "            content,\n",
    "            chunk_overlap=AST_OVERLAP,\n",
    "            chunk_expansion=AST_EXPANSION,\n",
    "            repo_level_metadata={\"filepath\": file_path}\n",
    "        )\n",
    "        \n",
    "        chunks = []\n",
    "        for i, c in enumerate(raw_chunks):\n",
    "            chunk_content = c[\"content\"] if isinstance(c, dict) else c.content\n",
    "            if chunk_content.strip():\n",
    "                chunks.append(Chunk(\n",
    "                    content=chunk_content,\n",
    "                    file_path=file_path,\n",
    "                    strategy=\"ast\",\n",
    "                    chunk_index=i,\n",
    "                    file_type=\"python\" if language == \"python\" else \"code\",\n",
    "                ))\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"AST parsing failed for {file_path}: {e}\")\n",
    "        # Fall back to middleground (not naive) for consistency\n",
    "        chunks = chunk_middleground(content, file_path)\n",
    "        for c in chunks:\n",
    "            c.strategy = \"ast_fallback\"\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate All Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_chunks(file_contents: dict, chunker_fn, strategy_name: str) -> list[Chunk]:\n",
    "    \"\"\"Generate chunks for all files using a given chunking function.\"\"\"\n",
    "    all_chunks = []\n",
    "    for file_path, content in file_contents.items():\n",
    "        rel_path = str(file_path.relative_to(CODEBASE_ROOT))\n",
    "        chunks = chunker_fn(content, rel_path)\n",
    "        all_chunks.extend(chunks)\n",
    "    return all_chunks\n",
    "\n",
    "# Generate chunks for all three strategies\n",
    "print(\"Generating naive chunks...\")\n",
    "naive_chunks = generate_all_chunks(file_contents, chunk_naive, \"naive\")\n",
    "print(f\"  {len(naive_chunks)} chunks\")\n",
    "\n",
    "print(\"Generating middleground chunks...\")\n",
    "middleground_chunks = generate_all_chunks(file_contents, chunk_middleground, \"middleground\")\n",
    "print(f\"  {len(middleground_chunks)} chunks\")\n",
    "\n",
    "print(\"Generating AST chunks...\")\n",
    "ast_chunks = generate_all_chunks(file_contents, chunk_ast, \"ast\")\n",
    "print(f\"  {len(ast_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare chunk statistics\n",
    "def chunk_stats(chunks: list[Chunk], name: str):\n",
    "    sizes = [len(c.content) for c in chunks]\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Total chunks: {len(chunks)}\")\n",
    "    print(f\"  Avg size: {np.mean(sizes):.0f} chars\")\n",
    "    print(f\"  Min/Max size: {min(sizes)}/{max(sizes)} chars\")\n",
    "    print(f\"  Total chars: {sum(sizes):,}\")\n",
    "\n",
    "chunk_stats(naive_chunks, \"Naive\")\n",
    "chunk_stats(middleground_chunks, \"Middleground\")\n",
    "chunk_stats(ast_chunks, \"AST-based\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Vector Indexes\n",
    "\n",
    "To compare chunking strategies fairly, we embed each strategy's chunks using the same embedding model (`databricks-gte-large-en`) and store them in separate vector indexes. We delete and recreate indexes on each run to ensure no stale data affects results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts: list[str], batch_size: int = 20) -> list[list[float]]:\n",
    "    \"\"\"Get embeddings using databricks-gte-large-en.\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        response = w.serving_endpoints.query(\n",
    "            name=EMBEDDING_MODEL,\n",
    "            input=batch\n",
    "        )\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# Test embedding to verify connection\n",
    "test_emb = get_embeddings([\"Hello world\"])\n",
    "print(f\"✓ Embedding test passed, dimension: {len(test_emb[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use LanceDB as a lightweight local vector store. It requires no server setup and stores data in the local directory, making the notebook self-contained and easy to run anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import shutil\n\n# LanceDB path: /tmp on Databricks (Workspace FS doesn't support atomic rename), local dir otherwise\nLANCEDB_PATH = \"/tmp/lancedb\" if ON_DATABRICKS else \"./lancedb\"\n\ndef create_lancedb_index(chunks: list[Chunk], strategy_name: str) -> lancedb.table.Table:\n    \"\"\"Create LanceDB index for a chunking strategy.\n    \n    Always overwrites existing tables to ensure fresh data.\n    \"\"\"\n    print(f\"Creating index for {strategy_name}...\")\n    \n    # Get embeddings\n    texts = [c.content for c in chunks]\n    print(f\"  Embedding {len(texts)} chunks...\")\n    embeddings = get_embeddings(texts)\n    \n    # Prepare data for LanceDB\n    data = [\n        {\n            \"content\": c.content,\n            \"file_path\": c.file_path,\n            \"strategy\": c.strategy,\n            \"chunk_index\": c.chunk_index,\n            \"file_type\": c.file_type,\n            \"vector\": emb\n        }\n        for c, emb in zip(chunks, embeddings)\n    ]\n    \n    # Create table (mode=\"overwrite\" ensures fresh data)\n    db = lancedb.connect(LANCEDB_PATH)\n    table = db.create_table(f\"chunks_{strategy_name}\", data, mode=\"overwrite\")\n    print(f\"  Created table with {len(data)} rows\")\n    \n    return table\n\n# Delete existing LanceDB to ensure fresh indexes\nif Path(LANCEDB_PATH).exists():\n    shutil.rmtree(LANCEDB_PATH)\n    print(f\"Deleted existing LanceDB at {LANCEDB_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indexes for all three strategies\n",
    "naive_index = create_lancedb_index(naive_chunks, \"naive\")\n",
    "middleground_index = create_lancedb_index(middleground_chunks, \"middleground\")\n",
    "ast_index = create_lancedb_index(ast_chunks, \"ast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define RAG Pipeline\n",
    "\n",
    "The RAG pipeline has two steps: (1) embed the query and retrieve the top-k most similar chunks from the vector index, (2) pass those chunks as context to an LLM to generate an answer. We use MLflow tracing to capture the full pipeline execution for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rag_pipeline(index: lancedb.table.Table):\n",
    "    \"\"\"Create a RAG pipeline function bound to a specific index.\"\"\"\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.RETRIEVER)\n",
    "    def retrieve(query: str) -> list[Document]:\n",
    "        \"\"\"Retrieve relevant chunks and return as MLflow Document objects.\"\"\"\n",
    "        query_emb = get_embeddings([query])[0]\n",
    "        results = index.search(query_emb).limit(K_CHUNKS).to_list()\n",
    "        return [\n",
    "            Document(\n",
    "                page_content=r[\"content\"],\n",
    "                metadata={\"doc_uri\": r[\"file_path\"], \"chunk_id\": r[\"chunk_index\"]}\n",
    "            )\n",
    "            for r in results\n",
    "        ]\n",
    "    \n",
    "    @mlflow.trace(name=\"generate\", span_type=SpanType.LLM)\n",
    "    def generate(context: str, question: str) -> str:\n",
    "        \"\"\"Generate answer from retrieved context.\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Based on this code context, answer concisely:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "            }],\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        # Handle Gemini's structured response format\n",
    "        if isinstance(content, list):\n",
    "            return \"\".join(block.get(\"text\", \"\") for block in content if isinstance(block, dict))\n",
    "        return content\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.CHAIN)\n",
    "    def predict(question: str) -> str:\n",
    "        \"\"\"Full RAG pipeline: retrieve then generate.\"\"\"\n",
    "        docs = retrieve(question)\n",
    "        context = \"\\n\\n---\\n\\n\".join([\n",
    "            f\"File: {d.metadata['doc_uri']}\\n{d.page_content}\" \n",
    "            for d in docs\n",
    "        ])\n",
    "        return generate(context, question)\n",
    "    \n",
    "    return predict\n",
    "\n",
    "print(\"✓ RAG pipeline factory defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG pipeline\n",
    "test_predict = make_rag_pipeline(middleground_index)\n",
    "test_answer = test_predict(\"What is the purpose of the complaint agent?\")\n",
    "print(\"Test answer:\")\n",
    "print(test_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "To measure which chunking strategy works best, we need two things: a set of questions with expected answers, and scorers that assess answer quality. MLflow's genai evaluation framework handles the orchestration, running each question through the RAG pipeline and applying scorers to the results.\n",
    "\n",
    "### 6.1 Evaluation Dataset\n",
    "\n",
    "We created 24 questions across different categories (conceptual, code examples, architecture, how-to, troubleshooting) and difficulty levels. The code example questions are the most demanding since they require retrieving specific implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation questions about Casper's Kitchens codebase\n",
    "# Categories: Business User, Developer, Demo Presenter\n",
    "# Complexity: Easy (factual), Medium (architecture/how-to), Hard (code examples)\n",
    "\n",
    "eval_data = [\n",
    "    # === BUSINESS USER - CONCEPTUAL (4 questions) ===\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"What is Casper's Kitchens and what problem does it solve?\"},\n",
    "     \"expectations\": {\"expected_response\": \"Casper's Kitchens is a demonstration platform simulating a ghost kitchen food delivery service. It showcases end-to-end Databricks capabilities including data pipelines, AI agents, streaming, and apps.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"What AI agents are included in the project and what business decisions does each one help with?\"},\n",
    "     \"expectations\": {\"expected_response\": \"Two AI agents: (1) Refund Recommender Agent - recommends whether to approve refund requests, (2) Complaint Agent - triages customer complaints and decides whether to suggest credit or escalate to human review.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"Walk me through the end-to-end flow of how a customer complaint gets processed.\"},\n",
    "     \"expectations\": {\"expected_response\": \"A scheduled job generates synthetic complaints from delivered orders. These complaints are processed by a DSPy ReAct agent that retrieves order context using Unity Catalog functions, then makes a triage decision to either suggest a credit amount or escalate to human review.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"What types of data does the project generate and process?\"},\n",
    "     \"expectations\": {\"expected_response\": \"The project generates simulated order events (order_created, delivery tracking), refund requests, and customer complaints. Data flows through bronze/silver/gold medallion layers with Spark transformations.\"}},\n",
    "    \n",
    "    # === CODE EXAMPLES - HARD (6 questions) ===\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"Show the deletion order used by UCState.clear_all() and explain why this ordering matters.\"},\n",
    "     \"expectations\": {\"expected_response\": \"deletion_order = ['experiments', 'jobs', 'pipelines', 'endpoints', 'apps', 'warehouses', 'databasecatalogs', 'catalogs', 'databaseinstances']. This order matters because resources may depend on each other - you must delete dependent resources before their dependencies.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"How does the resolve_with_parents function calculate depth for topological sorting of task dependencies?\"},\n",
    "     \"expectations\": {\"expected_response\": \"Uses a recursive depth() function with memoization: d = 0 if not parents else 1 + max(depth(p) for p in parents). Tasks are sorted by (depth(n), n).\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"Show the JSON recovery logic in parse_agent_response that handles trailing junk after valid JSON.\"},\n",
    "     \"expectations\": {\"expected_response\": \"If direct json.loads fails and '}' is in the string, tries: obj = json.loads(s[: s.rfind('}') + 1]) to trim to the last closing brace.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"What Spark transformations are applied in silver_order_items to normalize order events into item-level rows?\"},\n",
    "     \"expectations\": {\"expected_response\": \"filter for order_created → from_json to parse body → explode items array → withColumn extended_price = price * qty → to_date for order_day partitioning\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"How does CaspersDataSource.read() differentiate between initial historical catchup and subsequent streaming runs?\"},\n",
    "     \"expectations\": {\"expected_response\": \"Checks is_initial flag from offset. If True, outputs all data from day 0 to START_DAY + current time without speed multiplier. Otherwise, uses elapsed_real_seconds * speed_multiplier.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"Show the task dependencies for Complaint_Agent_Stream in the databricks.yml complaints target.\"},\n",
    "     \"expectations\": {\"expected_response\": \"depends_on: [Complaint_Agent, Complaint_Generator_Stream]. Runs after both the agent is deployed and the complaint generator stream is running.\"}},\n",
    "    \n",
    "    # === ARCHITECTURE - MEDIUM (5 questions) ===\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"What is the overall structure of the Casper's Kitchens data pipeline?\"},\n",
    "     \"expectations\": {\"expected_response\": \"The pipeline includes Raw_Data ingestion, Spark_Declarative_Pipeline for Lakeflow processing, Refund_Recommender_Agent, streaming refund processing, Complaint_Agent, complaint streaming, Lakebase integration, Reverse ETL, and Databricks App deployment.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"What is the purpose of the UCState module and how does it track resources?\"},\n",
    "     \"expectations\": {\"expected_response\": \"UCState manages Databricks resources by storing their state in a Unity Catalog table. It tracks experiments, jobs, pipelines, endpoints, apps, warehouses, and catalogs.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"How does the DLT pipeline transform raw events into gold-level aggregates?\"},\n",
    "     \"expectations\": {\"expected_response\": \"Raw events flow through bronze (raw storage) → silver (filtered, parsed, exploded to item level with extended_price) → gold (aggregated per-order metrics like revenue, total_qty, total_items).\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"What is the role of the Databricks App in this project?\"},\n",
    "     \"expectations\": {\"expected_response\": \"The Databricks App (refund-manager) provides a web interface for managing refund requests and viewing complaint data.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"How does the streaming simulation work with time multipliers?\"},\n",
    "     \"expectations\": {\"expected_response\": \"CaspersDataSource uses a speed_multiplier to accelerate simulation time. Initial run outputs historical data, subsequent runs calculate elapsed_sim_seconds = elapsed_real_seconds * speed_multiplier.\"}},\n",
    "    \n",
    "    # === HOW-TO / DEPLOYMENT - MEDIUM (4 questions) ===\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"How do I deploy the Casper's Kitchens job using the Databricks CLI?\"},\n",
    "     \"expectations\": {\"expected_response\": \"Use 'databricks bundle deploy' to deploy the bundle, then 'databricks bundle run caspers' to run the main job.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"How do I validate the Databricks bundle configuration?\"},\n",
    "     \"expectations\": {\"expected_response\": \"Run 'databricks bundle validate' from the project root directory to check the databricks.yml configuration.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"What are the different deployment targets available and when would I use each one?\"},\n",
    "     \"expectations\": {\"expected_response\": \"Targets include: default (full pipeline with all components), complaints (complaint handling flow only), and free (Databricks Free Edition compatible). Use complaints target for focused demos, free target for free-tier workspaces.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"I want to add a new AI agent to the pipeline. What files would I need to create and how do I wire it into databricks.yml?\"},\n",
    "     \"expectations\": {\"expected_response\": \"Create a notebook in stages/ with your agent logic, then add a new task in databricks.yml under the tasks section with notebook_task pointing to your notebook and appropriate depends_on for task ordering.\"}},\n",
    "    \n",
    "    # === TROUBLESHOOTING - MEDIUM/HARD (2 questions) ===\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"My job failed with a task dependency error saying 'Complaint_Agent_Stream' couldn't start. What tasks does it depend on and what should I check?\"},\n",
    "     \"expectations\": {\"expected_response\": \"Complaint_Agent_Stream depends on Complaint_Agent and Complaint_Generator_Stream. Check that both predecessor tasks completed successfully.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"UCState.clear_all() failed partway through cleanup. Looking at the deletion order, what resources might be left behind?\"},\n",
    "     \"expectations\": {\"expected_response\": \"Deletion order is experiments, jobs, pipelines, endpoints, apps, warehouses, databasecatalogs, catalogs, databaseinstances. Resources after the failure point in this order would remain.\"}},\n",
    "    \n",
    "    # === FACTUAL - EASY (3 questions) ===\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"What framework is used for the complaint agent and what pattern does it implement?\"},\n",
    "     \"expectations\": {\"expected_response\": \"DSPy framework is used with a ReAct (Reasoning and Acting) pattern.\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"What is the default complaint rate parameter for the complaint generator?\"},\n",
    "     \"expectations\": {\"expected_response\": \"0.15 (15% of orders generate complaints)\"}},\n",
    "    \n",
    "    {\"inputs\": {\"question\": \"Can I run this project on a free-tier Databricks workspace?\"},\n",
    "     \"expectations\": {\"expected_response\": \"Yes, use the 'free' target when deploying the bundle.\"}},\n",
    "]\n",
    "\n",
    "print(f\"Evaluation dataset: {len(eval_data)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Scorers\n",
    "\n",
    "We use three scorers to assess different aspects of RAG quality:\n",
    "\n",
    "- **RetrievalSufficiency** (built-in): Do the retrieved chunks contain enough information to answer the question? This is the key metric for comparing chunking strategies since it measures retrieval quality independent of generation.\n",
    "\n",
    "- **Correctness** (built-in): Does the answer match the expected response? This is strict and penalizes correct answers that use different phrasing.\n",
    "\n",
    "- **sufficient_answer** (custom): A lenient scorer we wrote that asks \"did the response answer the core question correctly?\" without requiring exact phrasing. This gives more actionable signal than strict correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@scorer\n",
    "def sufficient_answer(\n",
    "    *,\n",
    "    inputs: dict,\n",
    "    outputs: str,\n",
    "    expectations: dict,\n",
    ") -> Feedback:\n",
    "    \"\"\"Lenient correctness: Is the answer sufficient and grounded?\n",
    "    \n",
    "    Unlike built-in Correctness which requires exact phrasing match,\n",
    "    this scorer asks: \"Did the response answer the core question correctly?\"\n",
    "    \"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "    expected = expectations.get(\"expected_response\", \"\")\n",
    "\n",
    "    prompt = f\"\"\"Evaluate if the response sufficiently answers the question.\n",
    "Question: {question}\n",
    "Expected key points: {expected}\n",
    "Actual response: {outputs}\n",
    "\n",
    "Criteria: \n",
    "1. Does it address the core question?\n",
    "2. Are the key facts correct?\n",
    "3. Is it grounded in the expected information?\n",
    "\n",
    "Respond with ONLY \"yes\" or \"no\" on the first line, then your rationale.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=JUDGE_MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=200,\n",
    "    )\n",
    "\n",
    "    result_text = response.choices[0].message.content.strip()\n",
    "    lines = result_text.split('\\n', 1)\n",
    "\n",
    "    return Feedback(\n",
    "        value=lines[0].strip().lower(),\n",
    "        rationale=lines[1].strip() if len(lines) > 1 else \"\",\n",
    "    )\n",
    "\n",
    "print(\"✓ Custom sufficient_answer scorer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strategy(name: str, index: lancedb.table.Table) -> dict:\n",
    "    \"\"\"Run MLflow genai evaluation with RAG-focused scorers.\"\"\"\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    \n",
    "    predict_fn = make_rag_pipeline(index)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"eval_{name}\"):\n",
    "        mlflow.log_param(\"strategy\", name)\n",
    "        mlflow.log_param(\"num_chunks\", len(index.to_pandas()))\n",
    "        mlflow.log_param(\"judge_model\", JUDGE_MODEL)\n",
    "        mlflow.log_param(\"k_chunks\", K_CHUNKS)\n",
    "        \n",
    "        # Scorers:\n",
    "        # - RetrievalSufficiency: Do retrieved chunks contain enough info? (KEY METRIC)\n",
    "        # - Correctness: Is the answer factually correct vs ground truth? (strict)\n",
    "        # - sufficient_answer: Lenient correctness - core question answered? (custom)\n",
    "        results = mlflow.genai.evaluate(\n",
    "            data=eval_data,\n",
    "            predict_fn=predict_fn,\n",
    "            scorers=[\n",
    "                RetrievalSufficiency(model=JUDGE_MODEL),\n",
    "                Correctness(model=JUDGE_MODEL),\n",
    "                AnswerRelevancy(model=JUDGE_MODEL, threshold=0.9),\n",
    "                Faithfulness(model=JUDGE_MODEL, threshold=0.9),\n",
    "                sufficient_answer,  # Custom lenient scorer\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        print(f\"  ✓ Metrics: {results.metrics}\")\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": name,\n",
    "            \"metrics\": results.metrics,\n",
    "            \"table\": results.tables.get(\"eval_results\"),\n",
    "        }\n",
    "\n",
    "print(\"✓ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluations for all three strategies\n",
    "naive_results = evaluate_strategy(\"naive\", naive_index)\n",
    "middleground_results = evaluate_strategy(\"middleground\", middleground_index)\n",
    "ast_results = evaluate_strategy(\"ast\", ast_index)\n",
    "\n",
    "print(\"\\n✓ All evaluations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison DataFrame\n",
    "metrics_to_show = [\n",
    "    \"retrieval_sufficiency/mean\",  # MLflow - KEY METRIC\n",
    "    \"correctness/mean\",            # MLflow (strict)\n",
    "    \"sufficient_answer/mean\",      # Custom (lenient)\n",
    "    \"AnswerRelevancy/mean\",        # DeepEval\n",
    "    \"Faithfulness/mean\",           # DeepEval\n",
    "]\n",
    "\n",
    "comparison_data = []\n",
    "for result in [naive_results, middleground_results, ast_results]:\n",
    "    row = {\"strategy\": result[\"strategy\"]}\n",
    "    for m in metrics_to_show:\n",
    "        key = m.split(\"/\")[0]\n",
    "        row[key] = result[\"metrics\"].get(m, \"N/A\")\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Strategy':<15} {'Chunks':<8} {'Retrieval':<12} {'Correct':<10} {'Sufficient':<10}\")\n",
    "print(f\"{'':15} {'':8} {'Sufficiency':<12} {'(strict)':<10} {'(lenient)':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for result, chunks in [(naive_results, naive_chunks), \n",
    "                       (middleground_results, middleground_chunks), \n",
    "                       (ast_results, ast_chunks)]:\n",
    "    m = result[\"metrics\"]\n",
    "    print(f\"{result['strategy']:<15} {len(chunks):<8} \"\n",
    "          f\"{m.get('retrieval_sufficiency/mean', 0):<12.1%} \"\n",
    "          f\"{m.get('correctness/mean', 0):<10.1%} \"\n",
    "          f\"{m.get('sufficient_answer/mean', 0):<10.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AST-based chunking shows +13 pts retrieval sufficiency over naive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Conclusion\n\n### What the Results Show\n\n**AST chunking won on the metric that matters most.** The gap in Retrieval Sufficiency between AST and naive shows that semantic boundaries and structural metadata make a real difference for code retrieval.\n\n**Language-aware didn't outperform naive.** We expected heuristic separators to help, but strict size limits still split large functions awkwardly. Without metadata headers, chunks don't carry the structural context that helps retrieval.\n\n**Define the metrics that matter to you.** Look at the gap between Correctness and Sufficient Answer. Many responses were semantically correct but failed strict matching. The built-in Correctness scorer was too pedantic for our use case. The custom scorer gave us actionable signal about what we actually cared about: did the response answer the question?\n\n### Caveats and Open Questions\n\n- **Is it just chunk size?** AST chunks are larger on average. Maybe bigger chunks do better regardless of strategy. A follow-up experiment with naive chunking at 1500 characters could isolate this.\n- **Would code-specific embeddings help the other strategies?** We used `databricks-gte-large-en`. Code-specific models exist, but we were constrained to what integrates with production. Better embeddings might narrow the gap between strategies.\n- **Small evaluation set**: 24 questions may not capture all query patterns.\n\n### Takeaways\n\n**MLflow GenAI evaluation lets you measure what matters.** Without systematic evaluation, we would have guessed that language-aware chunking was good enough. The numbers showed otherwise.\n\n**Custom scorers are essential.** Built-in metrics are starting points. Your use case will have specific requirements that need custom evaluation logic.\n\n**The methodology is the main thing.** Once you can systematically evaluate RAG quality, you can make confident decisions about every component of your pipeline: chunking strategy, embedding model, k value, reranking approach, and more."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}