{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9dd8d3e-2923-4e16-9435-bd2fd134d94f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### refunder stream\n",
    "\n",
    "this notebook starts a stream to score completed orders for potential refunds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b5ea14-d02b-413b-9f69-86b932322ec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "DATABRICKS_HOST = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "REFUND_AGENT_ENDPOINT_NAME = dbutils.widgets.get(\"REFUND_AGENT_ENDPOINT_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2af6e4b-0a96-4810-9c15-eedb7e84db67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa1b2c3d-4e5f-6a7b-8c9d-0e1f2a3b4c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Resolve model version metadata from the serving endpoint (once per job run)\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "ENDPOINT_NAME = REFUND_AGENT_ENDPOINT_NAME\n",
    "MODEL_NAME = \"unknown\"\n",
    "MODEL_VERSION = \"unknown\"\n",
    "\n",
    "try:\n",
    "    _w = WorkspaceClient()\n",
    "    _endpoint = _w.serving_endpoints.get(ENDPOINT_NAME)\n",
    "    _served = _endpoint.config.served_entities[0]\n",
    "    MODEL_NAME = _served.entity_name\n",
    "    MODEL_VERSION = str(_served.entity_version)\n",
    "    print(f\"Resolved endpoint metadata: model={MODEL_NAME}, version={MODEL_VERSION}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: could not resolve endpoint metadata: {e}\")\n",
    "    print(f\"Using defaults: model={MODEL_NAME}, version={MODEL_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the canonical full demo run this stream job will start BEFORE the agent is actually ready\n",
    "# (because model serving deployment from previous stage takes 15~20min)\n",
    "# So, we seed some fake responses, and return them instead while the model isn't yet loaded\n",
    "# See next cell too\n",
    "fake_responses = [\n",
    "    {\n",
    "        \"refund_usd\": 0.0,\n",
    "        \"refund_class\": \"none\",\n",
    "        \"reason\": \"Order was delivered within the P75 delivery time\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 0.0,\n",
    "        \"refund_class\": \"none\",\n",
    "        \"reason\": \"Order was delivered within the P75 delivery time\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 4.47,\n",
    "        \"refund_class\": \"partial\",\n",
    "        \"reason\": \"Order was delivered late by 1.6 minutes\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 0.0,\n",
    "        \"refund_class\": \"none\",\n",
    "        \"reason\": \"Order was delivered on time\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 8.91,\n",
    "        \"refund_class\": \"partial\",\n",
    "        \"reason\": \"Order was delivered late by 5.5875 minutes\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 5.48,\n",
    "        \"refund_class\": \"partial\",\n",
    "        \"reason\": \"Order was delivered 1.9 minutes after the P75 delivery time\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 0.0,\n",
    "        \"refund_class\": \"none\",\n",
    "        \"reason\": \"Order was delivered on time\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 9.25,\n",
    "        \"refund_class\": \"partial\",\n",
    "        \"reason\": \"Order was delivered 18.9 minutes after creation, which is less than the P75 delivery time of 32.804165 minutes, but still late compared to the P50 delivery time of 29.016666 minutes\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 11.34,\n",
    "        \"refund_class\": \"partial\",\n",
    "        \"reason\": \"Order was delivered in 25.35 minutes, which is between the 75th percentile (32.80 minutes) and the 99th percentile (38.84 minutes) for the Bellevue location\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 8.66,\n",
    "        \"refund_class\": \"partial\",\n",
    "        \"reason\": \"Order was late by 2.6625 minutes\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 4.93,\n",
    "        \"refund_class\": \"partial\",\n",
    "        \"reason\": \"Order was late by 2.175 minutes\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 0.0,\n",
    "        \"refund_class\": \"none\",\n",
    "        \"reason\": \"Order was delivered on time\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 12.34,\n",
    "        \"refund_class\": \"partial\",\n",
    "        \"reason\": \"Order was delivered 7.97 minutes after the P75 delivery time\",\n",
    "    },\n",
    "    {\n",
    "        \"refund_usd\": 0.0,\n",
    "        \"refund_class\": \"none\",\n",
    "        \"reason\": \"Order was delivered within the P75 delivery time\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5540baa0-a16b-4bd1-a61d-dcb80eb2ce98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Configuration for inference capping\n",
    "CHECKPOINT_PATH = f\"/Volumes/{CATALOG}/recommender/checkpoints/refundrecommenderstream\"\n",
    "MAX_INFERENCES_PER_BATCH = 50\n",
    "\n",
    "def is_first_run():\n",
    "    \"\"\"Check if checkpoint exists (indicates this is NOT the first run)\"\"\"\n",
    "    return not os.path.exists(CHECKPOINT_PATH) or len(os.listdir(CHECKPOINT_PATH)) == 0\n",
    "\n",
    "def get_chat_completion(content: str) -> str:\n",
    "    \"\"\"Call the refund agent endpoint for real inference.\n",
    "    Injects __synthetic flag into the response JSON to track provenance.\"\"\"\n",
    "    client = OpenAI(\n",
    "        api_key=DATABRICKS_TOKEN,\n",
    "        base_url=f\"{DATABRICKS_HOST}/serving-endpoints\",\n",
    "    )\n",
    "    default_response = json.dumps({\n",
    "        \"refund_usd\": 0.0,\n",
    "        \"refund_class\": \"error\",\n",
    "        \"reason\": \"agent did not return valid JSON\",\n",
    "        \"__synthetic\": True\n",
    "    })\n",
    "\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                model=f\"{REFUND_AGENT_ENDPOINT_NAME}\",\n",
    "                messages=[{\"role\": \"user\", \"content\": content}],\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Fake data injection when serving endpoint isn't available\n",
    "            obj = dict(random.choice(fake_responses))\n",
    "            obj[\"__synthetic\"] = True\n",
    "            response = json.dumps(obj)\n",
    "        else:\n",
    "            response = chat_completion.messages[-1].get(\"content\")\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(response)\n",
    "            # Tag real inference responses\n",
    "            if \"__synthetic\" not in parsed:\n",
    "                parsed[\"__synthetic\"] = False\n",
    "            return json.dumps(parsed)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    return default_response\n",
    "\n",
    "def get_fake_response(order_id: str) -> str:\n",
    "    \"\"\"Return a fake response (fast path for backfill and overflow)\"\"\"\n",
    "    obj = dict(random.choice(fake_responses))\n",
    "    obj[\"__synthetic\"] = True\n",
    "    return json.dumps(obj)\n",
    "\n",
    "get_chat_completion_udf = udf(get_chat_completion, StringType())\n",
    "get_fake_response_udf = udf(get_fake_response, StringType())\n",
    "\n",
    "def _add_model_columns(df):\n",
    "    \"\"\"Add model version tracking columns to a DataFrame.\n",
    "    Derives is_synthetic from the __synthetic flag in agent_response JSON.\"\"\"\n",
    "    return df \\\n",
    "        .withColumn(\"model_name\", F.lit(MODEL_NAME)) \\\n",
    "        .withColumn(\"model_version\", F.lit(MODEL_VERSION)) \\\n",
    "        .withColumn(\"endpoint_name\", F.lit(ENDPOINT_NAME)) \\\n",
    "        .withColumn(\"is_synthetic\",\n",
    "            F.coalesce(\n",
    "                F.get_json_object(F.col(\"agent_response\"), \"$.__synthetic\").cast(BooleanType()),\n",
    "                F.lit(True)\n",
    "            )\n",
    "        )\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    \"\"\"Process each micro-batch with inference capping\n",
    "\n",
    "    - First run (no checkpoint): ALL rows get fake responses (fast backfill)\n",
    "    - Subsequent runs: First 50 rows get real inference, rest get fake\n",
    "    \"\"\"\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    first_run = is_first_run()\n",
    "    row_count = batch_df.count()\n",
    "\n",
    "    print(f\"Processing batch {batch_id}: {row_count} rows, first_run={first_run}\")\n",
    "\n",
    "    if first_run:\n",
    "        # First run: ALL rows get fake responses (fast)\n",
    "        print(f\"  -> First run detected, using fake responses for all {row_count} rows\")\n",
    "        result_df = batch_df.select(\n",
    "            F.col(\"order_id\"),\n",
    "            F.current_timestamp().alias(\"ts\"),\n",
    "            F.to_timestamp(F.col(\"ts\")).alias(\"order_ts\"),\n",
    "            get_fake_response_udf(F.col(\"order_id\")).alias(\"agent_response\")\n",
    "        )\n",
    "    else:\n",
    "        # Subsequent runs: First N rows get real inference, rest get fake\n",
    "        windowed = batch_df \\\n",
    "            .withColumn(\"order_ts\", F.to_timestamp(F.col(\"ts\"))) \\\n",
    "            .withColumn(\"row_num\", F.row_number().over(Window.orderBy(F.col(\"order_ts\"), F.col(\"order_id\"))))\n",
    "\n",
    "        real_count = min(row_count, MAX_INFERENCES_PER_BATCH)\n",
    "        fake_count = max(0, row_count - MAX_INFERENCES_PER_BATCH)\n",
    "        print(f\"  -> Real inference: {real_count} rows, fake: {fake_count} rows\")\n",
    "\n",
    "        # Split into real inference vs fake\n",
    "        real_inference_df = windowed.filter(f\"row_num <= {MAX_INFERENCES_PER_BATCH}\") \\\n",
    "            .select(\n",
    "                F.col(\"order_id\"),\n",
    "                F.current_timestamp().alias(\"ts\"),\n",
    "                F.col(\"order_ts\"),\n",
    "                get_chat_completion_udf(F.col(\"order_id\")).alias(\"agent_response\")\n",
    "            )\n",
    "\n",
    "        fake_response_df = windowed.filter(f\"row_num > {MAX_INFERENCES_PER_BATCH}\") \\\n",
    "            .select(\n",
    "                F.col(\"order_id\"),\n",
    "                F.current_timestamp().alias(\"ts\"),\n",
    "                F.col(\"order_ts\"),\n",
    "                get_fake_response_udf(F.col(\"order_id\")).alias(\"agent_response\")\n",
    "            )\n",
    "\n",
    "        result_df = real_inference_df.union(fake_response_df)\n",
    "\n",
    "    # Add model version tracking columns\n",
    "    result_df = _add_model_columns(result_df)\n",
    "\n",
    "    # Write to table (mergeSchema handles the new columns on first write)\n",
    "    result_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{CATALOG}.recommender.refund_recommendations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aa36d99-1219-4998-878e-7d49ff9dad2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Read stream of delivered events - no sampling needed, we control volume in foreachBatch\ndelivered_events = spark.readStream.table(f\"{CATALOG}.lakeflow.all_events\") \\\n    .filter(\"event_type = 'delivered'\")"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cf733f0-a483-4c1b-a142-e0f0e0539bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS ${CATALOG}.recommender;\n",
    "CREATE VOLUME IF NOT EXISTS ${CATALOG}.recommender.checkpoints;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a12df85-36d4-485d-ac94-74dd12266a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS ${CATALOG}.recommender.refund_recommendations (\n",
    "  order_id STRING,\n",
    "  ts TIMESTAMP,\n",
    "  order_ts TIMESTAMP,\n",
    "  agent_response STRING,\n",
    "  model_name STRING,\n",
    "  model_version STRING,\n",
    "  endpoint_name STRING,\n",
    "  is_synthetic BOOLEAN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a3cd40-e993-4308-a860-139372ba8d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Enable CDC only if not already enabled (avoids unnecessary table writes)\ntable_name = f\"{CATALOG}.recommender.refund_recommendations\"\n\ntry:\n    props = spark.sql(f\"SHOW TBLPROPERTIES {table_name}\").collect()\n    cdc_enabled = any(row.key == \"delta.enableChangeDataFeed\" and row.value == \"true\" for row in props)\nexcept Exception:\n    cdc_enabled = False\n\nif not cdc_enabled:\n    print(f\"Enabling CDC on {table_name}\")\n    spark.sql(f\"ALTER TABLE {table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\nelse:\n    print(f\"CDC already enabled on {table_name}, skipping\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40d33309-fcdc-4e94-a65c-de78aac4422e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Process with foreachBatch for fine-grained control over inference volume\ndelivered_events.writeStream \\\n    .foreachBatch(process_batch) \\\n    .option(\"checkpointLocation\", f\"/Volumes/{CATALOG}/recommender/checkpoints/refundrecommenderstream\") \\\n    .trigger(availableNow=True) \\\n    .start() \\\n    .awaitTermination()"
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8812248119570203,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "refund_recommender_stream",
   "widgets": {
    "CATALOG": {
     "currentValue": "cazzper",
     "nuid": "4fed5096-973b-4a59-acc5-e4a96cc00360",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "CATALOG",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "CATALOG",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "REFUND_AGENT_ENDPOINT_NAME": {
     "currentValue": "",
     "nuid": "bf162244-8e75-4570-82f4-ef93955c6891",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "REFUND_AGENT_ENDPOINT_NAME",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "REFUND_AGENT_ENDPOINT_NAME",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}