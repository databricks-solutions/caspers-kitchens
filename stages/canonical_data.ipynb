{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Canonical Data\n",
        "\n",
        "This notebook bootstraps Caspers canonical data into the provided catalog and schema.\n",
        "\n",
        "Unlike the old generator, this uses pre-generated data from the canonical dataset that is replayed at configurable speeds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install --upgrade databricks-sdk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dbutils.library.restartPython()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
        "EVENTS_VOLUME = dbutils.widgets.get(\"EVENTS_VOLUME\")\n",
        "SIMULATOR_SCHEMA = dbutils.widgets.get(\"SIMULATOR_SCHEMA\")\n",
        "START_DAY = dbutils.widgets.get(\"START_DAY\") if dbutils.widgets.get(\"START_DAY\") else \"20\"\n",
        "SPEED_MULTIPLIER = dbutils.widgets.get(\"SPEED_MULTIPLIER\") if dbutils.widgets.get(\"SPEED_MULTIPLIER\") else \"1.0\"\n",
        "SCHEDULE_MINUTES = dbutils.widgets.get(\"SCHEDULE_MINUTES\") if dbutils.widgets.get(\"SCHEDULE_MINUTES\") else \"5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Create main catalog, simulator related schemas and volumes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%sql\n",
        "CREATE CATALOG IF NOT EXISTS ${CATALOG};\n",
        "CREATE SCHEMA IF NOT EXISTS ${CATALOG}.${SIMULATOR_SCHEMA};\n",
        "CREATE VOLUME IF NOT EXISTS ${CATALOG}.${SIMULATOR_SCHEMA}.${EVENTS_VOLUME};\n",
        "CREATE VOLUME IF NOT EXISTS ${CATALOG}.${SIMULATOR_SCHEMA}.misc;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Create tables from canonical dataset parquet files\n",
        "\n",
        "Load dimensional data from the canonical dataset (not from ./data/dimensional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dimension tables from canonical dataset\n",
        "spark.createDataFrame(pd.read_parquet(\"../data/canonical/canonical_dataset/brands.parquet\")) \\\n",
        "    .write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SIMULATOR_SCHEMA}.brands\")\n",
        "\n",
        "spark.createDataFrame(pd.read_parquet(\"../data/canonical/canonical_dataset/locations.parquet\")) \\\n",
        "    .write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SIMULATOR_SCHEMA}.locations\")\n",
        "\n",
        "spark.createDataFrame(pd.read_parquet(\"../data/canonical/canonical_dataset/menus.parquet\")) \\\n",
        "    .write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SIMULATOR_SCHEMA}.menus\")\n",
        "\n",
        "spark.createDataFrame(pd.read_parquet(\"../data/canonical/canonical_dataset/categories.parquet\")) \\\n",
        "    .write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SIMULATOR_SCHEMA}.categories\")\n",
        "\n",
        "spark.createDataFrame(pd.read_parquet(\"../data/canonical/canonical_dataset/items.parquet\")) \\\n",
        "    .write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SIMULATOR_SCHEMA}.items\")\n",
        "\n",
        "spark.createDataFrame(pd.read_parquet(\"../data/canonical/canonical_dataset/brand_locations.parquet\")) \\\n",
        "    .write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SIMULATOR_SCHEMA}.brand_locations\")\n",
        "\n",
        "print(\"‚úÖ Dimensional tables created from canonical dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Start canonical data replay\n",
        "\n",
        "Create a scheduled job that runs the canonical generator notebook at the specified interval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from databricks.sdk import WorkspaceClient\n",
        "import databricks.sdk.service.jobs as j\n",
        "import os\n",
        "\n",
        "w = WorkspaceClient()\n",
        "\n",
        "notebook_abs_path = os.path.abspath(\"../data/canonical/canonical_generator_simple\")\n",
        "notebook_dbx_path = notebook_abs_path.replace(\n",
        "    os.environ.get(\"DATABRICKS_WORKSPACE_ROOT\", \"/Workspace\"),\n",
        "    \"/Workspace\"\n",
        ")\n",
        "\n",
        "import sys\n",
        "sys.path.append('../utils')\n",
        "from uc_state import add\n",
        "\n",
        "job_name = f\"Canonical Data Replay ({CATALOG})\"\n",
        "schedule_minutes = int(SCHEDULE_MINUTES)\n",
        "cron_expression = f\"0 0/{schedule_minutes} * * * ?\"\n",
        "\n",
        "task_def = [\n",
        "    j.Task(\n",
        "        task_key=\"canonical_data_replay\",\n",
        "        notebook_task=j.NotebookTask(\n",
        "            notebook_path=notebook_dbx_path,\n",
        "            base_parameters={\n",
        "                \"CATALOG\": CATALOG,\n",
        "                \"VOLUME\": EVENTS_VOLUME,\n",
        "                \"SCHEMA\": SIMULATOR_SCHEMA,\n",
        "                \"START_DAY\": START_DAY,\n",
        "                \"SPEED_MULTIPLIER\": SPEED_MULTIPLIER,\n",
        "            },\n",
        "        )\n",
        "    )\n",
        "]\n",
        "schedule_def = j.CronSchedule(\n",
        "    quartz_cron_expression=cron_expression,\n",
        "    timezone_id=\"UTC\",\n",
        "    pause_status=j.PauseStatus.UNPAUSED,\n",
        ")\n",
        "\n",
        "existing = [jb for jb in w.jobs.list(name=job_name) if jb.settings.name == job_name]\n",
        "if existing:\n",
        "    job_id = existing[0].job_id\n",
        "    w.jobs.reset(job_id=job_id, new_settings=j.JobSettings(\n",
        "        name=job_name, tasks=task_def, schedule=schedule_def,\n",
        "    ))\n",
        "    print(f\"‚ôªÔ∏è Updated existing job_id={job_id} for {job_name}\")\n",
        "else:\n",
        "    job = w.jobs.create(name=job_name, tasks=task_def, schedule=schedule_def)\n",
        "    job_id = job.job_id\n",
        "    add(CATALOG, \"jobs\", job)\n",
        "    print(f\"‚úÖ Created scheduled job_id={job_id} for {job_name}\")\n",
        "\n",
        "print(f\"   Schedule: Every {schedule_minutes} minutes\")\n",
        "\n",
        "w.jobs.run_now(job_id=job_id)\n",
        "print(f\"üöÄ Started initial run of job {job_id}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Blocking cell to wait for some data to arrive at the volume.\n",
        "\n",
        "The lakeflow declarative pipeline that comes next infers the schema from existing data.\n",
        "\n",
        "Lakeflow Jobs doesn't have a file arrival trigger at the task level (yet?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "# Construct the path to the volume where JSONs will arrive\n",
        "volume_path = f\"/Volumes/{CATALOG}/{SIMULATOR_SCHEMA}/{EVENTS_VOLUME}\"\n",
        "\n",
        "def wait_for_data(path, timeout=300, poll_interval=5):\n",
        "    \"\"\"\n",
        "    Wait until at least one file appears in the given path.\n",
        "    Args:\n",
        "        path (str): The directory to watch.\n",
        "        timeout (int): Maximum seconds to wait.\n",
        "        poll_interval (int): Seconds between checks.\n",
        "    Raises:\n",
        "        TimeoutError: If no file appears within the timeout.\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    while time.time() - start < timeout:\n",
        "        files = dbutils.fs.ls(path)\n",
        "        if any(f.size > 0 for f in files if not f.path.endswith('/')):\n",
        "            print(\"‚úÖ Data arrived. Safe to proceed.\")\n",
        "            return\n",
        "        time.sleep(poll_interval)\n",
        "    raise TimeoutError(f\"No data found in {path} after {timeout} seconds.\")\n",
        "\n",
        "wait_for_data(volume_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}