{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection Data\n",
    "\n",
    "This stage uploads pre-generated food safety inspection report PDFs to a Unity\n",
    "Catalog volume and loads the structured metadata as dimension tables. Each PDF\n",
    "contains an inspection report for one of the 4 ghost kitchen locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade databricks-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = dbutils.widgets.get(\"CATALOG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create catalog, schema, and volume for food safety documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.food_safety\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.food_safety.reports\")\n",
    "print(f\"\\u2705 Created schema {CATALOG}.food_safety and volume reports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copy PDF files from the repo into the Unity Catalog volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "pdf_source_dir = os.path.abspath(\"../data/inspections/pdfs\")\n",
    "volume_path = f\"/Volumes/{CATALOG}/food_safety/reports\"\n",
    "\n",
    "pdf_files = glob.glob(os.path.join(pdf_source_dir, \"*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDF files to upload\")\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    filename = os.path.basename(pdf_file)\n",
    "    with open(pdf_file, \"rb\") as src:\n",
    "        with open(f\"{volume_path}/{filename}\", \"wb\") as dst:\n",
    "            dst.write(src.read())\n",
    "    print(f\"  Uploaded: {filename}\")\n",
    "\n",
    "print(f\"\\u2705 Uploaded {len(pdf_files)} PDFs to {volume_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load structured metadata as dimension tables\n",
    "\n",
    "The inspection_metadata.json contains the source data used to generate the PDFs,\n",
    "including scores, violations, and corrective actions per inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata_path = os.path.abspath(\"../data/inspections/inspection_metadata.json\")\n",
    "with open(metadata_path) as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"Loaded metadata for {len(metadata['inspections'])} inspections across {len(metadata['locations'])} locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DateType\n",
    ")\n",
    "from datetime import date as dt_date\n",
    "\n",
    "# Flatten inspections (one row per inspection)\n",
    "inspection_rows = []\n",
    "for insp in metadata[\"inspections\"]:\n",
    "    inspection_rows.append({\n",
    "        \"inspection_id\": insp[\"inspection_id\"],\n",
    "        \"location_id\": insp[\"location_id\"],\n",
    "        \"location_name\": insp[\"location_name\"],\n",
    "        \"address\": insp[\"address\"],\n",
    "        \"jurisdiction\": insp[\"jurisdiction\"],\n",
    "        \"inspection_date\": dt_date.fromisoformat(insp[\"inspection_date\"]),\n",
    "        \"inspector_name\": insp[\"inspector_name\"],\n",
    "        \"score\": insp[\"score\"],\n",
    "        \"grade\": insp[\"grade\"],\n",
    "        \"violation_count\": insp[\"violation_count\"],\n",
    "        \"critical_count\": insp[\"critical_count\"],\n",
    "        \"major_count\": insp[\"major_count\"],\n",
    "        \"minor_count\": insp[\"minor_count\"],\n",
    "        \"follow_up_status\": insp[\"follow_up_status\"],\n",
    "    })\n",
    "\n",
    "inspection_schema = StructType([\n",
    "    StructField(\"inspection_id\", StringType()),\n",
    "    StructField(\"location_id\", IntegerType()),\n",
    "    StructField(\"location_name\", StringType()),\n",
    "    StructField(\"address\", StringType()),\n",
    "    StructField(\"jurisdiction\", StringType()),\n",
    "    StructField(\"inspection_date\", DateType()),\n",
    "    StructField(\"inspector_name\", StringType()),\n",
    "    StructField(\"score\", IntegerType()),\n",
    "    StructField(\"grade\", StringType()),\n",
    "    StructField(\"violation_count\", IntegerType()),\n",
    "    StructField(\"critical_count\", IntegerType()),\n",
    "    StructField(\"major_count\", IntegerType()),\n",
    "    StructField(\"minor_count\", IntegerType()),\n",
    "    StructField(\"follow_up_status\", StringType()),\n",
    "])\n",
    "\n",
    "df_inspections = spark.createDataFrame(inspection_rows, schema=inspection_schema)\n",
    "df_inspections.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.food_safety.inspections\")\n",
    "print(f\"\\u2705 Created inspections table with {df_inspections.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten violations (one row per violation)\n",
    "violation_rows = []\n",
    "for insp in metadata[\"inspections\"]:\n",
    "    for v in insp[\"violations\"]:\n",
    "        violation_rows.append({\n",
    "            \"inspection_id\": insp[\"inspection_id\"],\n",
    "            \"location_id\": insp[\"location_id\"],\n",
    "            \"location_name\": insp[\"location_name\"],\n",
    "            \"inspection_date\": dt_date.fromisoformat(insp[\"inspection_date\"]),\n",
    "            \"code\": v[\"code\"],\n",
    "            \"severity\": v[\"severity\"],\n",
    "            \"category\": v[\"category\"],\n",
    "            \"description\": v[\"description\"],\n",
    "            \"corrective_action\": v[\"corrective_action\"],\n",
    "            \"deadline_days\": v[\"deadline_days\"],\n",
    "        })\n",
    "\n",
    "violation_schema = StructType([\n",
    "    StructField(\"inspection_id\", StringType()),\n",
    "    StructField(\"location_id\", IntegerType()),\n",
    "    StructField(\"location_name\", StringType()),\n",
    "    StructField(\"inspection_date\", DateType()),\n",
    "    StructField(\"code\", StringType()),\n",
    "    StructField(\"severity\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"corrective_action\", StringType()),\n",
    "    StructField(\"deadline_days\", IntegerType()),\n",
    "])\n",
    "\n",
    "df_violations = spark.createDataFrame(violation_rows, schema=violation_schema)\n",
    "df_violations.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.food_safety.violations\")\n",
    "print(f\"\\u2705 Created violations table with {df_violations.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Register resources with uc_state for cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "print(\"\\u2705 Inspection data stage complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
