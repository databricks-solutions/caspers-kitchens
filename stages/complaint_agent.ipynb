{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "#### complaint agent\n",
    "\n",
    "Builds and ships an order-complaint agent: author tools, assemble the LangGraph workflow, evaluate it, and promote the packaged model into production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "#### Tool & View Registration\n",
    "\n",
    "- `CREATE SCHEMA` guarantees the shared `${CATALOG}.ai` workspace exists for agent assets.\n",
    "- `order_delivery_times_per_location_view` summarizes delivery percentiles per brand/location.\n",
    "- `get_order_overview(oid)` returns structured order metadata, items, and customer info.\n",
    "- `get_order_timing(oid)` exposes created/delivered timestamps plus transit duration.\n",
    "- `get_location_timings(loc)` yields P50/P75/P99 delivery benchmarks for benchmarking complaints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS ${CATALOG}.ai;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tdy6yy3gheg",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW ${CATALOG}.ai.order_delivery_times_per_location_view AS\n",
    "WITH order_times AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    MAX(CASE WHEN event_type = 'order_created' THEN try_to_timestamp(ts) END) AS order_created_time,\n",
    "    MAX(CASE WHEN event_type = 'delivered' THEN try_to_timestamp(ts) END) AS delivered_time\n",
    "  FROM\n",
    "    ${CATALOG}.lakeflow.all_events\n",
    "  WHERE\n",
    "    try_to_timestamp(ts) >= CURRENT_TIMESTAMP() - INTERVAL 1 DAY\n",
    "  GROUP BY\n",
    "    order_id,\n",
    "    location\n",
    "),\n",
    "total_order_times AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    (UNIX_TIMESTAMP(delivered_time) - UNIX_TIMESTAMP(order_created_time)) / 60 AS total_order_time_minutes\n",
    "  FROM\n",
    "    order_times\n",
    "  WHERE\n",
    "    order_created_time IS NOT NULL\n",
    "    AND delivered_time IS NOT NULL\n",
    ")\n",
    "SELECT\n",
    "  location,\n",
    "  PERCENTILE(total_order_time_minutes, 0.50) AS P50,\n",
    "  PERCENTILE(total_order_time_minutes, 0.75) AS P75,\n",
    "  PERCENTILE(total_order_time_minutes, 0.99) AS P99\n",
    "FROM\n",
    "  total_order_times\n",
    "GROUP BY\n",
    "  location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_overview(oid STRING COMMENT 'The unique order identifier to retrieve information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  location STRING COMMENT 'Order location',\n",
    "  items_json STRING COMMENT 'JSON array of ordered items with details',\n",
    "  customer_address STRING COMMENT 'Customer delivery address',\n",
    "  brand_id BIGINT COMMENT 'Brand ID for the order',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created'\n",
    ")\n",
    "COMMENT 'Returns basic order information including items, location, and customer details'\n",
    "RETURN\n",
    "  WITH order_created_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      location,\n",
    "      get_json_object(body, '$.items') as items_json,\n",
    "      get_json_object(body, '$.customer_addr') as customer_address,\n",
    "      -- Extract brand_id from first item in the order\n",
    "      CAST(get_json_object(get_json_object(body, '$.items[0]'), '$.brand_id') AS BIGINT) as brand_id,\n",
    "      try_to_timestamp(ts) as order_created_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid AND event_type = 'order_created'\n",
    "    LIMIT 1\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    items_json,\n",
    "    customer_address,\n",
    "    brand_id,\n",
    "    order_created_ts\n",
    "  FROM order_created_events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_timing(oid STRING COMMENT 'The unique order identifier to get timing information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created',\n",
    "  delivered_ts TIMESTAMP COMMENT 'When the order was delivered (NULL if not delivered)',\n",
    "  delivery_duration_minutes FLOAT COMMENT 'Time from order creation to delivery in minutes (NULL if not delivered)',\n",
    "  delivery_status STRING COMMENT 'Current delivery status: delivered, in_progress, or unknown'\n",
    ")\n",
    "COMMENT 'Returns timing information for a specific order'\n",
    "RETURN\n",
    "  WITH order_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      event_type,\n",
    "      try_to_timestamp(ts) as event_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid\n",
    "  ),\n",
    "  timing_summary AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      MIN(CASE WHEN event_type = 'order_created' THEN event_ts END) as order_created_ts,\n",
    "      MAX(CASE WHEN event_type = 'delivered' THEN event_ts END) as delivered_ts\n",
    "    FROM order_events\n",
    "    GROUP BY order_id\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    order_created_ts,\n",
    "    delivered_ts,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL AND order_created_ts IS NOT NULL THEN\n",
    "        CAST((UNIX_TIMESTAMP(delivered_ts) - UNIX_TIMESTAMP(order_created_ts)) / 60 AS FLOAT)\n",
    "      ELSE NULL\n",
    "    END as delivery_duration_minutes,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL THEN 'delivered'\n",
    "      WHEN order_created_ts IS NOT NULL THEN 'in_progress'\n",
    "      ELSE 'unknown'\n",
    "    END as delivery_status\n",
    "  FROM timing_summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_location_timings(loc STRING COMMENT 'Location name as a string')\n",
    "RETURNS TABLE (\n",
    "  location STRING COMMENT 'Location of the order source',\n",
    "  P50 FLOAT COMMENT '50th percentile delivery time in minutes',\n",
    "  P75 FLOAT COMMENT '75th percentile delivery time in minutes',\n",
    "  P99 FLOAT COMMENT '99th percentile delivery time in minutes'\n",
    ")\n",
    "COMMENT 'Returns the 50/75/99th percentile of delivery times for a location to benchmark order timing'\n",
    "RETURN\n",
    "  SELECT location, P50, P75, P99\n",
    "  FROM ${CATALOG}.ai.order_delivery_times_per_location_view AS odlt\n",
    "  WHERE odlt.location = loc;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dq5ml4wp6v",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "- Install LangGraph, Databricks agent packages, and restart Python for a clean runtime.\n",
    "- Capture widget inputs (`CATALOG`, `LLM_MODEL`) and create an MLflow dev experiment for trace logging.\n",
    "- Define a templated `%%writefilev` magic that emits files with notebook variable substitution.\n",
    "- Materialize `agent.py` containing the LangGraph complaint workflow wired to UC SQL tools and the chosen LLM endpoint.\n",
    "- Pull a delivered `order_id` sample and build the MLflow model signature/resources for logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3tu3r2gso",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow-skinny[databricks] langgraph==0.3.4 databricks-langchain databricks-agents uv\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dl04kgwv9ap",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "LLM_MODEL = dbutils.widgets.get(\"LLM_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ktoahik8tl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Create/set dev experiment for development and evaluation traces\n",
    "# Use shared path for job compatibility\n",
    "dev_experiment_name = f\"/Shared/{CATALOG}_complaint_agent_dev\"\n",
    "\n",
    "# set_experiment creates the experiment if it doesn't exist, or activates it if it does\n",
    "dev_experiment = mlflow.set_experiment(dev_experiment_name)\n",
    "dev_experiment_id = dev_experiment.experiment_id\n",
    "print(f\"✅ Using dev experiment: {dev_experiment_name} (ID: {dev_experiment_id})\")\n",
    "\n",
    "# Add experiment to UC state for cleanup\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "experiment_data = {\n",
    "    \"experiment_id\": dev_experiment_id,\n",
    "    \"name\": dev_experiment_name\n",
    "}\n",
    "add(CATALOG, \"experiments\", experiment_data)\n",
    "print(f\"✅ Added dev experiment to UC state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bruu85upqq5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def writefilev(line, cell):\n",
    "    \"\"\"\n",
    "    %%writefilev file.py\n",
    "    Allows {{var}} substitutions while leaving normal {} intact.\n",
    "    \"\"\"\n",
    "    filename = line.strip()\n",
    "\n",
    "    def replacer(match):\n",
    "        expr = match.group(1)\n",
    "        return str(eval(expr, globals(), locals()))\n",
    "\n",
    "    # Replace only double braces {{var}}\n",
    "    content = re.sub(r\"\\{\\{(.*?)\\}\\}\", replacer, cell)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Wrote file with substitutions: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2c70l4ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefilev agent.py\n",
    "import json\n",
    "from typing import Annotated, Any, Generator, Optional, Sequence, TypedDict, Union, Literal, cast\n",
    "from uuid import uuid4\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    ")\n",
    "from langchain_core.messages.utils import convert_to_messages\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "LLM_MODEL = \"{{LLM_MODEL}}\"\n",
    "CATALOG = \"{{CATALOG}}\"\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "class ComplaintResponse(TypedDict):\n",
    "    order_id: str\n",
    "    complaint_category: Literal[\"delivery_delay\", \"missing_items\", \"food_quality\", \"service_issue\", \"billing\", \"other\"]\n",
    "    decision: Literal[\"suggest_credit\", \"escalate\"]\n",
    "    credit_amount: Optional[float]\n",
    "    confidence: Optional[Literal[\"high\", \"medium\", \"low\"]]\n",
    "    priority: Optional[Literal[\"standard\", \"urgent\"]]\n",
    "    rationale: str\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "\n",
    "LLM_ENDPOINT_NAME = f\"{LLM_MODEL}\"\n",
    "base_llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "system_prompt = \"\"\"You are the Complaint Triage Agent for Casper's Kitchens, a ghost kitchen network. Your job is to analyze customer complaints and recommend actions for internal customer service staff.\n",
    "\n",
    "PROCESS:\n",
    "1. Extract the order_id from the complaint text\n",
    "2. Call get_order_overview(order_id) to get order details and items\n",
    "3. Call get_order_timing(order_id) to get delivery timing\n",
    "4. For delivery delay complaints, call get_location_timings(location) to get percentile benchmarks\n",
    "5. Analyze the data and make a recommendation\n",
    "\n",
    "DECISION FRAMEWORK:\n",
    "\n",
    "SUGGEST_CREDIT - Use when you can make a data-backed recommendation:\n",
    "\n",
    "Delivery delays (high confidence when data-backed):\n",
    "- Compare actual delivery time to location percentiles\n",
    "- >P75 but <P90: Suggest 15% of order total\n",
    "- >P90 but <P99: Suggest 25% of order total  \n",
    "- >P99: Suggest 50% of order total\n",
    "- On-time delivery: Suggest $0 credit (low confidence - might be other issues)\n",
    "\n",
    "Missing items:\n",
    "- Parse items_json from get_order_overview to find actual item prices\n",
    "- Use real item costs when available for accurate refund\n",
    "- Check if claimed missing item actually makes sense for this order (cross-reference with items list)\n",
    "- If claimed item not in order or doesn't match (e.g., claiming missing fries on salad-only order): affects confidence negatively\n",
    "- If item plausibly missing but can't verify price: estimate $8-12 per item (medium confidence)\n",
    "- If no evidence of missing items but customer claims it: $0 credit (low confidence)\n",
    "\n",
    "Food quality issues:\n",
    "- If specific details provided (cold, soggy, wrong preparation): $10-15 (medium confidence)\n",
    "- If vague (\"bad\", \"gross\"): escalate with priority=\"standard\" instead\n",
    "- If food quality complaint but order shows delivery delay >P90: consider combined credit\n",
    "\n",
    "ESCALATE - Use when human judgment is needed:\n",
    "- priority=\"standard\": \n",
    "  * Vague complaints without specifics (\"food was bad\", \"not happy\")\n",
    "  * Missing or incomplete order data\n",
    "  * Edge cases that don't fit clear patterns\n",
    "  * Billing issues or promo code problems\n",
    "  * Service complaints (rude driver, wrong location)\n",
    "- priority=\"urgent\":\n",
    "  * Legal threats or mentions of lawyers\n",
    "  * Health/safety concerns (allergies, food poisoning, foreign objects)\n",
    "  * Suspected fraud or inconsistencies in complaint vs order data\n",
    "  * Abusive or threatening language\n",
    "\n",
    "OUTPUT RULES:\n",
    "- For suggest_credit: Include credit_amount (can be $0) and confidence, set priority to null\n",
    "- For escalate: Include priority, set credit_amount and confidence to null\n",
    "- Always include a data-focused rationale citing specific numbers (delivery times, percentiles, order details, item verification)\n",
    "- Rationale is for internal staff - be factual, not apologetic\n",
    "\n",
    "RATIONALE GUIDELINES:\n",
    "- Rationale should clearly articulate the justification for the decision\n",
    "- All evidence used to support the decision should be cited in the rationale. Rationale MUST cite evidence.\n",
    "- Rationale should be detailed and specific, but no longer than 150 words\n",
    "- Rationale should clearly justify the decision, credit amount (if applicable), confidence level, and priority (if applicable)\n",
    "- If a refund is suggested, the rationale should clearly justify the credit amount, based on the evidence provided.\n",
    "\n",
    "CONFIDENCE GUIDELINES:\n",
    "- high: Strong data support (delivery >P90, item prices from order data, clear verification)\n",
    "- medium: Reasonable inference (delivery P75-P90, estimated item costs, plausible but unverified)\n",
    "- low: Weak or contradictory evidence ($0 credits when data doesn't support claim, can't verify complaint details)\n",
    "\n",
    "IMPORTANT NOTES:\n",
    "- If order_id not found or data unavailable: escalate with priority=\"standard\"\n",
    "- Round credit amounts to nearest $0.50\n",
    "- When in doubt between two options, prefer escalate with priority=\"standard\"\n",
    "- A suggest_credit with $0 and low confidence is valid when complaint seems unfounded\"\"\"\n",
    "\n",
    "tools: list[BaseTool] = []\n",
    "uc_tool_names = [\n",
    "    f\"{CATALOG}.ai.get_order_overview\",\n",
    "    f\"{CATALOG}.ai.get_order_timing\",\n",
    "    f\"{CATALOG}.ai.get_location_timings\",\n",
    "]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools.extend(uc_toolkit.tools)\n",
    "\n",
    "RESPONSE_FIELDS = {\"order_id\", \"complaint_category\", \"decision\", \"rationale\"}\n",
    "\n",
    "def parse_structured_response(obj: Union[AIMessage, dict[str, Any]]) -> ComplaintResponse:\n",
    "    if isinstance(obj, dict):\n",
    "        candidate = obj\n",
    "    else:\n",
    "        parsed = obj.additional_kwargs.get(\"parsed_structured_output\")\n",
    "        if isinstance(parsed, dict):\n",
    "            candidate = parsed\n",
    "        else:\n",
    "            content = obj.content\n",
    "            if isinstance(content, str):\n",
    "                raw = content\n",
    "            elif isinstance(content, list):\n",
    "                raw = \"\".join(part.get(\"text\", \"\") if isinstance(part, dict) else str(part) for part in content)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported message content type\")\n",
    "            candidate = json.loads(raw)\n",
    "\n",
    "    missing = RESPONSE_FIELDS.difference(candidate.keys())\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required fields: {sorted(missing)}\")\n",
    "\n",
    "    decision = candidate.get(\"decision\")\n",
    "    if decision == \"suggest_credit\":\n",
    "        if candidate.get(\"credit_amount\") is None:\n",
    "            raise ValueError(\"suggest_credit requires credit_amount\")\n",
    "        if candidate.get(\"confidence\") is None:\n",
    "            raise ValueError(\"suggest_credit requires confidence\")\n",
    "        # Automatically set priority to None if incorrectly provided\n",
    "        if candidate.get(\"priority\") is not None:\n",
    "            candidate[\"priority\"] = None\n",
    "    elif decision == \"escalate\":\n",
    "        if candidate.get(\"priority\") is None:\n",
    "            raise ValueError(\"escalate requires priority\")\n",
    "        # Automatically set credit_amount and confidence to None if incorrectly provided\n",
    "        if candidate.get(\"credit_amount\") is not None:\n",
    "            candidate[\"credit_amount\"] = None\n",
    "        if candidate.get(\"confidence\") is not None:\n",
    "            candidate[\"confidence\"] = None\n",
    "\n",
    "    return cast(ComplaintResponse, candidate)\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    tool_model = model.bind_tools(tools, tool_choice=\"auto\")\n",
    "    structured_model = tool_model.with_structured_output(ComplaintResponse)\n",
    "\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        return \"end\"\n",
    "\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "    tool_runnable = preprocessor | tool_model\n",
    "    structured_runnable = preprocessor | structured_model\n",
    "\n",
    "    def call_model(state: AgentState, config: RunnableConfig):\n",
    "        response = tool_runnable.invoke(state, config)\n",
    "        if not isinstance(response, AIMessage):\n",
    "            raise ValueError(f\"Expected AIMessage, received {type(response)}\")\n",
    "\n",
    "        if response.tool_calls:\n",
    "            return {\"messages\": [response]}\n",
    "\n",
    "        try:\n",
    "            parsed = parse_structured_response(response)\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            structured = structured_runnable.invoke(state, config)\n",
    "            parsed = parse_structured_response(structured)\n",
    "\n",
    "        structured_message = AIMessage(\n",
    "            id=response.id or str(uuid4()),\n",
    "            content=json.dumps(parsed),\n",
    "            additional_kwargs={\"parsed_structured_output\": parsed},\n",
    "        )\n",
    "        return {\"messages\": [structured_message]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\"continue\": \"tools\", \"end\": END},\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    def _langchain_to_responses(self, messages: list[BaseMessage]) -> list[dict[str, Any]]:\n",
    "        items: list[dict[str, Any]] = []\n",
    "\n",
    "        for raw_message in messages or []:\n",
    "            message = raw_message.model_dump() if hasattr(raw_message, \"model_dump\") else raw_message\n",
    "            role = message.get(\"type\")\n",
    "\n",
    "            if role == \"ai\":\n",
    "                if tool_calls := message.get(\"tool_calls\"):\n",
    "                    for tool_call in tool_calls:\n",
    "                        items.append(\n",
    "                            self.create_function_call_item(\n",
    "                                id=message.get(\"id\") or str(uuid4()),\n",
    "                                call_id=tool_call[\"id\"],\n",
    "                                name=tool_call[\"name\"],\n",
    "                                arguments=json.dumps(tool_call.get(\"args\", {})),\n",
    "                            )\n",
    "                        )\n",
    "                    continue\n",
    "\n",
    "                content = message.get(\"content\")\n",
    "                if isinstance(content, list):\n",
    "                    text_content = \"\".join(ch.get(\"text\", \"\") if isinstance(ch, dict) else str(ch) for ch in content)\n",
    "                elif isinstance(content, str):\n",
    "                    text_content = content\n",
    "                else:\n",
    "                    text_content = json.dumps(content)\n",
    "\n",
    "                items.append(\n",
    "                    self.create_text_output_item(\n",
    "                        text=text_content,\n",
    "                        id=message.get(\"id\") or str(uuid4()),\n",
    "                    )\n",
    "                )\n",
    "            elif role == \"tool\":\n",
    "                items.append(\n",
    "                    self.create_function_call_output_item(\n",
    "                        call_id=message.get(\"tool_call_id\"),\n",
    "                        output=message.get(\"content\"),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return items\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        lc_msgs = convert_to_messages(self.prep_msgs_for_cc_llm(request.input))\n",
    "\n",
    "        for event in self.agent.stream({\"messages\": lc_msgs}, stream_mode=[\"updates\", \"messages\"]):\n",
    "            if event[0] == \"updates\":\n",
    "                for node_data in event[1].values():\n",
    "                    for item in self._langchain_to_responses(node_data.get(\"messages\", [])):\n",
    "                        yield ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=item)\n",
    "            elif event[0] == \"messages\":\n",
    "                chunk = event[1][0]\n",
    "                if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "                    yield ResponsesAgentStreamEvent(\n",
    "                        **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "                    )\n",
    "\n",
    "agent = create_tool_calling_agent(base_llm, tools, system_prompt)\n",
    "AGENT = LangGraphResponsesAgent(agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6gjrp4mx6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an actual order_id for input example\n",
    "sample_order_id = spark.sql(f\"\"\"\n",
    "    SELECT order_id \n",
    "    FROM {CATALOG}.lakeflow.all_events \n",
    "    WHERE event_type='delivered'\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0]['order_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ih3tt5qeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sample_order_id is not None\n",
    "print(sample_order_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23o4h7j6amzh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME, tools\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    "for tool in tools:\n",
    "    resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "input_example = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"My order was really late! Order ID: {sample_order_id}\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"complaint_agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        input_example=input_example,\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
    "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "mlflow.set_active_model(model_id = logged_agent_info.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gfympowd7q",
   "metadata": {},
   "source": [
    "#### Evaluate the Agent\n",
    "\n",
    "- Synthesize diverse complaint scenarios (delivery delays, wrong items, missing items, etc.) from recent orders.\n",
    "- Configure multiple MLflow `Guidelines` scorers that check refund reason, messaging quality, and policy compliance.\n",
    "- Run batch evaluations against the agent to quantify decision quality before promotion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2loovjto96g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive complaint scenarios for evaluation\n",
    "import random\n",
    "\n",
    "# Get sample order IDs for different scenarios\n",
    "all_order_ids = [\n",
    "    row['order_id'] for row in spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT order_id \n",
    "        FROM {CATALOG}.lakeflow.all_events \n",
    "        WHERE event_type='delivered'\n",
    "        LIMIT 50\n",
    "    \"\"\").collect()\n",
    "]\n",
    "\n",
    "# Create diverse, realistic complaint scenarios\n",
    "complaint_scenarios = []\n",
    "\n",
    "# 1. Delivery delay complaints - mix of legitimate and questionable\n",
    "for oid in all_order_ids[:8]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My order took forever to arrive! Order ID: {oid}\",\n",
    "        f\"Been waiting 2 hours, this is ridiculous. Order {oid}\",\n",
    "        f\"Order {oid} arrived late and cold\",\n",
    "        f\"Delivery was slower than usual for order {oid}\",\n",
    "    ])\n",
    "\n",
    "# 2. Food quality issues - range from specific to vague\n",
    "for oid in all_order_ids[8:12]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My falafel was completely soggy and inedible. Order: {oid}\",\n",
    "        f\"The food was cold when it arrived, very disappointing. Order: {oid}\",\n",
    "        f\"Everything tasted bad. Order {oid}\",\n",
    "        f\"Not happy with the quality. {oid}\",\n",
    "        f\"The gyro meat was overcooked and dry, very disappointing. Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# 3. Missing items - some verifiable, some suspicious\n",
    "for oid in all_order_ids[12:16]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My entire falafel bowl was missing from the order! Order: {oid}\",\n",
    "        f\"No drinks or sides in my order {oid}\",\n",
    "        f\"Missing my gyro from order {oid}\",\n",
    "        f\"You forgot half my items. {oid}\",\n",
    "    ])\n",
    "\n",
    "# 4. Items claimed that might not match the order\n",
    "for oid in all_order_ids[16:18]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Where are my chicken wings?! Order {oid}\",\n",
    "        f\"Missing my pizza from order {oid}\",\n",
    "    ])\n",
    "\n",
    "# 5. Service issues - should escalate\n",
    "for oid in all_order_ids[18:20]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Your driver was extremely rude to me. Order: {oid}\",\n",
    "        f\"Driver left my food at wrong address. Order: {oid}\",\n",
    "        f\"The delivery person refused to come to my door. {oid}\",\n",
    "    ])\n",
    "\n",
    "# 6. Multiple issues in one complaint\n",
    "for oid in all_order_ids[20:22]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Order {oid} was late AND missing items AND cold!\",\n",
    "        f\"Late delivery, rude driver, and food quality was poor. Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# 7. Escalation triggers - legal threats, health concerns\n",
    "for oid in all_order_ids[22:24]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"I'm calling my lawyer about this terrible service! Order: {oid}\",\n",
    "        f\"This food made me sick, possible food poisoning. Order: {oid}\",\n",
    "        f\"Found a piece of plastic in my food! Order {oid} - this is dangerous!\",\n",
    "    ])\n",
    "\n",
    "# 8. Vague complaints without specifics\n",
    "for oid in all_order_ids[24:26]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Not satisfied with order {oid}\",\n",
    "        f\"Bad experience. {oid}\",\n",
    "        f\"Order {oid} was wrong\",\n",
    "    ])\n",
    "\n",
    "# 9. Billing/promo issues\n",
    "for oid in all_order_ids[26:28]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My promo code didn't work on order {oid}\",\n",
    "        f\"I was charged twice for order {oid}!\",\n",
    "    ])\n",
    "\n",
    "# 10. Edge cases - no order ID or invalid format\n",
    "complaint_scenarios.extend([\n",
    "    \"My order was really late and the food was cold!\",  # Missing order ID\n",
    "    \"terrible service, do better\",  # No order ID, vague\n",
    "    \"Order ABC123 never arrived\",  # Invalid order ID format\n",
    "])\n",
    "\n",
    "# 11. Non-complaints / comments\n",
    "for oid in all_order_ids[28:30]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Great food, loved it! Order {oid}\",  # Positive comment\n",
    "        f\"Just wanted to say the driver was very polite today. {oid}\",  # Positive feedback\n",
    "        f\"How do I reorder my previous order {oid}?\",  # Question, not complaint\n",
    "    ])\n",
    "\n",
    "# 12. Unfounded complaints (on-time delivery but claiming delay)\n",
    "for oid in all_order_ids[30:32]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"This took way too long! Order {oid}\",\n",
    "    ])\n",
    "\n",
    "# Sample for reasonable eval size (aim for ~25-30 scenarios)\n",
    "complaint_scenarios = random.sample(complaint_scenarios, min(30, len(complaint_scenarios)))\n",
    "\n",
    "# Wrap in correct input schema for ResponsesAgent\n",
    "data = []\n",
    "for complaint in complaint_scenarios:\n",
    "    data.append({\n",
    "        \"inputs\": {\n",
    "            \"input\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": complaint\n",
    "            }]\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(f\"Created {len(data)} diverse evaluation scenarios including:\")\n",
    "print(\"  - Delivery delays (legitimate & questionable)\")\n",
    "print(\"  - Food quality issues (specific & vague)\")\n",
    "print(\"  - Missing items (verifiable & suspicious)\")\n",
    "print(\"  - Service complaints\")\n",
    "print(\"  - Escalation triggers (health/legal)\")\n",
    "print(\"  - Edge cases (no order ID, invalid ID)\")\n",
    "print(\"  - Non-complaints (positive feedback, questions)\")\n",
    "print(\"  - Multiple issues in one complaint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9l6zrp5n03b",
   "metadata": {},
   "outputs": [],
   "source": "# Create three focused scorers and run evaluation\n\nfrom mlflow.genai.scorers import Guidelines\nimport mlflow\nimport sys\nimport os\nsys.path.append(os.getcwd())\n\nnotebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\nproject_directory = os.path.dirname(notebook_path)\nsys.path.append(project_directory)\n\nfrom agent import AGENT\n\n# Scorer 1: Evidence-Based Reasoning - Does the rationale support the decision?\nevidence_reasoning = Guidelines(\n    name=\"evidence_reasoning\",\n    guidelines=[\n        \"The rationale should provide specific evidence or reasoning for the decision made\",\n        \"For timing complaints, rationale should mention delivery times or timing data if available\",\n        \"For missing item complaints, rationale should reference the items in question\",\n        \"For escalations, rationale should explain why human judgment is needed\",\n        \"Rationale does not need to be overly detailed, but should demonstrate that some analysis was done\",\n        \"It's acceptable if data wasn't available - the rationale should acknowledge this\"\n    ]\n)\n\n# Scorer 2: Credit Amount Reasonableness - Is the refund amount appropriate?\ncredit_reasonableness = Guidelines(\n    name=\"credit_reasonableness\",\n    guidelines=[\n        \"If decision='escalate', automatically pass this check (no credit amount to evaluate)\",\n        \"If decision='suggest_credit' with credit_amount > $0, the amount should be reasonable for a food delivery order (typically $5-$50)\",\n        \"Credit amounts over $50 should only be for severe issues mentioned in the rationale\",\n        \"A credit_amount of $0 is valid when the rationale indicates no issue was found\",\n        \"The credit amount should be proportional to the severity of the issue described\",\n        \"Missing items should roughly correspond to typical item costs ($8-$15 per item)\",\n        \"Delivery delay credits should scale with the severity described\"\n    ]\n)\n\n# Scorer 3: Decision Metadata Consistency - Are confidence and priority used correctly?\ndecision_metadata = Guidelines(\n    name=\"decision_metadata\",\n    guidelines=[\n        \"If decision='suggest_credit': confidence should be present (high/medium/low) and priority should be null\",\n        \"If decision='escalate': priority should be present (standard/urgent) and confidence should be null\",\n        \"URGENT priority is ONLY for serious threats: food poisoning/illness, foreign objects in food, severe allergic reactions, legal threats, or abusive/threatening language\",\n        \"STANDARD priority is appropriate for: service complaints (rude driver, wrong address), vague complaints, billing issues, generic food quality complaints (cold, soggy, bland, overcooked), or missing data\",\n        \"Generic food quality complaints (tastes bad, cold, soggy, overcooked) are NOT health/safety concerns and should use priority='standard' if escalated\",\n        \"High confidence should have clear supporting evidence in the rationale; low confidence is appropriate when data is limited\",\n        \"The complaint_category field does not determine priority - only the actual severity of the issue matters\"\n    ]\n)\n\n# ResponsesAgent predict function wrapper for evaluation\n# Note: parameter name must match the key in eval data (\"input\")\ndef predict_fn(input):\n    from mlflow.types.responses import ResponsesAgentRequest\n    request = ResponsesAgentRequest(input=input)\n    response = AGENT.predict(request)\n    # Extract text from ResponsesAgent output structure\n    # output[-1] is the final structured response\n    output_item = response.output[-1]\n    if hasattr(output_item, 'content') and output_item.content:\n        return output_item.content[0][\"text\"]\n    return str(output_item)\n\n# Run evaluation with three focused scorers\nresults = mlflow.genai.evaluate(\n    data=data,\n    scorers=[evidence_reasoning, credit_reasonableness, decision_metadata],\n    predict_fn=predict_fn\n)\n\nprint(f\"✅ Evaluation complete with 3 focused scorers.\")\nprint(f\"   Scorers: evidence_reasoning, credit_reasonableness, decision_metadata\")"
  },
  {
   "cell_type": "markdown",
   "id": "ttse3kj3pcj",
   "metadata": {},
   "source": [
    "#### Log the agent to `UC`\n",
    "\n",
    "- Point MLflow at the Unity Catalog registry and name the artifact `${CATALOG}.ai.complaint_agent`.\n",
    "- Register the run-produced model so versioned deployments can be promoted through UC stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y8lfco9zzn",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "UC_MODEL_NAME = f\"{CATALOG}.ai.complaint_agent\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "me3m6ovfqkd",
   "metadata": {},
   "source": [
    "#### deploy the agent to model serving\n",
    "\n",
    "- Ensure a production MLflow experiment exists for live trace capture.\n",
    "- Call `agents.deploy` to create/update the Databricks Model Serving endpoint backed by the UC model version.\n",
    "- Wait until the serving endpoint reports READY before continuing to downstream steps.\n",
    "- Pass the prod experiment ID via environment variables so inference logs are persisted automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qjnjjztbbna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Create prod experiment for production inference traces\n",
    "# Use shared path for job compatibility and visibility\n",
    "prod_experiment_name = f\"/Shared/{CATALOG}_complaint_agent_prod\"\n",
    "\n",
    "# set_experiment creates the experiment if it doesn't exist, or activates it if it does\n",
    "prod_experiment = mlflow.set_experiment(prod_experiment_name)\n",
    "prod_experiment_id = prod_experiment.experiment_id\n",
    "print(f\"✅ Using prod experiment: {prod_experiment_name} (ID: {prod_experiment_id})\")\n",
    "\n",
    "# Add experiment to UC state for cleanup\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "experiment_data = {\n",
    "    \"experiment_id\": prod_experiment_id,\n",
    "    \"name\": prod_experiment_name\n",
    "}\n",
    "add(CATALOG, \"experiments\", experiment_data)\n",
    "print(f\"✅ Added prod experiment to UC state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exckljo2zx4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from databricks import agents\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointStateReady\n",
    "\n",
    "endpoint_name = dbutils.widgets.get(\"COMPLAINT_AGENT_ENDPOINT_NAME\")\n",
    "deployment_info = agents.deploy(\n",
    "    model_name=UC_MODEL_NAME,\n",
    "    model_version=uc_registered_model_info.version,\n",
    "    scale_to_zero=False,\n",
    "    endpoint_name=endpoint_name,\n",
    "    environment_vars={\"MLFLOW_EXPERIMENT_ID\": str(prod_experiment_id)},\n",
    ")\n",
    "\n",
    "workspace = WorkspaceClient()\n",
    "ready_endpoint = workspace.serving_endpoints.wait_get_serving_endpoint_not_updating(\n",
    "    name=endpoint_name,\n",
    "    timeout=timedelta(minutes=30),\n",
    ")\n",
    "\n",
    "if ready_endpoint.state.ready != EndpointStateReady.READY:\n",
    "    raise RuntimeError(\n",
    "        f\"Endpoint {endpoint_name} is {ready_endpoint.state.ready} after deployment; retry or investigate.\"\n",
    "    )\n",
    "\n",
    "print(f\"✅ Endpoint {endpoint_name} is READY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i1syfkwcivl",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deployment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4i9yj8vjs2",
   "metadata": {},
   "source": [
    "##### Record model in state\n",
    "\n",
    "- Store the new deployment metadata with `uc_state.add` to facilitate cleanup in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ihnhnv5plw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also add to UC-state\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "add(dbutils.widgets.get(\"CATALOG\"), \"endpoints\", deployment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bhjj7zuan9g",
   "metadata": {},
   "source": [
    "#### production monitoring\n",
    "\n",
    "- Scaffold MLflow guideline scorers for sampled live traffic to flag decision drift or policy regressions.\n",
    "- Keep monitoring hooks commented until the serving endpoint is stable, then enable to automate QA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v1j0a0y21ct",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Guidelines, ScorerSamplingConfig\n",
    "\n",
    "# Register scorers for production monitoring (10% sampling)\n",
    "decision_quality_monitor = Guidelines(\n",
    "    name=\"decision_quality_prod\",\n",
    "    guidelines=[\n",
    "        \"Food quality complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Missing item complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Legal threats or serious health concerns should be classified as 'escalate'\"\n",
    "    ]\n",
    ").register(name=f\"{UC_MODEL_NAME}_decision_quality\")\n",
    "\n",
    "refund_reason_monitor = Guidelines(\n",
    "    name=\"refund_reason_prod\",\n",
    "    guidelines=[\n",
    "        \"If a refund is offered, it must clearly relate to the complaint made by the user\"\n",
    "    ]\n",
    ").register(name=f\"{UC_MODEL_NAME}_refund_reason\")\n",
    "\n",
    "# Start monitoring with 10% sampling of production traffic\n",
    "decision_quality_monitor = decision_quality_monitor.start(\n",
    "    sampling_config=ScorerSamplingConfig(sample_rate=0.1)\n",
    ")\n",
    "\n",
    "refund_reason_monitor = refund_reason_monitor.start(\n",
    "    sampling_config=ScorerSamplingConfig(sample_rate=0.1)\n",
    ")\n",
    "\n",
    "print(\"✅ Production monitoring enabled with 10% sampling\")\n",
    "print(f\"   - decision_quality scorer monitoring: {decision_quality_monitor}\")\n",
    "print(f\"   - refund_reason scorer monitoring: {refund_reason_monitor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}