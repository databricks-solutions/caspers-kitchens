{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "#### complaint agent\n",
    "\n",
    "Builds and ships an order-complaint agent: author tools, assemble the LangGraph workflow, evaluate it, and promote the packaged model into production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "#### Tool & View Registration\n",
    "\n",
    "- `CREATE SCHEMA` guarantees the shared `${CATALOG}.ai` workspace exists for agent assets.\n",
    "- `order_delivery_times_per_location_view` summarizes delivery percentiles per brand/location.\n",
    "- `get_order_overview(oid)` returns structured order metadata, items, and customer info.\n",
    "- `get_order_timing(oid)` exposes created/delivered timestamps plus transit duration.\n",
    "- `get_location_timings(loc)` yields P50/P75/P99 delivery benchmarks for benchmarking complaints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS ${CATALOG}.ai;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tdy6yy3gheg",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW ${CATALOG}.ai.order_delivery_times_per_location_view AS\n",
    "WITH order_times AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    MAX(CASE WHEN event_type = 'order_created' THEN try_to_timestamp(ts) END) AS order_created_time,\n",
    "    MAX(CASE WHEN event_type = 'delivered' THEN try_to_timestamp(ts) END) AS delivered_time\n",
    "  FROM\n",
    "    ${CATALOG}.lakeflow.all_events\n",
    "  WHERE\n",
    "    try_to_timestamp(ts) >= CURRENT_TIMESTAMP() - INTERVAL 1 DAY\n",
    "  GROUP BY\n",
    "    order_id,\n",
    "    location\n",
    "),\n",
    "total_order_times AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    (UNIX_TIMESTAMP(delivered_time) - UNIX_TIMESTAMP(order_created_time)) / 60 AS total_order_time_minutes\n",
    "  FROM\n",
    "    order_times\n",
    "  WHERE\n",
    "    order_created_time IS NOT NULL\n",
    "    AND delivered_time IS NOT NULL\n",
    ")\n",
    "SELECT\n",
    "  location,\n",
    "  PERCENTILE(total_order_time_minutes, 0.50) AS P50,\n",
    "  PERCENTILE(total_order_time_minutes, 0.75) AS P75,\n",
    "  PERCENTILE(total_order_time_minutes, 0.99) AS P99\n",
    "FROM\n",
    "  total_order_times\n",
    "GROUP BY\n",
    "  location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_overview(oid STRING COMMENT 'The unique order identifier to retrieve information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  location STRING COMMENT 'Order location',\n",
    "  items_json STRING COMMENT 'JSON array of ordered items with details',\n",
    "  customer_address STRING COMMENT 'Customer delivery address',\n",
    "  brand_id BIGINT COMMENT 'Brand ID for the order',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created'\n",
    ")\n",
    "COMMENT 'Returns basic order information including items, location, and customer details'\n",
    "RETURN\n",
    "  WITH order_created_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      location,\n",
    "      get_json_object(body, '$.items') as items_json,\n",
    "      get_json_object(body, '$.customer_addr') as customer_address,\n",
    "      -- Extract brand_id from first item in the order\n",
    "      CAST(get_json_object(get_json_object(body, '$.items[0]'), '$.brand_id') AS BIGINT) as brand_id,\n",
    "      try_to_timestamp(ts) as order_created_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid AND event_type = 'order_created'\n",
    "    LIMIT 1\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    items_json,\n",
    "    customer_address,\n",
    "    brand_id,\n",
    "    order_created_ts\n",
    "  FROM order_created_events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_timing(oid STRING COMMENT 'The unique order identifier to get timing information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created',\n",
    "  delivered_ts TIMESTAMP COMMENT 'When the order was delivered (NULL if not delivered)',\n",
    "  delivery_duration_minutes FLOAT COMMENT 'Time from order creation to delivery in minutes (NULL if not delivered)',\n",
    "  delivery_status STRING COMMENT 'Current delivery status: delivered, in_progress, or unknown'\n",
    ")\n",
    "COMMENT 'Returns timing information for a specific order'\n",
    "RETURN\n",
    "  WITH order_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      event_type,\n",
    "      try_to_timestamp(ts) as event_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid\n",
    "  ),\n",
    "  timing_summary AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      MIN(CASE WHEN event_type = 'order_created' THEN event_ts END) as order_created_ts,\n",
    "      MAX(CASE WHEN event_type = 'delivered' THEN event_ts END) as delivered_ts\n",
    "    FROM order_events\n",
    "    GROUP BY order_id\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    order_created_ts,\n",
    "    delivered_ts,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL AND order_created_ts IS NOT NULL THEN\n",
    "        CAST((UNIX_TIMESTAMP(delivered_ts) - UNIX_TIMESTAMP(order_created_ts)) / 60 AS FLOAT)\n",
    "      ELSE NULL\n",
    "    END as delivery_duration_minutes,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL THEN 'delivered'\n",
    "      WHEN order_created_ts IS NOT NULL THEN 'in_progress'\n",
    "      ELSE 'unknown'\n",
    "    END as delivery_status\n",
    "  FROM timing_summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_location_timings(loc STRING COMMENT 'Location name as a string')\n",
    "RETURNS TABLE (\n",
    "  location STRING COMMENT 'Location of the order source',\n",
    "  P50 FLOAT COMMENT '50th percentile delivery time in minutes',\n",
    "  P75 FLOAT COMMENT '75th percentile delivery time in minutes',\n",
    "  P99 FLOAT COMMENT '99th percentile delivery time in minutes'\n",
    ")\n",
    "COMMENT 'Returns the 50/75/99th percentile of delivery times for a location to benchmark order timing'\n",
    "RETURN\n",
    "  SELECT location, P50, P75, P99\n",
    "  FROM ${CATALOG}.ai.order_delivery_times_per_location_view AS odlt\n",
    "  WHERE odlt.location = loc;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dq5ml4wp6v",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "- Install LangGraph, Databricks agent packages, and restart Python for a clean runtime.\n",
    "- Capture widget inputs (`CATALOG`, `LLM_MODEL`) and create an MLflow dev experiment for trace logging.\n",
    "- Define a templated `%%writefilev` magic that emits files with notebook variable substitution.\n",
    "- Materialize `agent.py` containing the LangGraph complaint workflow wired to UC SQL tools and the chosen LLM endpoint.\n",
    "- Pull a delivered `order_id` sample and build the MLflow model signature/resources for logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3tu3r2gso",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow-skinny[databricks] unitycatalog-openai[databricks] openai databricks-sdk databricks-agents uv\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dl04kgwv9ap",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "LLM_MODEL = dbutils.widgets.get(\"LLM_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ktoahik8tl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Create/set dev experiment for development and evaluation traces\n",
    "# Use shared path for job compatibility\n",
    "dev_experiment_name = f\"/Shared/{CATALOG}_complaint_agent_dev\"\n",
    "\n",
    "# set_experiment creates the experiment if it doesn't exist, or activates it if it does\n",
    "dev_experiment = mlflow.set_experiment(dev_experiment_name)\n",
    "dev_experiment_id = dev_experiment.experiment_id\n",
    "print(f\"✅ Using dev experiment: {dev_experiment_name} (ID: {dev_experiment_id})\")\n",
    "\n",
    "# Add experiment to UC state for cleanup\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "experiment_data = {\n",
    "    \"experiment_id\": dev_experiment_id,\n",
    "    \"name\": dev_experiment_name\n",
    "}\n",
    "add(CATALOG, \"experiments\", experiment_data)\n",
    "print(f\"✅ Added dev experiment to UC state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bruu85upqq5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def writefilev(line, cell):\n",
    "    \"\"\"\n",
    "    %%writefilev file.py\n",
    "    Allows {{var}} substitutions while leaving normal {} intact.\n",
    "    \"\"\"\n",
    "    filename = line.strip()\n",
    "\n",
    "    def replacer(match):\n",
    "        expr = match.group(1)\n",
    "        return str(eval(expr, globals(), locals()))\n",
    "\n",
    "    # Replace only double braces {{var}}\n",
    "    content = re.sub(r\"\\{\\{(.*?)\\}\\}\", replacer, cell)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Wrote file with substitutions: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2c70l4ca8",
   "metadata": {},
   "outputs": [],
   "source": "%%writefilev agent.py\nimport json\nimport warnings\nfrom typing import Optional, Literal\nfrom uuid import uuid4\nfrom pydantic import BaseModel\n\n# Suppress databricks-connect Spark session warning\nwarnings.filterwarnings(\"ignore\", message=\".*Ignoring the default notebook Spark session.*\")\n\nimport mlflow\nfrom mlflow.entities import SpanType\nfrom databricks.sdk import WorkspaceClient\nfrom unitycatalog.ai.core.base import get_uc_function_client\nfrom unitycatalog.ai.openai.toolkit import UCFunctionToolkit\nfrom mlflow.pyfunc import ResponsesAgent\nfrom mlflow.types.responses import (\n    ResponsesAgentRequest,\n    ResponsesAgentResponse,\n)\n\n# Disable autologging - we need custom span nesting for tool calls\nmlflow.openai.autolog(disable=True)\n\nLLM_MODEL = \"{{LLM_MODEL}}\"\nCATALOG = \"{{CATALOG}}\"\n\n# Initialize OpenAI client via Databricks SDK\nworkspace = WorkspaceClient()\nopenai_client = workspace.serving_endpoints.get_open_ai_client()\n\n# Initialize UC function client and tools\nuc_client = get_uc_function_client()\nuc_function_names = [\n    f\"{CATALOG}.ai.get_order_overview\",\n    f\"{CATALOG}.ai.get_order_timing\",\n    f\"{CATALOG}.ai.get_location_timings\",\n]\nuc_toolkit = UCFunctionToolkit(function_names=uc_function_names)\ntools = uc_toolkit.tools\n\n# Map sanitized tool names back to UC function names\n# OpenAI Chat Completions sanitizes function names (dots -> __)\ntool_name_to_uc_function = {}\nfor tool, uc_name in zip(tools, uc_function_names):\n    tool_name_to_uc_function[tool[\"function\"][\"name\"]] = uc_name\n\n\nclass ComplaintResponse(BaseModel):\n    order_id: str\n    complaint_category: Literal[\"delivery_delay\", \"missing_items\", \"food_quality\", \"service_issue\", \"billing\", \"other\"]\n    decision: Literal[\"suggest_credit\", \"escalate\"]\n    credit_amount: Optional[float] = None\n    confidence: Optional[Literal[\"high\", \"medium\", \"low\"]] = None\n    priority: Optional[Literal[\"standard\", \"urgent\"]] = None\n    rationale: str\n\n\nLLM_ENDPOINT_NAME = LLM_MODEL\n\nsystem_prompt = \"\"\"You are the Complaint Triage Agent for Casper's Kitchens, a ghost kitchen network. Your job is to analyze customer complaints and recommend actions for internal customer service staff.\n\nPROCESS:\n1. Extract the order_id from the complaint text\n2. Call get_order_overview(order_id) to get order details and items\n3. Call get_order_timing(order_id) to get delivery timing\n4. For delivery delay complaints, call get_location_timings(location) to get percentile benchmarks\n5. Analyze the data and make a recommendation\n\nDECISION FRAMEWORK:\n\nSUGGEST_CREDIT - Use when you can make a data-backed recommendation:\n\nDelivery delays (high confidence when data-backed):\n- Compare actual delivery time to location percentiles\n- >P75 but <P90: Suggest 15% of order total\n- >P90 but <P99: Suggest 25% of order total  \n- >P99: Suggest 50% of order total\n- On-time delivery: Suggest $0 credit (low confidence - might be other issues)\n\nMissing items:\n- Parse items_json from get_order_overview to find actual item prices\n- Use real item costs when available for accurate refund\n- Check if claimed missing item actually makes sense for this order (cross-reference with items list)\n- If claimed item not in order or doesn't match (e.g., claiming missing fries on salad-only order): affects confidence negatively\n- If item plausibly missing but can't verify price: estimate $8-12 per item (medium confidence)\n- If no evidence of missing items but customer claims it: $0 credit (low confidence)\n\nFood quality issues:\n- If specific details provided (cold, soggy, wrong preparation): $10-15 (medium confidence)\n- If vague (\"bad\", \"gross\"): escalate with priority=\"standard\" instead\n- If food quality complaint but order shows delivery delay >P90: consider combined credit\n\nESCALATE - Use when human judgment is needed:\n- priority=\"standard\": \n  * Vague complaints without specifics (\"food was bad\", \"not happy\")\n  * Missing or incomplete order data\n  * Edge cases that don't fit clear patterns\n  * Billing issues or promo code problems\n  * Service complaints (rude driver, wrong location)\n- priority=\"urgent\":\n  * Legal threats or mentions of lawyers\n  * Health/safety concerns (allergies, food poisoning, foreign objects)\n  * Suspected fraud or inconsistencies in complaint vs order data\n  * Abusive or threatening language\n\nOUTPUT RULES:\n- For suggest_credit: Include credit_amount (can be $0) and confidence, set priority to null\n- For escalate: Include priority, set credit_amount and confidence to null\n- Always include a data-focused rationale citing specific numbers (delivery times, percentiles, order details, item verification)\n- Rationale is for internal staff - be factual, not apologetic\n\nRATIONALE GUIDELINES:\n- Rationale should clearly articulate the justification for the decision\n- All evidence used to support the decision should be cited in the rationale. Rationale MUST cite evidence.\n- Rationale should be detailed and specific, but no longer than 150 words\n- Rationale should clearly justify the decision, credit amount (if applicable), confidence level, and priority (if applicable)\n- If a refund is suggested, the rationale should clearly justify the credit amount, based on the evidence provided.\n\nCONFIDENCE GUIDELINES:\n- high: Strong data support (delivery >P90, item prices from order data, clear verification)\n- medium: Reasonable inference (delivery P75-P90, estimated item costs, plausible but unverified)\n- low: Weak or contradictory evidence ($0 credits when data doesn't support claim, can't verify complaint details)\n\nIMPORTANT NOTES:\n- If order_id not found or data unavailable: escalate with priority=\"standard\"\n- Round credit amounts to nearest $0.50\n- When in doubt between two options, prefer escalate with priority=\"standard\"\n- A suggest_credit with $0 and low confidence is valid when complaint seems unfounded\"\"\"\n\nMAX_ITERATIONS = 10\n\n# Generate JSON schema for structured output\n# Databricks requires: additionalProperties=false and all fields in required array\nschema = ComplaintResponse.model_json_schema()\ncomplaint_response_schema = {\n    \"type\": \"json_schema\",\n    \"json_schema\": {\n        \"name\": \"ComplaintResponse\",\n        \"schema\": {\n            **schema,\n            \"additionalProperties\": False,\n            \"required\": list(schema[\"properties\"].keys())\n        },\n        \"strict\": True\n    }\n}\n\n\ndef _call_uc_tool(uc_function_name: str, arguments: dict):\n    \"\"\"Execute UC tool with automatic tracing.\"\"\"\n    with mlflow.start_span(name=uc_function_name, span_type=SpanType.TOOL):\n        result = uc_client.execute_function(uc_function_name, arguments)\n        return result\n\n\nclass ComplaintResponsesAgent(ResponsesAgent):\n    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n        # Check if there's an auto-generated \"predict\" span from pyfunc\n        existing_span = mlflow.get_current_active_span()\n        \n        # Extract first user message and build messages list\n        first_user_message = None\n        for msg in request.input:\n            msg_dict = msg.model_dump() if hasattr(msg, \"model_dump\") else msg\n            if msg_dict.get(\"role\") == \"user\":\n                first_user_message = msg_dict.get(\"content\", \"\")\n                break\n        \n        messages = [{\"role\": \"system\", \"content\": system_prompt}] + [\n            msg.model_dump() if hasattr(msg, \"model_dump\") else msg for msg in request.input\n        ]\n        \n        if existing_span:\n            # Hijack the auto-generated \"predict\" span\n            existing_span.set_span_type(SpanType.CHAIN)\n            existing_span.set_inputs({\"messages\": messages})\n            root_span = existing_span\n            span_context = None  # Don't create a new context\n        else:\n            # No existing span, create our own\n            span_context = mlflow.start_span(name=\"Complaint Triage Agent\", span_type=SpanType.CHAIN)\n            root_span = span_context.__enter__()\n            root_span.set_inputs({\"messages\": messages})\n        \n        try:\n            # Agentic loop: call completions with tools until no tool calls\n            for _ in range(MAX_ITERATIONS):\n                with mlflow.start_span(name=\"chat.completions.create\", span_type=SpanType.CHAT_MODEL) as llm_span:\n                    response = openai_client.chat.completions.create(\n                        model=LLM_ENDPOINT_NAME,\n                        messages=messages,\n                        tools=tools,\n                    )\n                    \n                    llm_span.set_inputs({\n                        \"messages\": messages,\n                        \"model\": LLM_ENDPOINT_NAME,\n                        \"tools\": tools\n                    })\n                    llm_span.set_outputs(response)\n                    \n                    message = response.choices[0].message\n                    \n                    if message.tool_calls:\n                        # Add assistant message with tool calls to conversation\n                        messages.append(message.model_dump(exclude_none=True))\n                        \n                        # Execute each tool call (creates nested TOOL spans)\n                        for tool_call in message.tool_calls:\n                            uc_function_name = tool_name_to_uc_function.get(\n                                tool_call.function.name,\n                                tool_call.function.name\n                            )\n                            \n                            result = _call_uc_tool(\n                                uc_function_name,\n                                json.loads(tool_call.function.arguments)\n                            )\n                            \n                            messages.append({\n                                \"role\": \"tool\",\n                                \"content\": json.dumps({\"content\": result.value}),\n                                \"tool_call_id\": tool_call.id\n                            })\n                    else:\n                        # No tool calls - ready for structured output\n                        break\n            else:\n                # Hit max iterations without completion\n                raise RuntimeError(f\"Max iterations ({MAX_ITERATIONS}) reached without completion\")\n            \n            # Get final structured output\n            with mlflow.start_span(name=\"chat.completions.create\", span_type=SpanType.CHAT_MODEL) as final_span:\n                final_response = openai_client.chat.completions.create(\n                    model=LLM_ENDPOINT_NAME,\n                    messages=messages,\n                    response_format=complaint_response_schema,\n                )\n                \n                final_span.set_inputs({\n                    \"messages\": messages,\n                    \"model\": LLM_ENDPOINT_NAME,\n                    \"response_format\": complaint_response_schema\n                })\n                final_span.set_outputs(final_response)\n            \n            # Parse and validate structured output\n            parsed = ComplaintResponse.model_validate_json(\n                final_response.choices[0].message.content\n            )\n            \n            root_span.set_outputs(parsed.model_dump())\n            \n            return ResponsesAgentResponse(\n                output=[self.create_text_output_item(text=parsed.model_dump_json(), id=str(uuid4()))],\n                custom_outputs=request.custom_inputs\n            )\n        finally:\n            # Only exit the span context if we created it\n            if span_context:\n                span_context.__exit__(None, None, None)\n\n\nAGENT = ComplaintResponsesAgent()\nmlflow.models.set_model(AGENT)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6gjrp4mx6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an actual order_id for input example\n",
    "sample_order_id = spark.sql(f\"\"\"\n",
    "    SELECT order_id \n",
    "    FROM {CATALOG}.lakeflow.all_events \n",
    "    WHERE event_type='delivered'\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0]['order_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ih3tt5qeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sample_order_id is not None\n",
    "print(sample_order_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23o4h7j6amzh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    "# Add UC function resources directly\n",
    "uc_tool_names = [\n",
    "    f\"{CATALOG}.ai.get_order_overview\",\n",
    "    f\"{CATALOG}.ai.get_order_timing\",\n",
    "    f\"{CATALOG}.ai.get_location_timings\",\n",
    "]\n",
    "for func_name in uc_tool_names:\n",
    "    resources.append(DatabricksFunction(function_name=func_name))\n",
    "\n",
    "input_example = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"My order was really late! Order ID: {sample_order_id}\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"complaint_agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        input_example=input_example,\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
    "            \"mlflow==3.6.0\",\n",
    "            f\"unitycatalog-openai[databricks]=={get_distribution('unitycatalog-openai').version}\",\n",
    "            f\"openai=={get_distribution('openai').version}\",\n",
    "            f\"databricks-sdk=={get_distribution('databricks-sdk').version}\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "mlflow.set_active_model(model_id = logged_agent_info.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gfympowd7q",
   "metadata": {},
   "source": [
    "#### Evaluate the Agent\n",
    "\n",
    "- Synthesize diverse complaint scenarios (delivery delays, wrong items, missing items, etc.) from recent orders.\n",
    "- Configure multiple MLflow `Guidelines` scorers that check refund reason, messaging quality, and policy compliance.\n",
    "- Run batch evaluations against the agent to quantify decision quality before promotion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2loovjto96g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive complaint scenarios for evaluation\n",
    "import random\n",
    "\n",
    "# Get sample order IDs for different scenarios\n",
    "all_order_ids = [\n",
    "    row['order_id'] for row in spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT order_id \n",
    "        FROM {CATALOG}.lakeflow.all_events \n",
    "        WHERE event_type='delivered'\n",
    "        LIMIT 50\n",
    "    \"\"\").collect()\n",
    "]\n",
    "\n",
    "# Create diverse, realistic complaint scenarios\n",
    "complaint_scenarios = []\n",
    "\n",
    "# 1. Delivery delay complaints - mix of legitimate and questionable\n",
    "for oid in all_order_ids[:8]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My order took forever to arrive! Order ID: {oid}\",\n",
    "        f\"Been waiting 2 hours, this is ridiculous. Order {oid}\",\n",
    "        f\"Order {oid} arrived late and cold\",\n",
    "        f\"Delivery was slower than usual for order {oid}\",\n",
    "    ])\n",
    "\n",
    "# 2. Food quality issues - range from specific to vague\n",
    "for oid in all_order_ids[8:12]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My falafel was completely soggy and inedible. Order: {oid}\",\n",
    "        f\"The food was cold when it arrived, very disappointing. Order: {oid}\",\n",
    "        f\"Everything tasted bad. Order {oid}\",\n",
    "        f\"Not happy with the quality. {oid}\",\n",
    "        f\"The gyro meat was overcooked and dry, very disappointing. Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# 3. Missing items - some verifiable, some suspicious\n",
    "for oid in all_order_ids[12:16]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My entire falafel bowl was missing from the order! Order: {oid}\",\n",
    "        f\"No drinks or sides in my order {oid}\",\n",
    "        f\"Missing my gyro from order {oid}\",\n",
    "        f\"You forgot half my items. {oid}\",\n",
    "    ])\n",
    "\n",
    "# 4. Items claimed that might not match the order\n",
    "for oid in all_order_ids[16:18]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Where are my chicken wings?! Order {oid}\",\n",
    "        f\"Missing my pizza from order {oid}\",\n",
    "    ])\n",
    "\n",
    "# 5. Service issues - should escalate\n",
    "for oid in all_order_ids[18:20]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Your driver was extremely rude to me. Order: {oid}\",\n",
    "        f\"Driver left my food at wrong address. Order: {oid}\",\n",
    "        f\"The delivery person refused to come to my door. {oid}\",\n",
    "    ])\n",
    "\n",
    "# 6. Multiple issues in one complaint\n",
    "for oid in all_order_ids[20:22]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Order {oid} was late AND missing items AND cold!\",\n",
    "        f\"Late delivery, rude driver, and food quality was poor. Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# 7. Escalation triggers - legal threats, health concerns\n",
    "for oid in all_order_ids[22:24]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"I'm calling my lawyer about this terrible service! Order: {oid}\",\n",
    "        f\"This food made me sick, possible food poisoning. Order: {oid}\",\n",
    "        f\"Found a piece of plastic in my food! Order {oid} - this is dangerous!\",\n",
    "    ])\n",
    "\n",
    "# 8. Vague complaints without specifics\n",
    "for oid in all_order_ids[24:26]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Not satisfied with order {oid}\",\n",
    "        f\"Bad experience. {oid}\",\n",
    "        f\"Order {oid} was wrong\",\n",
    "    ])\n",
    "\n",
    "# 9. Billing/promo issues\n",
    "for oid in all_order_ids[26:28]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My promo code didn't work on order {oid}\",\n",
    "        f\"I was charged twice for order {oid}!\",\n",
    "    ])\n",
    "\n",
    "# 10. Edge cases - no order ID or invalid format\n",
    "complaint_scenarios.extend([\n",
    "    \"My order was really late and the food was cold!\",  # Missing order ID\n",
    "    \"terrible service, do better\",  # No order ID, vague\n",
    "    \"Order ABC123 never arrived\",  # Invalid order ID format\n",
    "])\n",
    "\n",
    "# 11. Non-complaints / comments\n",
    "for oid in all_order_ids[28:30]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Great food, loved it! Order {oid}\",  # Positive comment\n",
    "        f\"Just wanted to say the driver was very polite today. {oid}\",  # Positive feedback\n",
    "        f\"How do I reorder my previous order {oid}?\",  # Question, not complaint\n",
    "    ])\n",
    "\n",
    "# 12. Unfounded complaints (on-time delivery but claiming delay)\n",
    "for oid in all_order_ids[30:32]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"This took way too long! Order {oid}\",\n",
    "    ])\n",
    "\n",
    "# Sample for reasonable eval size (aim for ~25-30 scenarios)\n",
    "complaint_scenarios = random.sample(complaint_scenarios, min(30, len(complaint_scenarios)))\n",
    "\n",
    "# Wrap in correct input schema for ResponsesAgent\n",
    "data = []\n",
    "for complaint in complaint_scenarios:\n",
    "    data.append({\n",
    "        \"inputs\": {\n",
    "            \"input\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": complaint\n",
    "            }]\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(f\"Created {len(data)} diverse evaluation scenarios including:\")\n",
    "print(\"  - Delivery delays (legitimate & questionable)\")\n",
    "print(\"  - Food quality issues (specific & vague)\")\n",
    "print(\"  - Missing items (verifiable & suspicious)\")\n",
    "print(\"  - Service complaints\")\n",
    "print(\"  - Escalation triggers (health/legal)\")\n",
    "print(\"  - Edge cases (no order ID, invalid ID)\")\n",
    "print(\"  - Non-complaints (positive feedback, questions)\")\n",
    "print(\"  - Multiple issues in one complaint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9l6zrp5n03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three focused scorers and run evaluation\n",
    "\n",
    "from mlflow.genai.scorers import Guidelines\n",
    "import mlflow\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "project_directory = os.path.dirname(notebook_path)\n",
    "sys.path.append(project_directory)\n",
    "\n",
    "from agent import AGENT\n",
    "\n",
    "# Scorer 1: Evidence-Based Reasoning - Does the rationale support the decision?\n",
    "evidence_reasoning = Guidelines(\n",
    "    name=\"evidence_reasoning\",\n",
    "    guidelines=[\n",
    "        \"The rationale should provide specific evidence or reasoning for the decision made\",\n",
    "        \"For timing complaints, rationale should mention delivery times or timing data if available\",\n",
    "        \"For missing item complaints, rationale should reference the items in question\",\n",
    "        \"For escalations, rationale should explain why human judgment is needed\",\n",
    "        \"Rationale does not need to be overly detailed, but should demonstrate that some analysis was done\",\n",
    "        \"It's acceptable if data wasn't available - the rationale should acknowledge this\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scorer 2: Credit Amount Reasonableness - Is the refund amount appropriate?\n",
    "credit_reasonableness = Guidelines(\n",
    "    name=\"credit_reasonableness\",\n",
    "    guidelines=[\n",
    "        \"If decision='escalate', automatically pass this check (no credit amount to evaluate)\",\n",
    "        \"If decision='suggest_credit' with credit_amount > $0, the amount should be reasonable for a food delivery order (typically $5-$50)\",\n",
    "        \"Credit amounts over $50 should only be for severe issues mentioned in the rationale\",\n",
    "        \"A credit_amount of $0 is valid when the rationale indicates no issue was found\",\n",
    "        \"The credit amount should be proportional to the severity of the issue described\",\n",
    "        \"Missing items should roughly correspond to typical item costs ($8-$15 per item)\",\n",
    "        \"Delivery delay credits should scale with the severity described\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scorer 3: Decision Metadata Consistency - Are confidence and priority used correctly?\n",
    "decision_metadata = Guidelines(\n",
    "    name=\"decision_metadata\",\n",
    "    guidelines=[\n",
    "        \"If decision='suggest_credit': confidence should be present (high/medium/low) and priority should be null\",\n",
    "        \"If decision='escalate': priority should be present (standard/urgent) and confidence should be null\",\n",
    "        \"URGENT priority is ONLY for serious threats: food poisoning/illness, foreign objects in food, severe allergic reactions, legal threats, or abusive/threatening language\",\n",
    "        \"STANDARD priority is appropriate for: service complaints (rude driver, wrong address), vague complaints, billing issues, generic food quality complaints (cold, soggy, bland, overcooked), or missing data\",\n",
    "        \"Generic food quality complaints (tastes bad, cold, soggy, overcooked) are NOT health/safety concerns and should use priority='standard' if escalated\",\n",
    "        \"High confidence should have clear supporting evidence in the rationale; low confidence is appropriate when data is limited\",\n",
    "        \"The complaint_category field does not determine priority - only the actual severity of the issue matters\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ResponsesAgent predict function wrapper for evaluation\n",
    "# Note: parameter name must match the key in eval data (\"input\")\n",
    "def predict_fn(input):\n",
    "    from mlflow.types.responses import ResponsesAgentRequest\n",
    "    request = ResponsesAgentRequest(input=input)\n",
    "    response = AGENT.predict(request)\n",
    "    # Extract text from ResponsesAgent output structure\n",
    "    # output[-1] is the final structured response\n",
    "    output_item = response.output[-1]\n",
    "    if hasattr(output_item, 'content') and output_item.content:\n",
    "        return output_item.content[0][\"text\"]\n",
    "    return str(output_item)\n",
    "\n",
    "# Run evaluation with three focused scorers\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=data,\n",
    "    scorers=[evidence_reasoning, credit_reasonableness, decision_metadata],\n",
    "    predict_fn=predict_fn\n",
    ")\n",
    "\n",
    "print(f\"✅ Evaluation complete with 3 focused scorers.\")\n",
    "print(f\"   Scorers: evidence_reasoning, credit_reasonableness, decision_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ttse3kj3pcj",
   "metadata": {},
   "source": [
    "#### Log the agent to `UC`\n",
    "\n",
    "- Point MLflow at the Unity Catalog registry and name the artifact `${CATALOG}.ai.complaint_agent`.\n",
    "- Register the run-produced model so versioned deployments can be promoted through UC stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y8lfco9zzn",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "UC_MODEL_NAME = f\"{CATALOG}.ai.complaint_agent\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "me3m6ovfqkd",
   "metadata": {},
   "source": [
    "#### deploy the agent to model serving\n",
    "\n",
    "- Ensure a production MLflow experiment exists for live trace capture.\n",
    "- Call `agents.deploy` to create/update the Databricks Model Serving endpoint backed by the UC model version.\n",
    "- Wait until the serving endpoint reports READY before continuing to downstream steps.\n",
    "- Pass the prod experiment ID via environment variables so inference logs are persisted automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qjnjjztbbna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Create prod experiment for production inference traces\n",
    "# Use shared path for job compatibility and visibility\n",
    "prod_experiment_name = f\"/Shared/{CATALOG}_complaint_agent_prod\"\n",
    "\n",
    "# set_experiment creates the experiment if it doesn't exist, or activates it if it does\n",
    "prod_experiment = mlflow.set_experiment(prod_experiment_name)\n",
    "prod_experiment_id = prod_experiment.experiment_id\n",
    "print(f\"✅ Using prod experiment: {prod_experiment_name} (ID: {prod_experiment_id})\")\n",
    "\n",
    "# Add experiment to UC state for cleanup\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "experiment_data = {\n",
    "    \"experiment_id\": prod_experiment_id,\n",
    "    \"name\": prod_experiment_name\n",
    "}\n",
    "add(CATALOG, \"experiments\", experiment_data)\n",
    "print(f\"✅ Added prod experiment to UC state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exckljo2zx4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from databricks import agents\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointStateReady\n",
    "\n",
    "endpoint_name = dbutils.widgets.get(\"COMPLAINT_AGENT_ENDPOINT_NAME\")\n",
    "deployment_info = agents.deploy(\n",
    "    model_name=UC_MODEL_NAME,\n",
    "    model_version=uc_registered_model_info.version,\n",
    "    scale_to_zero=False,\n",
    "    endpoint_name=endpoint_name,\n",
    "    environment_vars={\"MLFLOW_EXPERIMENT_ID\": str(prod_experiment_id)},\n",
    ")\n",
    "\n",
    "workspace = WorkspaceClient()\n",
    "ready_endpoint = workspace.serving_endpoints.wait_get_serving_endpoint_not_updating(\n",
    "    name=endpoint_name,\n",
    "    timeout=timedelta(minutes=30),\n",
    ")\n",
    "\n",
    "if ready_endpoint.state.ready != EndpointStateReady.READY:\n",
    "    raise RuntimeError(\n",
    "        f\"Endpoint {endpoint_name} is {ready_endpoint.state.ready} after deployment; retry or investigate.\"\n",
    "    )\n",
    "\n",
    "print(f\"✅ Endpoint {endpoint_name} is READY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i1syfkwcivl",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deployment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4i9yj8vjs2",
   "metadata": {},
   "source": [
    "##### Record model in state\n",
    "\n",
    "- Store the new deployment metadata with `uc_state.add` to facilitate cleanup in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ihnhnv5plw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also add to UC-state\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "add(dbutils.widgets.get(\"CATALOG\"), \"endpoints\", deployment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bhjj7zuan9g",
   "metadata": {},
   "source": [
    "#### production monitoring\n",
    "\n",
    "- Scaffold MLflow guideline scorers for sampled live traffic to flag decision drift or policy regressions.\n",
    "- Keep monitoring hooks commented until the serving endpoint is stable, then enable to automate QA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v1j0a0y21ct",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Guidelines, ScorerSamplingConfig\n",
    "\n",
    "# Register scorers for production monitoring (10% sampling)\n",
    "decision_quality_monitor = Guidelines(\n",
    "    name=\"decision_quality_prod\",\n",
    "    guidelines=[\n",
    "        \"Food quality complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Missing item complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Legal threats or serious health concerns should be classified as 'escalate'\"\n",
    "    ]\n",
    ").register(name=f\"{UC_MODEL_NAME}_decision_quality\")\n",
    "\n",
    "refund_reason_monitor = Guidelines(\n",
    "    name=\"refund_reason_prod\",\n",
    "    guidelines=[\n",
    "        \"If a refund is offered, it must clearly relate to the complaint made by the user\"\n",
    "    ]\n",
    ").register(name=f\"{UC_MODEL_NAME}_refund_reason\")\n",
    "\n",
    "# Start monitoring with 10% sampling of production traffic\n",
    "decision_quality_monitor = decision_quality_monitor.start(\n",
    "    sampling_config=ScorerSamplingConfig(sample_rate=0.1)\n",
    ")\n",
    "\n",
    "refund_reason_monitor = refund_reason_monitor.start(\n",
    "    sampling_config=ScorerSamplingConfig(sample_rate=0.1)\n",
    ")\n",
    "\n",
    "print(\"✅ Production monitoring enabled with 10% sampling\")\n",
    "print(f\"   - decision_quality scorer monitoring: {decision_quality_monitor}\")\n",
    "print(f\"   - refund_reason scorer monitoring: {refund_reason_monitor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}