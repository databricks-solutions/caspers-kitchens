{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "#### complaint agent\n\nBuilds and ships an order-complaint agent using DSPy: author Unity Catalog tools, assemble the DSPy ReAct workflow, evaluate it, and promote the packaged model into production."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "#### Tool & View Registration\n",
    "\n",
    "- `CREATE SCHEMA` guarantees the shared `${CATALOG}.ai` workspace exists for agent assets.\n",
    "- `order_delivery_times_per_location_view` summarizes delivery percentiles per brand/location.\n",
    "- `get_order_overview(oid)` returns structured order metadata, items, and customer info.\n",
    "- `get_order_timing(oid)` exposes created/delivered timestamps plus transit duration.\n",
    "- `get_location_timings(loc)` yields P50/P75/P99 delivery benchmarks for benchmarking complaints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS ${CATALOG}.ai;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tdy6yy3gheg",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW ${CATALOG}.ai.order_delivery_times_per_location_view AS\n",
    "WITH order_times AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    MAX(CASE WHEN event_type = 'order_created' THEN try_to_timestamp(ts) END) AS order_created_time,\n",
    "    MAX(CASE WHEN event_type = 'delivered' THEN try_to_timestamp(ts) END) AS delivered_time\n",
    "  FROM\n",
    "    ${CATALOG}.lakeflow.all_events\n",
    "  WHERE\n",
    "    try_to_timestamp(ts) >= CURRENT_TIMESTAMP() - INTERVAL 1 DAY\n",
    "  GROUP BY\n",
    "    order_id,\n",
    "    location\n",
    "),\n",
    "total_order_times AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    (UNIX_TIMESTAMP(delivered_time) - UNIX_TIMESTAMP(order_created_time)) / 60 AS total_order_time_minutes\n",
    "  FROM\n",
    "    order_times\n",
    "  WHERE\n",
    "    order_created_time IS NOT NULL\n",
    "    AND delivered_time IS NOT NULL\n",
    ")\n",
    "SELECT\n",
    "  location,\n",
    "  PERCENTILE(total_order_time_minutes, 0.50) AS P50,\n",
    "  PERCENTILE(total_order_time_minutes, 0.75) AS P75,\n",
    "  PERCENTILE(total_order_time_minutes, 0.99) AS P99\n",
    "FROM\n",
    "  total_order_times\n",
    "GROUP BY\n",
    "  location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_overview(oid STRING COMMENT 'The unique order identifier to retrieve information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  location STRING COMMENT 'Order location',\n",
    "  items_json STRING COMMENT 'JSON array of ordered items with details',\n",
    "  customer_address STRING COMMENT 'Customer delivery address',\n",
    "  brand_id BIGINT COMMENT 'Brand ID for the order',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created'\n",
    ")\n",
    "COMMENT 'Returns basic order information including items, location, and customer details'\n",
    "RETURN\n",
    "  WITH order_created_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      location,\n",
    "      get_json_object(body, '$.items') as items_json,\n",
    "      get_json_object(body, '$.customer_addr') as customer_address,\n",
    "      -- Extract brand_id from first item in the order\n",
    "      CAST(get_json_object(get_json_object(body, '$.items[0]'), '$.brand_id') AS BIGINT) as brand_id,\n",
    "      try_to_timestamp(ts) as order_created_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid AND event_type = 'order_created'\n",
    "    LIMIT 1\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    items_json,\n",
    "    customer_address,\n",
    "    brand_id,\n",
    "    order_created_ts\n",
    "  FROM order_created_events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_timing(oid STRING COMMENT 'The unique order identifier to get timing information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created',\n",
    "  delivered_ts TIMESTAMP COMMENT 'When the order was delivered (NULL if not delivered)',\n",
    "  delivery_duration_minutes FLOAT COMMENT 'Time from order creation to delivery in minutes (NULL if not delivered)',\n",
    "  delivery_status STRING COMMENT 'Current delivery status: delivered, in_progress, or unknown'\n",
    ")\n",
    "COMMENT 'Returns timing information for a specific order'\n",
    "RETURN\n",
    "  WITH order_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      event_type,\n",
    "      try_to_timestamp(ts) as event_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid\n",
    "  ),\n",
    "  timing_summary AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      MIN(CASE WHEN event_type = 'order_created' THEN event_ts END) as order_created_ts,\n",
    "      MAX(CASE WHEN event_type = 'delivered' THEN event_ts END) as delivered_ts\n",
    "    FROM order_events\n",
    "    GROUP BY order_id\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    order_created_ts,\n",
    "    delivered_ts,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL AND order_created_ts IS NOT NULL THEN\n",
    "        CAST((UNIX_TIMESTAMP(delivered_ts) - UNIX_TIMESTAMP(order_created_ts)) / 60 AS FLOAT)\n",
    "      ELSE NULL\n",
    "    END as delivery_duration_minutes,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL THEN 'delivered'\n",
    "      WHEN order_created_ts IS NOT NULL THEN 'in_progress'\n",
    "      ELSE 'unknown'\n",
    "    END as delivery_status\n",
    "  FROM timing_summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_location_timings(loc STRING COMMENT 'Location name as a string')\n",
    "RETURNS TABLE (\n",
    "  location STRING COMMENT 'Location of the order source',\n",
    "  P50 FLOAT COMMENT '50th percentile delivery time in minutes',\n",
    "  P75 FLOAT COMMENT '75th percentile delivery time in minutes',\n",
    "  P99 FLOAT COMMENT '99th percentile delivery time in minutes'\n",
    ")\n",
    "COMMENT 'Returns the 50/75/99th percentile of delivery times for a location to benchmark order timing'\n",
    "RETURN\n",
    "  SELECT location, P50, P75, P99\n",
    "  FROM ${CATALOG}.ai.order_delivery_times_per_location_view AS odlt\n",
    "  WHERE odlt.location = loc;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dq5ml4wp6v",
   "metadata": {},
   "source": "#### Model\n\n- Install DSPy, Databricks agent packages, and restart Python for a clean runtime.\n- Capture widget inputs (`CATALOG`, `LLM_MODEL`) and create an MLflow dev experiment for trace logging.\n- Define a templated `%%writefilev` magic that emits files with notebook variable substitution.\n- Materialize `agent.py` containing the DSPy ReAct complaint workflow wired to UC SQL tools and the chosen LLM endpoint.\n- Pull a delivered `order_id` sample and build the MLflow model signature/resources for logging."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3tu3r2gso",
   "metadata": {},
   "outputs": [],
   "source": "%pip install -U -qqqq typing_extensions dspy-ai mlflow unitycatalog-openai[databricks] openai databricks-sdk databricks-agents pydantic\n%restart_python"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dl04kgwv9ap",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "LLM_MODEL = dbutils.widgets.get(\"LLM_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ktoahik8tl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Create/set dev experiment for development and evaluation traces\n",
    "# Use shared path for job compatibility\n",
    "dev_experiment_name = f\"/Shared/{CATALOG}_complaint_agent_dev\"\n",
    "\n",
    "# set_experiment creates the experiment if it doesn't exist, or activates it if it does\n",
    "dev_experiment = mlflow.set_experiment(dev_experiment_name)\n",
    "dev_experiment_id = dev_experiment.experiment_id\n",
    "print(f\"✅ Using dev experiment: {dev_experiment_name} (ID: {dev_experiment_id})\")\n",
    "\n",
    "# Add experiment to UC state for cleanup\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "experiment_data = {\n",
    "    \"experiment_id\": dev_experiment_id,\n",
    "    \"name\": dev_experiment_name\n",
    "}\n",
    "add(CATALOG, \"experiments\", experiment_data)\n",
    "print(f\"✅ Added dev experiment to UC state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bruu85upqq5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def writefilev(line, cell):\n",
    "    \"\"\"\n",
    "    %%writefilev file.py\n",
    "    Allows {{var}} substitutions while leaving normal {} intact.\n",
    "    \"\"\"\n",
    "    filename = line.strip()\n",
    "\n",
    "    def replacer(match):\n",
    "        expr = match.group(1)\n",
    "        return str(eval(expr, globals(), locals()))\n",
    "\n",
    "    # Replace only double braces {{var}}\n",
    "    content = re.sub(r\"\\{\\{(.*?)\\}\\}\", replacer, cell)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Wrote file with substitutions: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2c70l4ca8",
   "metadata": {},
   "outputs": [],
   "source": "%%writefilev agent.py\nimport warnings\nfrom typing import Optional, Literal\nfrom uuid import uuid4\nfrom pydantic import BaseModel, Field, field_validator, ValidationError\n\nwarnings.filterwarnings(\"ignore\", message=\".*Ignoring the default notebook Spark session.*\")\n\nimport dspy\nimport mlflow\nfrom unitycatalog.ai.core.base import get_uc_function_client\nfrom mlflow.pyfunc import ResponsesAgent\nfrom mlflow.types.responses import (\n    ResponsesAgentRequest,\n    ResponsesAgentResponse,\n)\n\n# Enable DSPy autologging for automatic trace capture\nmlflow.dspy.autolog(log_traces=True)\n\nLLM_MODEL = \"{{LLM_MODEL}}\"\nCATALOG = \"{{CATALOG}}\"\n\n# Configure DSPy with Databricks LM\nlm = dspy.LM(f'databricks/{LLM_MODEL}', max_tokens=2000)\ndspy.configure(lm=lm)\n\n# Initialize UC function client\nuc_client = get_uc_function_client()\n\n\nclass ComplaintResponse(BaseModel):\n    \"\"\"Structured output for complaint triage decisions.\"\"\"\n    order_id: str\n    complaint_category: Literal[\"delivery_delay\", \"missing_items\", \"food_quality\", \"service_issue\", \"billing\", \"other\"] = Field(\n        description=\"Exactly ONE primary complaint category\"\n    )\n    decision: Literal[\"suggest_credit\", \"escalate\"]\n    credit_amount: Optional[float] = None\n    confidence: Optional[Literal[\"high\", \"medium\", \"low\"]] = None\n    priority: Optional[Literal[\"standard\", \"urgent\"]] = None\n    rationale: str\n    \n    @field_validator('complaint_category', mode='before')\n    @classmethod\n    def parse_category(cls, v):\n        \"\"\"Extract first valid category if multiple provided.\"\"\"\n        if not isinstance(v, str):\n            return v\n            \n        valid_categories = [\"delivery_delay\", \"missing_items\", \"food_quality\", \"service_issue\", \"billing\", \"other\"]\n        v_lower = v.lower().strip()\n        \n        # Exact match\n        if v_lower in valid_categories:\n            return v_lower\n        \n        # Find first valid category in string\n        for cat in valid_categories:\n            if cat in v_lower:\n                return cat\n        \n        return \"other\"\n    \n    @field_validator('confidence', mode='before')\n    @classmethod\n    def parse_confidence(cls, v):\n        \"\"\"Ensure valid confidence value.\"\"\"\n        if v is None or (isinstance(v, str) and v.lower() == \"null\"):\n            return None\n        if isinstance(v, str):\n            v_lower = v.lower().strip()\n            if v_lower in [\"high\", \"medium\", \"low\"]:\n                return v_lower\n            return \"medium\"\n        return v\n    \n    @field_validator('priority', mode='before')\n    @classmethod\n    def parse_priority(cls, v):\n        \"\"\"Ensure valid priority value.\"\"\"\n        if v is None or (isinstance(v, str) and v.lower() == \"null\"):\n            return None\n        if isinstance(v, str):\n            v_lower = v.lower().strip()\n            if v_lower in [\"standard\", \"urgent\"]:\n                return v_lower\n            return \"standard\"\n        return v\n\n\nclass ComplaintTriage(dspy.Signature):\n    \"\"\"Analyze customer complaints for Casper's Kitchens and recommend triage actions.\n    \n    Process:\n    1. Extract order_id from complaint\n    2. Use get_order_overview(order_id) for order details and items\n    3. Use get_order_timing(order_id) for delivery timing\n    4. For delays, use get_location_timings(location) for percentile benchmarks\n    5. Make data-backed decision\n    \n    Decision Framework:\n    \n    SUGGEST_CREDIT (with credit_amount and confidence):\n    - Delivery delays: Compare actual delivery time to location percentiles\n      * <P75: Suggest $0 credit (low confidence - on-time or minimal delay)\n      * P75-P99: Suggest 15% of order total (medium to high confidence)\n      * >P99: Suggest 25% of order total (high confidence)\n    - Missing items: Use actual item prices from order data when available\n      * Verify claimed item exists in order (affects confidence)\n      * Use real costs from order data, or estimate $8-12 per item if unavailable\n    - Food quality: 20-40% of order total based on severity\n      * Minor issues (slightly cold, minor preparation issue): 20% (medium confidence)\n      * Major issues (completely inedible, wrong preparation, health concern): 40% (high confidence)\n      * Vague complaints (\"bad\", \"gross\"): escalate instead\n    \n    ESCALATE (with priority):\n    - priority=\"standard\": Vague complaints, missing data, billing issues, service complaints\n    - priority=\"urgent\": Legal threats, health/safety concerns, suspected fraud, abusive language\n    \n    Output Requirements:\n    - For suggest_credit: credit_amount is REQUIRED and must be a number (can be 0.0 if no credit warranted), confidence is REQUIRED, priority must be null\n    - For escalate: priority is REQUIRED, credit_amount and confidence must be null\n    - complaint_category: Choose EXACTLY ONE category (the primary one)\n    - Rationale must cite specific evidence (delivery times, percentiles, item verification, order total)\n    - Rationale should be detailed but under 150 words\n    - Round credit amounts to nearest $0.50\n    - Confidence: high (strong data), medium (reasonable inference), low (weak/contradictory)\n    \"\"\"\n    \n    complaint: str = dspy.InputField(desc=\"Customer complaint text\")\n    order_id: str = dspy.OutputField(desc=\"Extracted order ID\")\n    complaint_category: str = dspy.OutputField(desc=\"EXACTLY ONE category: delivery_delay, missing_items, food_quality, service_issue, billing, or other\")\n    decision: str = dspy.OutputField(desc=\"EXACTLY ONE: suggest_credit or escalate\")\n    credit_amount: str = dspy.OutputField(desc=\"If suggest_credit: MUST be a number (e.g., 0.0, 10.5). If escalate: null\")\n    confidence: str = dspy.OutputField(desc=\"If suggest_credit: EXACTLY ONE of high, medium, low. If escalate: null\")\n    priority: str = dspy.OutputField(desc=\"If escalate: EXACTLY ONE of standard or urgent. If suggest_credit: null\")\n    rationale: str = dspy.OutputField(desc=\"Data-focused justification citing specific evidence\")\n\n\n# Unity Catalog tool wrappers\ndef get_order_overview(order_id: str) -> str:\n    \"\"\"Get order details including items, location, and customer info.\"\"\"\n    result = uc_client.execute_function(\n        f\"{CATALOG}.ai.get_order_overview\",\n        {\"oid\": order_id}\n    )\n    return str(result.value)\n\n\ndef get_order_timing(order_id: str) -> str:\n    \"\"\"Get timing information for a specific order.\"\"\"\n    result = uc_client.execute_function(\n        f\"{CATALOG}.ai.get_order_timing\",\n        {\"oid\": order_id}\n    )\n    return str(result.value)\n\n\ndef get_location_timings(location: str) -> str:\n    \"\"\"Get delivery time percentiles for a specific location.\"\"\"\n    result = uc_client.execute_function(\n        f\"{CATALOG}.ai.get_location_timings\",\n        {\"loc\": location}\n    )\n    return str(result.value)\n\n\nclass ComplaintTriageModule(dspy.Module):\n    \"\"\"DSPy module for complaint triage with tool calling.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.react = dspy.ReAct(\n            signature=ComplaintTriage,\n            tools=[get_order_overview, get_order_timing, get_location_timings],\n            max_iters=10\n        )\n    \n    def forward(self, complaint: str, max_retries: int = 2) -> ComplaintResponse:\n        \"\"\"Process complaint and return structured triage decision with retry on validation failure.\"\"\"\n        \n        for attempt in range(max_retries + 1):\n            try:\n                result = self.react(complaint=complaint)\n                \n                # Parse credit_amount\n                credit_amount = None\n                if result.credit_amount and result.credit_amount.lower() != \"null\":\n                    try:\n                        credit_amount = float(result.credit_amount)\n                    except (ValueError, TypeError):\n                        if result.decision == \"suggest_credit\":\n                            raise ValidationError(\"suggest_credit requires valid numeric credit_amount\")\n                \n                # Validate business rules before Pydantic construction\n                if result.decision == \"suggest_credit\" and credit_amount is None:\n                    raise ValidationError(\"suggest_credit requires credit_amount to be a number (can be 0.0)\")\n                \n                # Construct Pydantic model - field validators run here\n                return ComplaintResponse(\n                    order_id=result.order_id,\n                    complaint_category=result.complaint_category,\n                    decision=result.decision,\n                    credit_amount=credit_amount,\n                    confidence=result.confidence,\n                    priority=result.priority,\n                    rationale=result.rationale\n                )\n                \n            except (ValidationError, ValueError) as e:\n                if attempt < max_retries:\n                    # Retry - DSPy will regenerate with potentially different output\n                    continue\n                else:\n                    # Final attempt failed - re-raise\n                    raise\n\n\nclass DSPyComplaintAgent(ResponsesAgent):\n    \"\"\"ResponsesAgent wrapper for DSPy complaint triage module.\"\"\"\n    \n    def __init__(self):\n        self.module = ComplaintTriageModule()\n    \n    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n        \"\"\"Process complaint request and return structured response.\"\"\"\n        complaint = None\n        for msg in request.input:\n            msg_dict = msg.model_dump() if hasattr(msg, \"model_dump\") else msg\n            if msg_dict.get(\"role\") == \"user\":\n                complaint = msg_dict.get(\"content\", \"\")\n                break\n        \n        if not complaint:\n            raise ValueError(\"No user message found in request\")\n        \n        result = self.module(complaint=complaint)\n        \n        return ResponsesAgentResponse(\n            output=[\n                self.create_text_output_item(\n                    text=result.model_dump_json(),\n                    id=str(uuid4())\n                )\n            ],\n            custom_outputs=request.custom_inputs\n        )\n\n\n# Initialize agent\nAGENT = DSPyComplaintAgent()\nmlflow.models.set_model(AGENT)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6gjrp4mx6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an actual order_id for input example\n",
    "sample_order_id = spark.sql(f\"\"\"\n",
    "    SELECT order_id \n",
    "    FROM {CATALOG}.lakeflow.all_events \n",
    "    WHERE event_type='delivered'\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0]['order_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ih3tt5qeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sample_order_id is not None\n",
    "print(sample_order_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23o4h7j6amzh",
   "metadata": {},
   "outputs": [],
   "source": "import mlflow\nfrom agent import LLM_MODEL\nfrom mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\nfrom pkg_resources import get_distribution\n\nresources = [DatabricksServingEndpoint(endpoint_name=LLM_MODEL)]\n# Add UC function resources\nuc_tool_names = [\n    f\"{CATALOG}.ai.get_order_overview\",\n    f\"{CATALOG}.ai.get_order_timing\",\n    f\"{CATALOG}.ai.get_location_timings\",\n]\nfor func_name in uc_tool_names:\n    resources.append(DatabricksFunction(function_name=func_name))\n\ninput_example = {\n    \"input\": [\n        {\n            \"role\": \"user\",\n            \"content\": f\"My order was really late! Order ID: {sample_order_id}\"\n        }\n    ]\n}\n\n# Create custom conda environment with mlflow explicitly specified\nconda_env = {\n    \"channels\": [\"conda-forge\"],\n    \"dependencies\": [\n        \"python=3.11\",\n        \"pip\",\n        {\n            \"pip\": [\n                \"mlflow==3.6\",\n                f\"typing_extensions=={get_distribution('typing_extensions').version}\",\n                f\"dspy-ai=={get_distribution('dspy-ai').version}\",\n                f\"unitycatalog-openai[databricks]=={get_distribution('unitycatalog-openai').version}\",\n                f\"pydantic=={get_distribution('pydantic').version}\",\n            ]\n        }\n    ],\n    \"name\": \"mlflow-env\"\n}\n\nwith mlflow.start_run():\n    logged_agent_info = mlflow.pyfunc.log_model(\n        name=\"complaint_agent\",\n        python_model=\"agent.py\",\n        input_example=input_example,\n        resources=resources,\n        conda_env=conda_env,\n    )\n\nmlflow.set_active_model(model_id = logged_agent_info.model_id)"
  },
  {
   "cell_type": "markdown",
   "id": "gfympowd7q",
   "metadata": {},
   "source": "#### Evaluate the Agent\n\n- Synthesize diverse complaint scenarios from recent orders covering delivery delays, food quality issues, missing items, service complaints, and edge cases.\n- Configure MLflow `Guidelines` scorers to evaluate evidence-based reasoning, credit amount reasonableness, and decision metadata consistency.\n- Run batch evaluations with rate limiting (2s delay per request) to quantify decision quality before promotion."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2loovjto96g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive complaint scenarios for evaluation\n",
    "import random\n",
    "\n",
    "# Get sample order IDs for different scenarios\n",
    "all_order_ids = [\n",
    "    row['order_id'] for row in spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT order_id \n",
    "        FROM {CATALOG}.lakeflow.all_events \n",
    "        WHERE event_type='delivered'\n",
    "        LIMIT 50\n",
    "    \"\"\").collect()\n",
    "]\n",
    "\n",
    "# Create diverse, realistic complaint scenarios\n",
    "complaint_scenarios = []\n",
    "\n",
    "# 1. Delivery delay complaints - mix of legitimate and questionable\n",
    "for oid in all_order_ids[:8]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My order took forever to arrive! Order ID: {oid}\",\n",
    "        f\"Been waiting 2 hours, this is ridiculous. Order {oid}\",\n",
    "        f\"Order {oid} arrived late and cold\",\n",
    "        f\"Delivery was slower than usual for order {oid}\",\n",
    "    ])\n",
    "\n",
    "# 2. Food quality issues - range from specific to vague\n",
    "for oid in all_order_ids[8:12]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My falafel was completely soggy and inedible. Order: {oid}\",\n",
    "        f\"The food was cold when it arrived, very disappointing. Order: {oid}\",\n",
    "        f\"Everything tasted bad. Order {oid}\",\n",
    "        f\"Not happy with the quality. {oid}\",\n",
    "        f\"The gyro meat was overcooked and dry, very disappointing. Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# 3. Missing items - some verifiable, some suspicious\n",
    "for oid in all_order_ids[12:16]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My entire falafel bowl was missing from the order! Order: {oid}\",\n",
    "        f\"No drinks or sides in my order {oid}\",\n",
    "        f\"Missing my gyro from order {oid}\",\n",
    "        f\"You forgot half my items. {oid}\",\n",
    "    ])\n",
    "\n",
    "# 4. Items claimed that might not match the order\n",
    "for oid in all_order_ids[16:18]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Where are my chicken wings?! Order {oid}\",\n",
    "        f\"Missing my pizza from order {oid}\",\n",
    "    ])\n",
    "\n",
    "# 5. Service issues - should escalate\n",
    "for oid in all_order_ids[18:20]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Your driver was extremely rude to me. Order: {oid}\",\n",
    "        f\"Driver left my food at wrong address. Order: {oid}\",\n",
    "        f\"The delivery person refused to come to my door. {oid}\",\n",
    "    ])\n",
    "\n",
    "# 6. Multiple issues in one complaint\n",
    "for oid in all_order_ids[20:22]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Order {oid} was late AND missing items AND cold!\",\n",
    "        f\"Late delivery, rude driver, and food quality was poor. Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# 7. Escalation triggers - legal threats, health concerns\n",
    "for oid in all_order_ids[22:24]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"I'm calling my lawyer about this terrible service! Order: {oid}\",\n",
    "        f\"This food made me sick, possible food poisoning. Order: {oid}\",\n",
    "        f\"Found a piece of plastic in my food! Order {oid} - this is dangerous!\",\n",
    "    ])\n",
    "\n",
    "# 8. Vague complaints without specifics\n",
    "for oid in all_order_ids[24:26]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Not satisfied with order {oid}\",\n",
    "        f\"Bad experience. {oid}\",\n",
    "        f\"Order {oid} was wrong\",\n",
    "    ])\n",
    "\n",
    "# 9. Billing/promo issues\n",
    "for oid in all_order_ids[26:28]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My promo code didn't work on order {oid}\",\n",
    "        f\"I was charged twice for order {oid}!\",\n",
    "    ])\n",
    "\n",
    "# 10. Edge cases - no order ID or invalid format\n",
    "complaint_scenarios.extend([\n",
    "    \"My order was really late and the food was cold!\",  # Missing order ID\n",
    "    \"terrible service, do better\",  # No order ID, vague\n",
    "    \"Order ABC123 never arrived\",  # Invalid order ID format\n",
    "])\n",
    "\n",
    "# 11. Non-complaints / comments\n",
    "for oid in all_order_ids[28:30]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Great food, loved it! Order {oid}\",  # Positive comment\n",
    "        f\"Just wanted to say the driver was very polite today. {oid}\",  # Positive feedback\n",
    "        f\"How do I reorder my previous order {oid}?\",  # Question, not complaint\n",
    "    ])\n",
    "\n",
    "# 12. Unfounded complaints (on-time delivery but claiming delay)\n",
    "for oid in all_order_ids[30:32]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"This took way too long! Order {oid}\",\n",
    "    ])\n",
    "\n",
    "# Sample for reasonable eval size (aim for ~25-30 scenarios)\n",
    "complaint_scenarios = random.sample(complaint_scenarios, min(30, len(complaint_scenarios)))\n",
    "\n",
    "# Wrap in correct input schema for ResponsesAgent\n",
    "data = []\n",
    "for complaint in complaint_scenarios:\n",
    "    data.append({\n",
    "        \"inputs\": {\n",
    "            \"input\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": complaint\n",
    "            }]\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(f\"Created {len(data)} diverse evaluation scenarios including:\")\n",
    "print(\"  - Delivery delays (legitimate & questionable)\")\n",
    "print(\"  - Food quality issues (specific & vague)\")\n",
    "print(\"  - Missing items (verifiable & suspicious)\")\n",
    "print(\"  - Service complaints\")\n",
    "print(\"  - Escalation triggers (health/legal)\")\n",
    "print(\"  - Edge cases (no order ID, invalid ID)\")\n",
    "print(\"  - Non-complaints (positive feedback, questions)\")\n",
    "print(\"  - Multiple issues in one complaint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9l6zrp5n03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three focused scorers and run evaluation\n",
    "\n",
    "from mlflow.genai.scorers import Guidelines\n",
    "import mlflow\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "project_directory = os.path.dirname(notebook_path)\n",
    "sys.path.append(project_directory)\n",
    "\n",
    "from agent import AGENT\n",
    "\n",
    "# Scorer 1: Evidence-Based Reasoning - Does the rationale support the decision?\n",
    "evidence_reasoning = Guidelines(\n",
    "    name=\"evidence_reasoning\",\n",
    "    guidelines=[\n",
    "        \"The rationale should provide specific evidence or reasoning for the decision made\",\n",
    "        \"For timing complaints, rationale should mention delivery times or timing data if available\",\n",
    "        \"For missing item complaints, rationale should reference the items in question\",\n",
    "        \"For escalations, rationale should explain why human judgment is needed\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scorer 2: Credit Amount Reasonableness - Is the refund amount appropriate?\n",
    "credit_reasonableness = Guidelines(\n",
    "    name=\"credit_reasonableness\",\n",
    "    guidelines=[\n",
    "        \"If decision='escalate', automatically pass this check\",\n",
    "        \"If decision='suggest_credit' with credit_amount > $0, amount should be reasonable ($5-$50)\",\n",
    "        \"A credit_amount of $0 is valid when rationale indicates no issue\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scorer 3: Decision Metadata Consistency - Are confidence and priority used correctly?\n",
    "decision_metadata = Guidelines(\n",
    "    name=\"decision_metadata\",\n",
    "    guidelines=[\n",
    "        \"If decision='suggest_credit': confidence should be present and priority should be null\",\n",
    "        \"If decision='escalate': priority should be present and confidence should be null\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ResponsesAgent predict wrapper with rate limiting\n",
    "def predict_fn(input):\n",
    "    from mlflow.types.responses import ResponsesAgentRequest\n",
    "    time.sleep(2)  # Rate limiting to avoid hitting API limits\n",
    "    request = ResponsesAgentRequest(input=input)\n",
    "    response = AGENT.predict(request)\n",
    "    output_item = response.output[-1]\n",
    "    if hasattr(output_item, 'content') and output_item.content:\n",
    "        return output_item.content[0][\"text\"]\n",
    "    return str(output_item)\n",
    "\n",
    "# Run evaluation\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=data,\n",
    "    scorers=[evidence_reasoning, credit_reasonableness, decision_metadata],\n",
    "    predict_fn=predict_fn\n",
    ")\n",
    "\n",
    "print(f\"✅ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ttse3kj3pcj",
   "metadata": {},
   "source": "#### Log the Agent to Unity Catalog\n\n- Point MLflow at the Unity Catalog registry and name the artifact `${CATALOG}.ai.complaint_agent`.\n- Register the run-produced model so versioned deployments can be promoted through UC stages."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y8lfco9zzn",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "UC_MODEL_NAME = f\"{CATALOG}.ai.complaint_agent\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "me3m6ovfqkd",
   "metadata": {},
   "source": "#### Deploy the Agent to Model Serving\n\n- Create a production MLflow experiment for live trace capture.\n- Use `agents.deploy` to create/update the Databricks Model Serving endpoint backed by the UC model version.\n- Wait until the serving endpoint reports READY before continuing to downstream steps.\n- Pass the prod experiment ID via environment variables so inference traces are logged automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qjnjjztbbna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Create prod experiment for production inference traces\n",
    "# Use shared path for job compatibility and visibility\n",
    "prod_experiment_name = f\"/Shared/{CATALOG}_complaint_agent_prod\"\n",
    "\n",
    "# set_experiment creates the experiment if it doesn't exist, or activates it if it does\n",
    "prod_experiment = mlflow.set_experiment(prod_experiment_name)\n",
    "prod_experiment_id = prod_experiment.experiment_id\n",
    "print(f\"✅ Using prod experiment: {prod_experiment_name} (ID: {prod_experiment_id})\")\n",
    "\n",
    "# Add experiment to UC state for cleanup\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "experiment_data = {\n",
    "    \"experiment_id\": prod_experiment_id,\n",
    "    \"name\": prod_experiment_name\n",
    "}\n",
    "add(CATALOG, \"experiments\", experiment_data)\n",
    "print(f\"✅ Added prod experiment to UC state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exckljo2zx4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from databricks import agents\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointStateReady\n",
    "\n",
    "endpoint_name = dbutils.widgets.get(\"COMPLAINT_AGENT_ENDPOINT_NAME\")\n",
    "deployment_info = agents.deploy(\n",
    "    model_name=UC_MODEL_NAME,\n",
    "    model_version=uc_registered_model_info.version,\n",
    "    scale_to_zero=False,\n",
    "    endpoint_name=endpoint_name,\n",
    "    environment_vars={\"MLFLOW_EXPERIMENT_ID\": str(prod_experiment_id)},\n",
    ")\n",
    "\n",
    "workspace = WorkspaceClient()\n",
    "ready_endpoint = workspace.serving_endpoints.wait_get_serving_endpoint_not_updating(\n",
    "    name=endpoint_name,\n",
    "    timeout=timedelta(minutes=30),\n",
    ")\n",
    "\n",
    "if ready_endpoint.state.ready != EndpointStateReady.READY:\n",
    "    raise RuntimeError(\n",
    "        f\"Endpoint {endpoint_name} is {ready_endpoint.state.ready} after deployment; retry or investigate.\"\n",
    "    )\n",
    "\n",
    "print(f\"✅ Endpoint {endpoint_name} is READY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i1syfkwcivl",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deployment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4i9yj8vjs2",
   "metadata": {},
   "source": "#### Record Model in State\n\n- Store the deployment metadata with `uc_state.add` to facilitate cleanup in the future."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ihnhnv5plw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also add to UC-state\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "add(dbutils.widgets.get(\"CATALOG\"), \"endpoints\", deployment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bhjj7zuan9g",
   "metadata": {},
   "source": "#### Production Monitoring\n\n- Register MLflow guideline scorers to monitor decision quality and refund reasoning on production traffic.\n- Enable 10% sampling to flag decision drift or policy regressions without impacting performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v1j0a0y21ct",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Guidelines, ScorerSamplingConfig\n",
    "\n",
    "# Register scorers for production monitoring (10% sampling)\n",
    "decision_quality_monitor = Guidelines(\n",
    "    name=\"decision_quality_prod\",\n",
    "    guidelines=[\n",
    "        \"Food quality complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Missing item complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Legal threats or serious health concerns should be classified as 'escalate'\"\n",
    "    ]\n",
    ").register(name=f\"{UC_MODEL_NAME}_decision_quality\")\n",
    "\n",
    "refund_reason_monitor = Guidelines(\n",
    "    name=\"refund_reason_prod\",\n",
    "    guidelines=[\n",
    "        \"If a refund is offered, it must clearly relate to the complaint made by the user\"\n",
    "    ]\n",
    ").register(name=f\"{UC_MODEL_NAME}_refund_reason\")\n",
    "\n",
    "# Start monitoring with 10% sampling of production traffic\n",
    "decision_quality_monitor = decision_quality_monitor.start(\n",
    "    sampling_config=ScorerSamplingConfig(sample_rate=0.1)\n",
    ")\n",
    "\n",
    "refund_reason_monitor = refund_reason_monitor.start(\n",
    "    sampling_config=ScorerSamplingConfig(sample_rate=0.1)\n",
    ")\n",
    "\n",
    "print(\"✅ Production monitoring enabled with 10% sampling\")\n",
    "print(f\"   - decision_quality scorer monitoring: {decision_quality_monitor}\")\n",
    "print(f\"   - refund_reason scorer monitoring: {refund_reason_monitor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}