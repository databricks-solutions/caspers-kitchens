{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14a10bae-42e6-4e06-81fa-c6d2336d157a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Raw Data\n",
    "\n",
    "This notebook assumes raw_data has already run and creates a medallion architecture declarative pipeline to normalize the event stream and create summary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2953476c-3a7a-406c-b3f7-c1b45f1f7ca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade databricks-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8704c18-8794-4ac2-8d7e-1c69bb3b4371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "CATALOG = dbutils.widgets.get(\"CATALOG\")\nEVENTS_VOLUME = dbutils.widgets.get(\"EVENTS_VOLUME\")\nSIMULATOR_SCHEMA = dbutils.widgets.get(\"SIMULATOR_SCHEMA\")\nPIPELINE_SCHEDULE_MINUTES = int(dbutils.widgets.get(\"PIPELINE_SCHEDULE_MINUTES\"))\n\n# 0 = continuous mode, N > 0 = triggered mode with schedule every N minutes\ncontinuous_mode = (PIPELINE_SCHEDULE_MINUTES == 0)\n\nprint(f\"Pipeline mode: {'Continuous' if continuous_mode else f'Triggered (every {PIPELINE_SCHEDULE_MINUTES} minutes)'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69dfc806-58b2-43d5-b13e-29d2661ca099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "import os\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import pipelines\n\nw = WorkspaceClient()\n\nroot_abs_path = os.path.abspath(\"../pipelines/order_items\")\nroot_dbx_path = root_abs_path.replace(\n    os.environ.get(\"DATABRICKS_WORKSPACE_ROOT\", \"/Workspace\"),\n    \"/Workspace\"\n)\n\ncreated = w.pipelines.create(\n    catalog=CATALOG,\n    schema='lakeflow',\n    continuous=continuous_mode,\n    name=f\"Order Items Medallion Declarative Pipeline\",\n    serverless=True,\n    configuration={\n        \"RAW_DATA_CATALOG\":CATALOG,\n        \"RAW_DATA_SCHEMA\":SIMULATOR_SCHEMA,\n        \"RAW_DATA_VOLUME\":EVENTS_VOLUME\n    },\n    root_path=root_dbx_path,\n    libraries=[pipelines.PipelineLibrary(glob=pipelines.PathPattern(include=f\"{root_dbx_path}/**\"))],\n    allow_duplicate_names=True\n)\n\nprint(f\"Created pipeline_id={created.pipeline_id} (continuous={continuous_mode})\")\n\n# If triggered mode, create a scheduled job to run pipeline updates\nif not continuous_mode:\n    import databricks.sdk.service.jobs as j\n    \n    cron_expression = f\"0 0/{PIPELINE_SCHEDULE_MINUTES} * * * ?\"\n    \n    pipeline_job = w.jobs.create(\n        name=f\"Pipeline Update Scheduler (every {PIPELINE_SCHEDULE_MINUTES} min)\",\n        tasks=[\n            j.Task(\n                task_key=\"update_pipeline\",\n                pipeline_task=j.PipelineTask(\n                    pipeline_id=created.pipeline_id\n                )\n            )\n        ],\n        schedule=j.CronSchedule(\n            quartz_cron_expression=cron_expression,\n            timezone_id=\"UTC\",\n            pause_status=j.PauseStatus.UNPAUSED\n        )\n    )\n    \n    print(f\"Created scheduled job_id={pipeline_job.job_id} to run pipeline every {PIPELINE_SCHEDULE_MINUTES} minutes\")\n    \n    # Register the job with uc_state\n    import sys\n    sys.path.append('../utils')\n    from uc_state import add\n    add(CATALOG, \"jobs\", pipeline_job)\n    \n    # Run immediately once\n    w.jobs.run_now(job_id=pipeline_job.job_id)\n    print(f\"Started initial pipeline run\")"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78fe42c2-32da-4526-ac02-99094ac85d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# wait for the tables to be created\n",
    "# future stages may require their existence before being able to be run\n",
    "\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        if spark.catalog.tableExists(f\"{CATALOG}.lakeflow.all_events\"):\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24a507e5-644b-42de-91d4-3214146ae409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Also add to UC-state\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "add(CATALOG, \"pipelines\", created)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lakeflow",
   "widgets": {
    "CATALOG": {
     "currentValue": "",
     "nuid": "1b879c13-e967-45d3-9848-122420dc1369",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "CATALOG",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "CATALOG",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "EVENTS_VOLUME": {
     "currentValue": "",
     "nuid": "267538f4-e4bf-4e45-a232-83760867245e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "EVENTS_VOLUME",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "EVENTS_VOLUME",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "SIMULATOR_SCHEMA": {
     "currentValue": "",
     "nuid": "1eebe964-7af5-44a0-9d82-804fa88755f7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "SIMULATOR_SCHEMA",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "SIMULATOR_SCHEMA",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}