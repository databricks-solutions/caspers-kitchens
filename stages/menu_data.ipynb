{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Menu Data\n",
        "\n",
        "This stage uploads pre-generated restaurant menu PDFs to a Unity Catalog volume\n",
        "and loads the structured metadata as a dimension table. These PDFs contain\n",
        "nutritional information and allergen data for each brand's menu items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install --upgrade databricks-sdk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dbutils.library.restartPython()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "CATALOG = dbutils.widgets.get(\"CATALOG\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Create catalog, schema, and volume for menu documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.menu_documents\")\n",
        "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.menu_documents.menus\")\n",
        "print(f\"\\u2705 Created schema {CATALOG}.menu_documents and volume menus\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Copy PDF files from the repo into the Unity Catalog volume"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "pdf_source_dir = os.path.abspath(\"../data/menus/pdfs\")\n",
        "volume_path = f\"/Volumes/{CATALOG}/menu_documents/menus\"\n",
        "\n",
        "pdf_files = glob.glob(os.path.join(pdf_source_dir, \"*.pdf\"))\n",
        "print(f\"Found {len(pdf_files)} PDF files to upload\")\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    filename = os.path.basename(pdf_file)\n",
        "    with open(pdf_file, \"rb\") as src:\n",
        "        with open(f\"{volume_path}/{filename}\", \"wb\") as dst:\n",
        "            dst.write(src.read())\n",
        "    print(f\"  Uploaded: {filename}\")\n",
        "\n",
        "print(f\"\\u2705 Uploaded {len(pdf_files)} PDFs to {volume_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Load structured metadata as dimension tables\n",
        "\n",
        "The menu_metadata.json contains the source data used to generate the PDFs,\n",
        "including items, nutritional info, and allergens per brand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "\n",
        "metadata_path = os.path.abspath(\"../data/menus/menu_metadata.json\")\n",
        "with open(metadata_path) as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "print(f\"Loaded metadata for {len(metadata['brands'])} brands\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
        ")\n",
        "\n",
        "# Flatten items with their brand context\n",
        "rows = []\n",
        "for brand in metadata[\"brands\"]:\n",
        "    for item in brand[\"items\"]:\n",
        "        rows.append({\n",
        "            \"brand_name\": brand[\"brand_name\"],\n",
        "            \"cuisine\": brand[\"cuisine\"],\n",
        "            \"pdf_filename\": brand[\"pdf_filename\"],\n",
        "            \"item_name\": item[\"name\"],\n",
        "            \"description\": item[\"description\"],\n",
        "            \"category\": item[\"category\"],\n",
        "            \"price\": float(item[\"price\"]),\n",
        "            \"calories\": int(item[\"calories\"]),\n",
        "            \"protein_g\": int(item[\"protein_g\"]),\n",
        "            \"fat_g\": int(item[\"fat_g\"]),\n",
        "            \"carbs_g\": int(item[\"carbs_g\"]),\n",
        "            \"allergens\": item[\"allergens\"],\n",
        "        })\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"brand_name\", StringType()),\n",
        "    StructField(\"cuisine\", StringType()),\n",
        "    StructField(\"pdf_filename\", StringType()),\n",
        "    StructField(\"item_name\", StringType()),\n",
        "    StructField(\"description\", StringType()),\n",
        "    StructField(\"category\", StringType()),\n",
        "    StructField(\"price\", DoubleType()),\n",
        "    StructField(\"calories\", IntegerType()),\n",
        "    StructField(\"protein_g\", IntegerType()),\n",
        "    StructField(\"fat_g\", IntegerType()),\n",
        "    StructField(\"carbs_g\", IntegerType()),\n",
        "    StructField(\"allergens\", ArrayType(StringType())),\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(rows, schema=schema)\n",
        "df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.menu_documents.brands_metadata\")\n",
        "print(f\"\\u2705 Created brands_metadata table with {df.count()} items\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Register resources with uc_state for cleanup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "sys.path.append('../utils')\n",
        "from uc_state import add\n",
        "\n",
        "# The schema and volume will be cleaned up when the catalog is dropped,\n",
        "# but we register the catalog itself if it was newly created.\n",
        "# Note: other stages may have already registered the catalog.\n",
        "print(\"\\u2705 Menu data stage complete\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}