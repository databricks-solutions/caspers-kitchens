{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casper's Kitchens - Canonical Data Replay\n",
    "\n",
    "This notebook replays pre-generated ghost kitchen events from the canonical dataset using a custom PySpark streaming data source.\n",
    "\n",
    "**Features:**\n",
    "- Reads from single self-contained events.parquet file (34 MB)\n",
    "- Uses checkpoint to track progress\n",
    "- Supports variable speed multiplier (1x, 60x, 3600x, etc.)\n",
    "- Works with `availableNow` trigger for scheduled runs\n",
    "\n",
    "**Dataset:**\n",
    "- 75,780 orders across 4 cities (SF, SV, Bellevue, Chicago)\n",
    "- 1,014,290 events (order lifecycle + driver tracking)\n",
    "- Real OpenStreetMap routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get parameters from widgets\n",
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "SCHEMA = dbutils.widgets.get(\"SCHEMA\")\n",
    "VOLUME = dbutils.widgets.get(\"VOLUME\")\n",
    "START_DAY = int(dbutils.widgets.get(\"START_DAY\"))\n",
    "SPEED_MULTIPLIER = float(dbutils.widgets.get(\"SPEED_MULTIPLIER\"))\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = os.path.abspath(\"../data/canonical/canonical_dataset\")  # Relative path to dataset in workspace\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "CHECKPOINT_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/misc/checkpoints\"\n",
    "\n",
    "print(f\"ðŸ“‹ Configuration:\")\n",
    "print(f\"   Catalog: {CATALOG}\")\n",
    "print(f\"   Schema: {SCHEMA}\")\n",
    "print(f\"   Volume: {VOLUME}\")\n",
    "print(f\"   Start Day: {START_DAY}\")\n",
    "print(f\"   Speed Multiplier: {SPEED_MULTIPLIER}x\")\n",
    "print(f\"   Dataset Path: {DATASET_PATH}\")\n",
    "print(f\"   Output Path: {VOLUME_PATH}\")\n",
    "print(f\"   Checkpoint Path: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.datasource import DataSource, DataSourceStreamReader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Source Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaspersDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    Custom streaming data source that replays pre-generated Casper's Kitchens events.\n",
    "\n",
    "    Options:\n",
    "    - datasetPath: Path to canonical_dataset directory\n",
    "    - simulationStartDay: Which day to start simulation (0-89)\n",
    "    - speedMultiplier: Speed of replay (1.0=realtime, 60.0=60x speed, 3600.0=1hr per second)\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        return \"caspers\"\n",
    "\n",
    "    def schema(self):\n",
    "        \"\"\"Define the output schema for events.\"\"\"\n",
    "        return StructType([\n",
    "            StructField(\"event_id\", StringType(), False),\n",
    "            StructField(\"event_type\", StringType(), False),\n",
    "            StructField(\"ts\", StringType(), False),\n",
    "            StructField(\"location_id\", IntegerType(), False),\n",
    "            StructField(\"order_id\", StringType(), False),\n",
    "            StructField(\"sequence\", IntegerType(), False),\n",
    "            StructField(\"body\", StringType(), False),\n",
    "        ])\n",
    "\n",
    "    def simpleStreamReader(self, schema: StructType):\n",
    "        \"\"\"Return a simple stream reader instance (no partitioning needed).\"\"\"\n",
    "        return CaspersStreamReader(schema, self.options)\n",
    "\n",
    "\n",
    "class CaspersStreamReader(DataSourceStreamReader):\n",
    "    \"\"\"\n",
    "    Stream reader that tracks simulation time as offset.\n",
    "\n",
    "    Offset = {simulation_seconds: Unix timestamp, offset_timestamp: ISO string}\n",
    "\n",
    "    Each run processes: (current_real_time - checkpoint_time) Ã— speed_multiplier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, schema, options):\n",
    "        self.schema = schema\n",
    "        self.options = options\n",
    "\n",
    "        # Parse options\n",
    "        self.dataset_path = options.get(\"datasetPath\", \"./canonical_dataset\")\n",
    "        self.sim_start_day = int(options.get(\"simulationStartDay\", \"20\"))\n",
    "        self.speed_multiplier = float(options.get(\"speedMultiplier\", \"1.0\"))\n",
    "\n",
    "        # Load canonical dataset using pandas (NOT spark.read - avoids circular dependency!)\n",
    "        print(f\"ðŸ“¦ Loading dataset from {self.dataset_path}...\")\n",
    "        self.events_df = pd.read_parquet(f\"{self.dataset_path}/events.parquet\")\n",
    "\n",
    "        unique_orders = self.events_df['order_id'].nunique()\n",
    "        print(f\"âœ… Caspers DataSource initialized\")\n",
    "        print(f\"   Orders: {unique_orders:,}\")\n",
    "        print(f\"   Events: {len(self.events_df):,}\")\n",
    "        print(f\"   Start Day: {self.sim_start_day}\")\n",
    "        print(f\"   Speed: {self.speed_multiplier}x\")\n",
    "\n",
    "    def initialOffset(self):\n",
    "        \"\"\"\n",
    "        Return the starting offset for the stream.\n",
    "        First run outputs all historical data from day 0 to START_DAY + current time.\n",
    "        Speed multiplier not used for first run - just historical catchup.\n",
    "        \"\"\"\n",
    "        now = datetime.utcnow()\n",
    "        # Dataset starts at 2024-01-01 00:00:00\n",
    "        dataset_epoch = datetime(2024, 1, 1).timestamp()\n",
    "\n",
    "        # Start from beginning of dataset (day 0)\n",
    "        initial_unix_ts = int(dataset_epoch)\n",
    "\n",
    "        initial = {\n",
    "            \"simulation_seconds\": initial_unix_ts,\n",
    "            \"offset_timestamp\": now.isoformat(),\n",
    "            \"is_initial\": True  # Flag to indicate this is the first run\n",
    "        }\n",
    "\n",
    "        print(f\"ðŸŽ¬ Initial offset: day 0 (dataset start)\")\n",
    "        print(f\"   First run will output all data from day 0 â†’ day {self.sim_start_day} @ {now.strftime('%H:%M:%S')}\")\n",
    "        print(f\"   Speed multiplier ({self.speed_multiplier}x) will be used for subsequent runs\")\n",
    "        return json.dumps(initial)\n",
    "\n",
    "    def latestOffset(self):\n",
    "        \"\"\"\n",
    "        Return the current offset based on time elapsed since last checkpoint.\n",
    "        Each run processes: (current_time - last_offset_time) * speed_multiplier\n",
    "        \"\"\"\n",
    "        now = datetime.utcnow()\n",
    "        # End of dataset: 2024-01-01 + 90 days\n",
    "        dataset_epoch = datetime(2024, 1, 1).timestamp()\n",
    "        end_unix_ts = int(dataset_epoch + (90 * 86400))\n",
    "\n",
    "        latest = {\n",
    "            \"simulation_seconds\": end_unix_ts,  # End of dataset as Unix timestamp\n",
    "            \"offset_timestamp\": now.isoformat()\n",
    "        }\n",
    "        return json.dumps(latest)\n",
    "\n",
    "    def read(self, start_offset):\n",
    "        \"\"\"\n",
    "        Read events from start_offset to current time.\n",
    "        First run: Output all data from day 0 to START_DAY + current time (no multiplier).\n",
    "        Subsequent runs: Use speed multiplier to advance simulation time.\n",
    "        For simpleStreamReader, this returns (iterator, end_offset).\n",
    "        \"\"\"\n",
    "        # Parse start offset\n",
    "        start = json.loads(start_offset) if start_offset else {\"simulation_seconds\": 0, \"offset_timestamp\": datetime.utcnow().isoformat()}\n",
    "        start_sim_seconds = start[\"simulation_seconds\"]\n",
    "        start_real_time = datetime.fromisoformat(start[\"offset_timestamp\"])\n",
    "        is_initial = start.get(\"is_initial\", False)\n",
    "\n",
    "        # Calculate elapsed real time since last checkpoint\n",
    "        now = datetime.utcnow()\n",
    "        dataset_epoch = datetime(2024, 1, 1).timestamp()\n",
    "\n",
    "        if is_initial:\n",
    "            # First run: Just output all historical data up to START_DAY + current time\n",
    "            current_time_of_day = (now.hour * 3600) + (now.minute * 60) + now.second\n",
    "            end_sim_seconds = int(dataset_epoch + (self.sim_start_day * 86400) + current_time_of_day)\n",
    "            print(f\"ðŸ“– Reading events (FIRST RUN - historical catchup):\")\n",
    "            print(f\"   Start: day 0 00:00:00\")\n",
    "            print(f\"   End:   day {self.sim_start_day} @ {now.strftime('%H:%M:%S')}\")\n",
    "            print(f\"   Outputting all historical data (speed multiplier not used)\")\n",
    "        else:\n",
    "            # Subsequent runs: Use speed multiplier\n",
    "            elapsed_real_seconds = (now - start_real_time).total_seconds()\n",
    "            elapsed_sim_seconds = int(elapsed_real_seconds * self.speed_multiplier)\n",
    "            end_sim_seconds = start_sim_seconds + elapsed_sim_seconds\n",
    "\n",
    "            start_day = int((start_sim_seconds - dataset_epoch) / 86400)\n",
    "            end_day = int((end_sim_seconds - dataset_epoch) / 86400)\n",
    "\n",
    "            print(f\"ðŸ“– Reading events:\")\n",
    "            print(f\"   Start: {start_sim_seconds} Unix timestamp (day {start_day})\")\n",
    "            print(f\"   End:   {end_sim_seconds} Unix timestamp (day {end_day})\")\n",
    "            print(f\"   Real time elapsed: {elapsed_real_seconds:.1f}s â†’ Sim time: {elapsed_sim_seconds}s ({self.speed_multiplier}x)\")\n",
    "\n",
    "        # Cap at end of dataset (2024-01-01 + 90 days)\n",
    "        max_sim_seconds = int(dataset_epoch + (90 * 86400))\n",
    "        end_sim_seconds = min(end_sim_seconds, max_sim_seconds)\n",
    "\n",
    "        # Filter events to time window (ts_seconds is already absolute Unix timestamp)\n",
    "        windowed_events = self.events_df[\n",
    "            (self.events_df[\"ts_seconds\"] >= start_sim_seconds) &\n",
    "            (self.events_df[\"ts_seconds\"] < end_sim_seconds)\n",
    "        ].copy()\n",
    "\n",
    "        print(f\"   Found {len(windowed_events)} events in window\")\n",
    "\n",
    "        # Expand to full JSON format\n",
    "        expanded_rows = self._expand_to_json(windowed_events)\n",
    "\n",
    "        print(f\"   âœ… Returning {len(expanded_rows)} events\")\n",
    "\n",
    "        # Convert to Spark Row objects\n",
    "        rows = [Row(**row) for row in expanded_rows]\n",
    "\n",
    "        # Create end offset with current timestamp (remove is_initial flag)\n",
    "        end_offset_dict = {\n",
    "            \"simulation_seconds\": end_sim_seconds,\n",
    "            \"offset_timestamp\": now.isoformat()\n",
    "            # is_initial removed - subsequent runs will use speed multiplier\n",
    "        }\n",
    "\n",
    "        # Return iterator and end offset (format for simpleStreamReader)\n",
    "        return (iter(rows), json.dumps(end_offset_dict))\n",
    "\n",
    "    def commit(self, end_offset):\n",
    "        \"\"\"\n",
    "        Commit is handled by Spark's checkpoint.\n",
    "        No-op for our use case.\n",
    "        \"\"\"\n",
    "        end = json.loads(end_offset)\n",
    "        dataset_epoch = datetime(2024, 1, 1).timestamp()\n",
    "        day_num = int((end['simulation_seconds'] - dataset_epoch) / 86400)\n",
    "        print(f\"âœ“ Committed up to {end['simulation_seconds']} Unix timestamp (day {day_num})\")\n",
    "\n",
    "    def _expand_to_json(self, windowed_events_df):\n",
    "        \"\"\"\n",
    "        Expand compact parquet format to full JSON event format.\n",
    "        Data is already embedded in events.parquet - just need to format.\n",
    "        Uses pandas, returns list of dicts.\n",
    "        \"\"\"\n",
    "        # Event type mapping\n",
    "        event_types = {\n",
    "            1: \"order_created\",\n",
    "            2: \"gk_started\",\n",
    "            3: \"gk_finished\",\n",
    "            4: \"gk_ready\",\n",
    "            5: \"driver_arrived\",\n",
    "            6: \"driver_picked_up\",\n",
    "            7: \"driver_ping\",\n",
    "            8: \"delivered\"\n",
    "        }\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        for _, row in windowed_events_df.iterrows():\n",
    "            event_type = event_types[row[\"event_type_id\"]]\n",
    "\n",
    "            # Build body based on event type (data already embedded in parquet)\n",
    "            body = {}\n",
    "            if event_type == \"order_created\":\n",
    "                body = {\n",
    "                    \"customer_lat\": float(row[\"customer_lat\"]),\n",
    "                    \"customer_lon\": float(row[\"customer_lon\"]),\n",
    "                    \"customer_addr\": row[\"customer_addr\"],\n",
    "                    \"items\": json.loads(row[\"items_json\"])\n",
    "                }\n",
    "            elif event_type == \"driver_picked_up\":\n",
    "                if pd.notna(row[\"route_json\"]):\n",
    "                    body = {\n",
    "                        \"route_points\": json.loads(row[\"route_json\"])\n",
    "                    }\n",
    "            elif event_type == \"driver_ping\":\n",
    "                if pd.notna(row[\"ping_lat\"]):\n",
    "                    body = {\n",
    "                        \"progress_pct\": float(row[\"ping_progress\"]),\n",
    "                        \"loc_lat\": float(row[\"ping_lat\"]),\n",
    "                        \"loc_lon\": float(row[\"ping_lon\"])\n",
    "                    }\n",
    "            elif event_type == \"delivered\":\n",
    "                if pd.notna(row[\"customer_lat\"]):\n",
    "                    body = {\n",
    "                        \"delivered_lat\": float(row[\"customer_lat\"]),\n",
    "                        \"delivered_lon\": float(row[\"customer_lon\"])\n",
    "                    }\n",
    "\n",
    "            # Build event record\n",
    "            event_record = {\n",
    "                \"event_id\": str(uuid.uuid4()),\n",
    "                \"event_type\": event_type,\n",
    "                \"ts\": datetime.fromtimestamp(row[\"ts_seconds\"]).strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3],\n",
    "                \"location_id\": int(row[\"location_id\"]),\n",
    "                \"order_id\": str(row[\"order_id\"]),\n",
    "                \"sequence\": int(row[\"sequence\"]),\n",
    "                \"body\": json.dumps(body)\n",
    "            }\n",
    "\n",
    "            rows.append(event_record)\n",
    "\n",
    "        # Sort by simulation time and sequence\n",
    "        rows.sort(key=lambda x: (x[\"ts\"], x[\"sequence\"]))\n",
    "\n",
    "        return rows\n",
    "\n",
    "\n",
    "print(\"âœ… CaspersDataSource class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the custom data source with Spark\n",
    "spark.dataSource.register(CaspersDataSource)\n",
    "print(\"âœ… Caspers data source registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Streaming DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming DataFrame\n",
    "caspers_stream = spark.readStream \\\n",
    "    .format(\"caspers\") \\\n",
    "    .option(\"datasetPath\", DATASET_PATH) \\\n",
    "    .option(\"simulationStartDay\", str(START_DAY)) \\\n",
    "    .option(\"speedMultiplier\", str(SPEED_MULTIPLIER)) \\\n",
    "    .load()\n",
    "\n",
    "print(\"âœ… Streaming DataFrame created\")\n",
    "caspers_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Stream to Volume\n",
    "\n",
    "Uses `availableNow` trigger for scheduled runs - processes all available data since last checkpoint and stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start streaming query with availableNow trigger\n",
    "# This processes all available data since last checkpoint and stops (perfect for scheduled notebooks)\n",
    "query = caspers_stream \\\n",
    "    .writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", VOLUME_PATH) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "print(\"ðŸš€ Streaming query started with availableNow trigger\")\n",
    "print(\"   This will process all available events and stop\")\n",
    "\n",
    "# Wait for completion\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"âœ… Streaming query complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count files in volume\n",
    "files = dbutils.fs.ls(VOLUME_PATH)\n",
    "json_files = [f for f in files if f.name.endswith('.json')]\n",
    "\n",
    "print(f\"ðŸ“Š Total JSON files in volume: {len(json_files)}\")\n",
    "print(f\"   Volume path: {VOLUME_PATH}\")\n",
    "\n",
    "if json_files:\n",
    "    # Show sample files\n",
    "    print(\"\\nðŸ“„ Sample files:\")\n",
    "    for f in json_files[:5]:\n",
    "        print(f\"   {f.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
