{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casper's Kitchens - Canonical Data Replay\n",
    "\n",
    "This notebook replays pre-generated ghost kitchen events from the canonical dataset using a custom PySpark streaming data source.\n",
    "\n",
    "**Features:**\n",
    "- Reads from single self-contained events.parquet file (34 MB)\n",
    "- Uses checkpoint to track progress\n",
    "- Supports variable speed multiplier (1x, 60x, 3600x, etc.)\n",
    "- Works with `availableNow` trigger for scheduled runs\n",
    "\n",
    "**Dataset:**\n",
    "- 75,780 orders across 4 cities (SF, SV, Bellevue, Chicago)\n",
    "- 1,014,290 events (order lifecycle + driver tracking)\n",
    "- Real OpenStreetMap routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get parameters from widgets\n",
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "SCHEMA = dbutils.widgets.get(\"SCHEMA\")\n",
    "VOLUME = dbutils.widgets.get(\"VOLUME\")\n",
    "START_DAY = int(dbutils.widgets.get(\"START_DAY\"))\n",
    "SPEED_MULTIPLIER = float(dbutils.widgets.get(\"SPEED_MULTIPLIER\"))\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = os.path.abspath(\"./canonical_dataset\")  # Relative path to dataset in workspace\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "CHECKPOINT_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/misc/checkpoints\"\n",
    "\n",
    "print(f\"ðŸ“‹ Configuration:\")\n",
    "print(f\"   Catalog: {CATALOG}\")\n",
    "print(f\"   Schema: {SCHEMA}\")\n",
    "print(f\"   Volume: {VOLUME}\")\n",
    "print(f\"   Start Day: {START_DAY}\")\n",
    "print(f\"   Speed Multiplier: {SPEED_MULTIPLIER}x\")\n",
    "print(f\"   Dataset Path: {DATASET_PATH}\")\n",
    "print(f\"   Output Path: {VOLUME_PATH}\")\n",
    "print(f\"   Checkpoint Path: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.datasource import DataSource, DataSourceStreamReader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Source Definition\n",
    "\n",
    "**Architecture:** Lightweight reader + Spark transformations\n",
    "- **Reader:** Returns raw parquet columns (minimal processing)\n",
    "- **Spark:** Handles all transformations (event type mapping, timestamp shifting, JSON construction)\n",
    "- **Benefits:** Better performance, easier to maintain, leverages Spark's distributed processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaspersDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    Custom streaming data source that replays pre-generated Casper's Kitchens events.\n",
    "\n",
    "    Options:\n",
    "    - datasetPath: Path to canonical_dataset directory\n",
    "    - simulationStartDay: Which day to start simulation (0-89)\n",
    "    - speedMultiplier: Speed of replay (1.0=realtime, 60.0=60x speed, 3600.0=1hr per second)\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        return \"caspers\"\n",
    "\n",
    "    def schema(self):\n",
    "        \"\"\"Define the output schema - raw columns from parquet.\"\"\"\n",
    "        return StructType([\n",
    "            StructField(\"event_type_id\", IntegerType(), False),\n",
    "            StructField(\"ts_seconds\", IntegerType(), False),\n",
    "            StructField(\"location_id\", IntegerType(), False),\n",
    "            StructField(\"order_id\", StringType(), False),\n",
    "            StructField(\"sequence\", IntegerType(), False),\n",
    "            # Optional fields for different event types\n",
    "            StructField(\"customer_lat\", StringType(), True),\n",
    "            StructField(\"customer_lon\", StringType(), True),\n",
    "            StructField(\"customer_addr\", StringType(), True),\n",
    "            StructField(\"items_json\", StringType(), True),\n",
    "            StructField(\"route_json\", StringType(), True),\n",
    "            StructField(\"ping_lat\", StringType(), True),\n",
    "            StructField(\"ping_lon\", StringType(), True),\n",
    "            StructField(\"ping_progress\", StringType(), True),\n",
    "        ])\n",
    "\n",
    "    def simpleStreamReader(self, schema: StructType):\n",
    "        \"\"\"Return a simple stream reader instance (no partitioning needed).\"\"\"\n",
    "        return CaspersStreamReader(schema, self.options)\n",
    "\n",
    "\n",
    "class CaspersStreamReader(DataSourceStreamReader):\n",
    "    \"\"\"\n",
    "    Lightweight stream reader that tracks simulation time as offset.\n",
    "\n",
    "    Offset = {simulation_seconds: Unix timestamp, offset_timestamp: ISO string}\n",
    "\n",
    "    Each run processes: (current_real_time - checkpoint_time) Ã— speed_multiplier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, schema, options):\n",
    "        self.schema = schema\n",
    "        self.options = options\n",
    "\n",
    "        # Parse options\n",
    "        self.dataset_path = options.get(\"datasetPath\", \"./canonical_dataset\")\n",
    "        self.sim_start_day = int(options.get(\"simulationStartDay\", \"20\"))\n",
    "        self.speed_multiplier = float(options.get(\"speedMultiplier\", \"1.0\"))\n",
    "\n",
    "        # Load canonical dataset using pandas (NOT spark.read - avoids circular dependency!)\n",
    "        print(f\"ðŸ“¦ Loading dataset from {self.dataset_path}...\")\n",
    "        self.events_df = pd.read_parquet(f\"{self.dataset_path}/events.parquet\")\n",
    "\n",
    "        unique_orders = self.events_df['order_id'].nunique()\n",
    "        print(f\"âœ… Caspers DataSource initialized\")\n",
    "        print(f\"   Orders: {unique_orders:,}\")\n",
    "        print(f\"   Events: {len(self.events_df):,}\")\n",
    "        print(f\"   Start Day: {self.sim_start_day}\")\n",
    "        print(f\"   Speed: {self.speed_multiplier}x\")\n",
    "\n",
    "    def initialOffset(self):\n",
    "        \"\"\"\n",
    "        Return the starting offset for the stream.\n",
    "        First run outputs all historical data from day 0 to START_DAY + current time.\n",
    "        Speed multiplier not used for first run - just historical catchup.\n",
    "        \"\"\"\n",
    "        now = datetime.utcnow()\n",
    "        # Dataset starts at 2024-01-01 00:00:00\n",
    "        dataset_epoch = datetime(2024, 1, 1).timestamp()\n",
    "\n",
    "        # Start from beginning of dataset (day 0)\n",
    "        initial_unix_ts = int(dataset_epoch)\n",
    "\n",
    "        initial = {\n",
    "            \"simulation_seconds\": initial_unix_ts,\n",
    "            \"offset_timestamp\": now.isoformat(),\n",
    "            \"is_initial\": True  # Flag to indicate this is the first run\n",
    "        }\n",
    "\n",
    "        print(f\"ðŸŽ¬ Initial offset: day 0 (dataset start)\")\n",
    "        print(f\"   First run will output all data from day 0 â†’ day {self.sim_start_day} @ {now.strftime('%H:%M:%S')}\")\n",
    "        print(f\"   Speed multiplier ({self.speed_multiplier}x) will be used for subsequent runs\")\n",
    "        return json.dumps(initial)\n",
    "\n",
    "    def latestOffset(self):\n",
    "        \"\"\"\n",
    "        Return the current offset based on time elapsed since last checkpoint.\n",
    "        Each run processes: (current_time - last_offset_time) * speed_multiplier\n",
    "        \"\"\"\n",
    "        now = datetime.utcnow()\n",
    "        # End of dataset: 2024-01-01 + 90 days\n",
    "        dataset_epoch = datetime(2024, 1, 1).timestamp()\n",
    "        end_unix_ts = int(dataset_epoch + (90 * 86400))\n",
    "\n",
    "        latest = {\n",
    "            \"simulation_seconds\": end_unix_ts,  # End of dataset as Unix timestamp\n",
    "            \"offset_timestamp\": now.isoformat()\n",
    "        }\n",
    "        return json.dumps(latest)\n",
    "\n",
    "    def read(self, start_offset):\n",
    "        \"\"\"\n",
    "        Read events from start_offset to current time.\n",
    "        Returns raw parquet columns - transformations done in Spark.\n",
    "        \"\"\"\n",
    "        # Parse start offset\n",
    "        start = json.loads(start_offset) if start_offset else {\"simulation_seconds\": 0, \"offset_timestamp\": datetime.utcnow().isoformat()}\n",
    "        start_sim_seconds = start[\"simulation_seconds\"]\n",
    "        start_real_time = datetime.fromisoformat(start[\"offset_timestamp\"])\n",
    "        is_initial = start.get(\"is_initial\", False)\n",
    "\n",
    "        # Calculate elapsed real time since last checkpoint\n",
    "        now = datetime.utcnow()\n",
    "        dataset_epoch = datetime(2024, 1, 1).timestamp()\n",
    "\n",
    "        if is_initial:\n",
    "            # First run: Just output all historical data up to START_DAY + current time\n",
    "            current_time_of_day = (now.hour * 3600) + (now.minute * 60) + now.second\n",
    "            end_sim_seconds = int(dataset_epoch + (self.sim_start_day * 86400) + current_time_of_day)\n",
    "            print(f\"ðŸ“– Reading events (FIRST RUN - historical catchup):\")\n",
    "            print(f\"   Start: day 0 00:00:00\")\n",
    "            print(f\"   End:   day {self.sim_start_day} @ {now.strftime('%H:%M:%S')}\")\n",
    "            print(f\"   Outputting all historical data (speed multiplier not used)\")\n",
    "        else:\n",
    "            # Subsequent runs: Use speed multiplier\n",
    "            elapsed_real_seconds = (now - start_real_time).total_seconds()\n",
    "            elapsed_sim_seconds = int(elapsed_real_seconds * self.speed_multiplier)\n",
    "            end_sim_seconds = start_sim_seconds + elapsed_sim_seconds\n",
    "\n",
    "            start_day = int((start_sim_seconds - dataset_epoch) / 86400)\n",
    "            end_day = int((end_sim_seconds - dataset_epoch) / 86400)\n",
    "\n",
    "            print(f\"ðŸ“– Reading events:\")\n",
    "            print(f\"   Start: {start_sim_seconds} Unix timestamp (day {start_day})\")\n",
    "            print(f\"   End:   {end_sim_seconds} Unix timestamp (day {end_day})\")\n",
    "            print(f\"   Real time elapsed: {elapsed_real_seconds:.1f}s â†’ Sim time: {elapsed_sim_seconds}s ({self.speed_multiplier}x)\")\n",
    "\n",
    "        # Cap at end of dataset (2024-01-01 + 90 days)\n",
    "        max_sim_seconds = int(dataset_epoch + (90 * 86400))\n",
    "        end_sim_seconds = min(end_sim_seconds, max_sim_seconds)\n",
    "\n",
    "        # Filter events to time window (ts_seconds is already absolute Unix timestamp)\n",
    "        windowed_events = self.events_df[\n",
    "            (self.events_df[\"ts_seconds\"] >= start_sim_seconds) &\n",
    "            (self.events_df[\"ts_seconds\"] < end_sim_seconds)\n",
    "        ].copy()\n",
    "\n",
    "        print(f\"   Found {len(windowed_events)} events in window\")\n",
    "        print(f\"   âœ… Returning {len(windowed_events)} raw events (transformations will be done in Spark)\")\n",
    "\n",
    "        # Convert to Spark Row objects - return raw columns as-is\n",
    "        rows = [Row(**row.to_dict()) for _, row in windowed_events.iterrows()]\n",
    "\n",
    "        # Create end offset with current timestamp (remove is_initial flag)\n",
    "        end_offset_dict = {\n",
    "            \"simulation_seconds\": end_sim_seconds,\n",
    "            \"offset_timestamp\": now.isoformat()\n",
    "            # is_initial removed - subsequent runs will use speed multiplier\n",
    "        }\n",
    "\n",
    "        # Return iterator and end offset (format for simpleStreamReader)\n",
    "        return (iter(rows), json.dumps(end_offset_dict))\n",
    "\n",
    "    def commit(self, end_offset):\n",
    "        \"\"\"\n",
    "        Commit is handled by Spark's checkpoint.\n",
    "        No-op for our use case.\n",
    "        \"\"\"\n",
    "        end = json.loads(end_offset)\n",
    "        dataset_epoch = datetime(2024, 1, 1).timestamp()\n",
    "        day_num = int((end['simulation_seconds'] - dataset_epoch) / 86400)\n",
    "        print(f\"âœ“ Committed up to {end['simulation_seconds']} Unix timestamp (day {day_num})\")\n",
    "\n",
    "\n",
    "print(\"âœ… CaspersDataSource class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the custom data source with Spark\n",
    "spark.dataSource.register(CaspersDataSource)\n",
    "print(\"âœ… Caspers data source registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Streaming DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming DataFrame\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"caspers\") \\\n",
    "    .option(\"datasetPath\", DATASET_PATH) \\\n",
    "    .option(\"simulationStartDay\", str(START_DAY)) \\\n",
    "    .option(\"speedMultiplier\", str(SPEED_MULTIPLIER)) \\\n",
    "    .load()\n",
    "\n",
    "print(\"âœ… Raw streaming DataFrame created\")\n",
    "raw_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Spark Transformations\n",
    "\n",
    "Transform raw parquet columns into final event schema with:\n",
    "- Event type mapping\n",
    "- Timestamp shifting\n",
    "- Body JSON construction\n",
    "- UUID generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate time shift: current date - START_DAY days = dataset day 0\n",
    "now = datetime.utcnow()\n",
    "today_midnight = datetime(now.year, now.month, now.day)\n",
    "dataset_day_0_in_current_time = today_midnight - timedelta(days=START_DAY)\n",
    "dataset_epoch = datetime(2024, 1, 1)\n",
    "time_shift_seconds = int((dataset_day_0_in_current_time - dataset_epoch).total_seconds())\n",
    "\n",
    "print(f\"â° Time shift calculation:\")\n",
    "print(f\"   Dataset day 0 (2024-01-01) â†’ {dataset_day_0_in_current_time.strftime('%Y-%m-%d')}\")\n",
    "print(f\"   Dataset day {START_DAY} (simulation start) â†’ {today_midnight.strftime('%Y-%m-%d')}\")\n",
    "print(f\"   Time shift: {time_shift_seconds} seconds\")\n",
    "\n",
    "# Step 1: Map event type IDs to names\n",
    "df_with_event_type = raw_stream.withColumn(\n",
    "    \"event_type\",\n",
    "    F.when(F.col(\"event_type_id\") == 1, \"order_created\")\n",
    "     .when(F.col(\"event_type_id\") == 2, \"gk_started\")\n",
    "     .when(F.col(\"event_type_id\") == 3, \"gk_finished\")\n",
    "     .when(F.col(\"event_type_id\") == 4, \"gk_ready\")\n",
    "     .when(F.col(\"event_type_id\") == 5, \"driver_arrived\")\n",
    "     .when(F.col(\"event_type_id\") == 6, \"driver_picked_up\")\n",
    "     .when(F.col(\"event_type_id\") == 7, \"driver_ping\")\n",
    "     .when(F.col(\"event_type_id\") == 8, \"delivered\")\n",
    ")\n",
    "\n",
    "# Step 2: Apply timestamp shift and format\n",
    "df_with_timestamp = df_with_event_type.withColumn(\n",
    "    \"shifted_ts_seconds\", \n",
    "    F.col(\"ts_seconds\") + F.lit(time_shift_seconds)\n",
    ").withColumn(\n",
    "    \"ts\",\n",
    "    F.date_format(F.from_unixtime(F.col(\"shifted_ts_seconds\")), \"yyyy-MM-dd HH:mm:ss.SSS\")\n",
    ")\n",
    "\n",
    "# Step 3: Build body JSON based on event type\n",
    "df_with_body = df_with_timestamp.withColumn(\n",
    "    \"body\",\n",
    "    F.when(F.col(\"event_type\") == \"order_created\",\n",
    "           F.to_json(F.struct(\n",
    "               F.col(\"customer_lat\").cast(\"double\").alias(\"customer_lat\"),\n",
    "               F.col(\"customer_lon\").cast(\"double\").alias(\"customer_lon\"),\n",
    "               F.col(\"customer_addr\"),\n",
    "               F.from_json(F.col(\"items_json\"), \"array<struct<id:int,name:string,price:double,qty:int>>\").alias(\"items\")\n",
    "           ))\n",
    "    )\n",
    "    .when(F.col(\"event_type\") == \"driver_picked_up\",\n",
    "           F.when(F.col(\"route_json\").isNotNull(),\n",
    "                  F.to_json(F.struct(\n",
    "                      F.from_json(F.col(\"route_json\"), \"array<array<double>>\").alias(\"route_points\")\n",
    "                  ))\n",
    "           ).otherwise(F.lit(\"{}\"))\n",
    "    )\n",
    "    .when(F.col(\"event_type\") == \"driver_ping\",\n",
    "           F.when(F.col(\"ping_lat\").isNotNull(),\n",
    "                  F.to_json(F.struct(\n",
    "                      F.col(\"ping_progress\").cast(\"double\").alias(\"progress_pct\"),\n",
    "                      F.col(\"ping_lat\").cast(\"double\").alias(\"loc_lat\"),\n",
    "                      F.col(\"ping_lon\").cast(\"double\").alias(\"loc_lon\")\n",
    "                  ))\n",
    "           ).otherwise(F.lit(\"{}\"))\n",
    "    )\n",
    "    .when(F.col(\"event_type\") == \"delivered\",\n",
    "           F.when(F.col(\"customer_lat\").isNotNull(),\n",
    "                  F.to_json(F.struct(\n",
    "                      F.col(\"customer_lat\").cast(\"double\").alias(\"delivered_lat\"),\n",
    "                      F.col(\"customer_lon\").cast(\"double\").alias(\"delivered_lon\")\n",
    "                  ))\n",
    "           ).otherwise(F.lit(\"{}\"))\n",
    "    )\n",
    "    .otherwise(F.lit(\"{}\"))\n",
    ")\n",
    "\n",
    "# Step 4: Generate UUIDs and select final schema\n",
    "caspers_stream = df_with_body \\\n",
    "    .withColumn(\"event_id\", F.expr(\"uuid()\")) \\\n",
    "    .select(\n",
    "        \"event_id\",\n",
    "        \"event_type\",\n",
    "        \"ts\",\n",
    "        \"location_id\",\n",
    "        \"order_id\",\n",
    "        \"sequence\",\n",
    "        \"body\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… Transformations applied\")\n",
    "caspers_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Stream to Volume\n",
    "\n",
    "Uses `availableNow` trigger for scheduled runs - processes all available data since last checkpoint and stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start streaming query with availableNow trigger\n",
    "# This processes all available data since last checkpoint and stops (perfect for scheduled notebooks)\n",
    "query = caspers_stream \\\n",
    "    .writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", VOLUME_PATH) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "print(\"ðŸš€ Streaming query started with availableNow trigger\")\n",
    "print(\"   This will process all available events and stop\")\n",
    "\n",
    "# Wait for completion\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"âœ… Streaming query complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count files in volume\n",
    "#files = dbutils.fs.ls(VOLUME_PATH)\n",
    "#json_files = [f for f in files if f.name.endswith('.json')]\n",
    "\n",
    "#print(f\"ðŸ“Š Total JSON files in volume: {len(json_files)}\")\n",
    "#print(f\"   Volume path: {VOLUME_PATH}\")\n",
    "\n",
    "#if json_files:\n",
    "#    # Show sample files\n",
    "#    print(\"\\nðŸ“„ Sample files:\")\n",
    "#    for f in json_files[:5]:\n",
    "#        print(f\"   {f.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
