{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casper's Kitchens - Simple Canonical Data Replay\n",
    "\n",
    "**State management:**\n",
    "- Watermark: derived from max timestamp in written data\n",
    "- Sim start time: stored in `_sim_start` file\n",
    "\n",
    "**Exactly-once:** If job fails before updating watermark, next run processes same window again (idempotent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Create widgets if running interactively\n",
    "try:\n",
    "    dbutils.widgets.text(\"CATALOG\", \"caspersdev\")\n",
    "    dbutils.widgets.text(\"SCHEMA\", \"simulator\")\n",
    "    dbutils.widgets.text(\"VOLUME\", \"events\")\n",
    "    dbutils.widgets.text(\"START_DAY\", \"70\")\n",
    "    dbutils.widgets.text(\"SPEED_MULTIPLIER\", \"60.0\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Get parameters\n",
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "SCHEMA = dbutils.widgets.get(\"SCHEMA\")\n",
    "VOLUME = dbutils.widgets.get(\"VOLUME\")\n",
    "START_DAY = int(dbutils.widgets.get(\"START_DAY\"))\n",
    "SPEED_MULTIPLIER = float(dbutils.widgets.get(\"SPEED_MULTIPLIER\"))\n",
    "\n",
    "# Paths\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "WATERMARK_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/misc/_watermark\"\n",
    "SIM_START_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/misc/_sim_start\"\n",
    "\n",
    "# Constants\n",
    "DATASET_EPOCH = int(datetime(2024, 1, 1).timestamp())\n",
    "DATASET_DAYS = 90\n",
    "CYCLE_SECONDS = DATASET_DAYS * 86400\n",
    "NOW = datetime.utcnow()\n",
    "\n",
    "print(f\"Config: START_DAY={START_DAY}, SPEED={SPEED_MULTIPLIER}x\")\n",
    "print(f\"Output: {VOLUME_PATH}\")\n",
    "print(f\"Dataset cycle: {DATASET_DAYS} days ({CYCLE_SECONDS} seconds)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Canonical Dataset (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to load from workspace (spark.read doesn't work with workspace paths)\n",
    "print(\"Loading canonical dataset...\")\n",
    "events_pdf = pd.read_parquet(\"./canonical_dataset/events.parquet\")\n",
    "print(f\"Loaded {len(events_pdf):,} events\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read State Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to read watermark (last processed virtual simulation timestamp)\n",
    "try:\n",
    "    watermark_data = spark.read.text(WATERMARK_PATH).first()[0]\n",
    "    last_sim_seconds = int(watermark_data)\n",
    "    is_first_run = False\n",
    "    virtual_day = int((last_sim_seconds - DATASET_EPOCH) / 86400)\n",
    "    loop_index = int((last_sim_seconds - DATASET_EPOCH) / CYCLE_SECONDS)\n",
    "    print(f\"Watermark: {last_sim_seconds} (virtual day {virtual_day}, loop {loop_index})\")\n",
    "except:\n",
    "    last_sim_seconds = DATASET_EPOCH\n",
    "    is_first_run = True\n",
    "    print(\"No watermark - first run\")\n",
    "\n",
    "# Try to read simulation start time\n",
    "try:\n",
    "    sim_start_data = spark.read.text(SIM_START_PATH).first()[0]\n",
    "    sim_start_time = datetime.fromisoformat(sim_start_data)\n",
    "    print(f\"Sim started: {sim_start_time.isoformat()}\")\n",
    "except:\n",
    "    sim_start_time = NOW\n",
    "    print(f\"Establishing sim start: {sim_start_time.isoformat()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate New Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_first_run:\n",
    "    # First run: day 0 -> START_DAY + current time\n",
    "    current_tod = (NOW.hour * 3600) + (NOW.minute * 60) + NOW.second\n",
    "    new_end_seconds = int(DATASET_EPOCH + (START_DAY * 86400) + current_tod)\n",
    "    print(f\"\\nFIRST RUN: day 0 -> day {START_DAY} @ {NOW.strftime('%H:%M:%S')}\")\n",
    "else:\n",
    "    # Subsequent runs: apply speed multiplier\n",
    "    elapsed_real = (NOW - sim_start_time).total_seconds()\n",
    "    elapsed_sim = int(elapsed_real * SPEED_MULTIPLIER)\n",
    "\n",
    "    sim_start_tod = (sim_start_time.hour * 3600) + (sim_start_time.minute * 60) + sim_start_time.second\n",
    "    start_position = int(DATASET_EPOCH + (START_DAY * 86400) + sim_start_tod)\n",
    "    new_end_seconds = start_position + elapsed_sim\n",
    "\n",
    "    print(\"\\nSPEED MODE:\")\n",
    "    print(f\"   Real elapsed: {elapsed_real:.0f}s ({elapsed_real/60:.1f} min)\")\n",
    "    print(f\"   Sim elapsed: {elapsed_sim}s ({elapsed_sim/3600:.1f} hours)\")\n",
    "\n",
    "start_virtual_day = int((last_sim_seconds - DATASET_EPOCH) / 86400)\n",
    "end_virtual_day = int((new_end_seconds - DATASET_EPOCH) / 86400)\n",
    "start_loop = int((last_sim_seconds - DATASET_EPOCH) / CYCLE_SECONDS)\n",
    "end_loop = int((new_end_seconds - DATASET_EPOCH) / CYCLE_SECONDS)\n",
    "\n",
    "print(f\"   Processing: virtual day {start_virtual_day} -> {end_virtual_day}\")\n",
    "print(f\"   Loops touched: {start_loop} -> {end_loop}\")\n",
    "\n",
    "if new_end_seconds <= last_sim_seconds:\n",
    "    print(\"No new data\")\n",
    "    dbutils.notebook.exit(\"No new data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build virtual window across one or more 90-day loops\n",
    "segments = []\n",
    "start_loop = int((last_sim_seconds - DATASET_EPOCH) / CYCLE_SECONDS)\n",
    "end_loop = int((new_end_seconds - DATASET_EPOCH) / CYCLE_SECONDS)\n",
    "\n",
    "for loop_idx in range(start_loop, end_loop + 1):\n",
    "    loop_start_virtual = DATASET_EPOCH + (loop_idx * CYCLE_SECONDS)\n",
    "    loop_end_virtual = loop_start_virtual + CYCLE_SECONDS\n",
    "\n",
    "    segment_start_virtual = max(last_sim_seconds, loop_start_virtual)\n",
    "    segment_end_virtual = min(new_end_seconds, loop_end_virtual)\n",
    "\n",
    "    if segment_end_virtual <= segment_start_virtual:\n",
    "        continue\n",
    "\n",
    "    # Map this virtual segment back into the base 90-day dataset window\n",
    "    segment_start_dataset = DATASET_EPOCH + (segment_start_virtual - loop_start_virtual)\n",
    "    segment_end_dataset = DATASET_EPOCH + (segment_end_virtual - loop_start_virtual)\n",
    "\n",
    "    segment_pdf = events_pdf[\n",
    "        (events_pdf[\"ts_seconds\"] > segment_start_dataset) &\n",
    "        (events_pdf[\"ts_seconds\"] <= segment_end_dataset)\n",
    "    ].copy()\n",
    "\n",
    "    if segment_pdf.empty:\n",
    "        continue\n",
    "\n",
    "    # Keep timestamps increasing forever across loops\n",
    "    segment_pdf[\"virtual_ts_seconds\"] = segment_pdf[\"ts_seconds\"] + (loop_idx * CYCLE_SECONDS)\n",
    "\n",
    "    # Keep order IDs unique across loops without adding schema columns\n",
    "    if loop_idx > 0:\n",
    "        segment_pdf[\"order_id\"] = segment_pdf[\"order_id\"].astype(str) + f\"-L{loop_idx}\"\n",
    "\n",
    "    segments.append(segment_pdf)\n",
    "\n",
    "if not segments:\n",
    "    print(\"No events in window\")\n",
    "    dbutils.notebook.exit(\"No events\")\n",
    "\n",
    "new_events_pdf = pd.concat(segments, ignore_index=True)\n",
    "event_count = len(new_events_pdf)\n",
    "print(f\"Processing {event_count:,} events\")\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "new_events = spark.createDataFrame(new_events_pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time shift\n",
    "today_midnight = datetime(NOW.year, NOW.month, NOW.day)\n",
    "dataset_day_0 = today_midnight - timedelta(days=START_DAY)\n",
    "TIME_SHIFT = int((dataset_day_0 - datetime(2024, 1, 1)).total_seconds())\n",
    "\n",
    "# Transform\n",
    "final_df = new_events     .withColumn(\"event_type\",\n",
    "        F.when(F.col(\"event_type_id\") == 1, \"order_created\")\n",
    "         .when(F.col(\"event_type_id\") == 2, \"gk_started\")\n",
    "         .when(F.col(\"event_type_id\") == 3, \"gk_finished\")\n",
    "         .when(F.col(\"event_type_id\") == 4, \"gk_ready\")\n",
    "         .when(F.col(\"event_type_id\") == 5, \"driver_arrived\")\n",
    "         .when(F.col(\"event_type_id\") == 6, \"driver_picked_up\")\n",
    "         .when(F.col(\"event_type_id\") == 7, \"driver_ping\")\n",
    "         .when(F.col(\"event_type_id\") == 8, \"delivered\")\n",
    "    )     .withColumn(\"ts\",\n",
    "        F.date_format(F.from_unixtime(F.col(\"virtual_ts_seconds\") + F.lit(TIME_SHIFT)), \"yyyy-MM-dd HH:mm:ss.SSS\")\n",
    "    )     .withColumn(\"body\",\n",
    "        F.when(F.col(\"event_type\") == \"order_created\",\n",
    "            F.to_json(F.struct(\n",
    "                F.col(\"customer_lat\").cast(\"double\").alias(\"customer_lat\"),\n",
    "                F.col(\"customer_lon\").cast(\"double\").alias(\"customer_lon\"),\n",
    "                F.col(\"customer_addr\"),\n",
    "                F.from_json(F.col(\"items_json\"), \n",
    "                    \"array<struct<id:int,category_id:int,menu_id:int,brand_id:int,name:string,price:double,qty:int>>\"\n",
    "                ).alias(\"items\")\n",
    "            ))\n",
    "        )\n",
    "        .when(F.col(\"event_type\") == \"driver_picked_up\",\n",
    "            F.when(F.col(\"route_json\").isNotNull(),\n",
    "                F.to_json(F.struct(F.from_json(F.col(\"route_json\"), \"array<array<double>>\").alias(\"route_points\")))\n",
    "            ).otherwise(F.lit(\"{}\"))\n",
    "        )\n",
    "        .when(F.col(\"event_type\") == \"driver_ping\",\n",
    "            F.when(F.col(\"ping_lat\").isNotNull(),\n",
    "                F.to_json(F.struct(\n",
    "                    F.col(\"ping_progress\").cast(\"double\").alias(\"progress_pct\"),\n",
    "                    F.col(\"ping_lat\").cast(\"double\").alias(\"loc_lat\"),\n",
    "                    F.col(\"ping_lon\").cast(\"double\").alias(\"loc_lon\")\n",
    "                ))\n",
    "            ).otherwise(F.lit(\"{}\"))\n",
    "        )\n",
    "        .when(F.col(\"event_type\") == \"delivered\",\n",
    "            F.when(F.col(\"customer_lat\").isNotNull(),\n",
    "                F.to_json(F.struct(\n",
    "                    F.col(\"customer_lat\").cast(\"double\").alias(\"delivered_lat\"),\n",
    "                    F.col(\"customer_lon\").cast(\"double\").alias(\"delivered_lon\")\n",
    "                ))\n",
    "            ).otherwise(F.lit(\"{}\"))\n",
    "        )\n",
    "        .otherwise(F.lit(\"{}\"))\n",
    "    )     .withColumn(\"event_id\", F.expr(\"uuid()\"))     .select(\"event_id\", \"event_type\", \"ts\", \"location_id\", \"order_id\", \"sequence\", \"body\")\n",
    "\n",
    "print(\"Transformed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write & Update State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write\n",
    "final_df.write.mode(\"append\").json(VOLUME_PATH)\n",
    "print(f\"Wrote {event_count:,} events\")\n",
    "\n",
    "# Update watermark (virtual simulation cursor)\n",
    "spark.createDataFrame([(str(new_end_seconds),)], [\"value\"]).write.mode(\"overwrite\").text(WATERMARK_PATH)\n",
    "end_virtual_day = int((new_end_seconds - DATASET_EPOCH) / 86400)\n",
    "end_loop = int((new_end_seconds - DATASET_EPOCH) / CYCLE_SECONDS)\n",
    "print(f\"Watermark: {new_end_seconds} (virtual day {end_virtual_day}, loop {end_loop})\")\n",
    "\n",
    "# Save sim start time (first run only)\n",
    "if is_first_run:\n",
    "    spark.createDataFrame([(sim_start_time.isoformat(),)], [\"value\"]).write.mode(\"overwrite\").text(SIM_START_PATH)\n",
    "    print(\"Saved sim start time\")\n",
    "\n",
    "print(\"\\nComplete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}