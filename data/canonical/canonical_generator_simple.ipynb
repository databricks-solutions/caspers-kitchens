{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casper's Kitchens - Simple Canonical Data Replay\n",
    "\n",
    "**State management:**\n",
    "- Watermark: derived from max timestamp in written data\n",
    "- Sim start time: stored in `_sim_start` file\n",
    "\n",
    "**Exactly-once:** If job fails before updating watermark, next run processes same window again (idempotent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Create widgets if running interactively\n",
    "try:\n",
    "    dbutils.widgets.text(\"CATALOG\", \"caspersdev\")\n",
    "    dbutils.widgets.text(\"SCHEMA\", \"simulator\")\n",
    "    dbutils.widgets.text(\"VOLUME\", \"events\")\n",
    "    dbutils.widgets.text(\"START_DAY\", \"70\")\n",
    "    dbutils.widgets.text(\"SPEED_MULTIPLIER\", \"60.0\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Get parameters\n",
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "SCHEMA = dbutils.widgets.get(\"SCHEMA\")\n",
    "VOLUME = dbutils.widgets.get(\"VOLUME\")\n",
    "START_DAY = int(dbutils.widgets.get(\"START_DAY\"))\n",
    "SPEED_MULTIPLIER = float(dbutils.widgets.get(\"SPEED_MULTIPLIER\"))\n",
    "\n",
    "# Paths\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "WATERMARK_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/misc/_watermark\"\n",
    "SIM_START_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/misc/_sim_start\"\n",
    "\n",
    "# Constants\n",
    "DATASET_EPOCH = datetime(2024, 1, 1).timestamp()\n",
    "NOW = datetime.utcnow()\n",
    "\n",
    "print(f\"üìã Config: START_DAY={START_DAY}, SPEED={SPEED_MULTIPLIER}x\")\n",
    "print(f\"üìÇ Output: {VOLUME_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Canonical Dataset (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to load from workspace (spark.read doesn't work with workspace paths)\n",
    "print(\"üì¶ Loading canonical dataset...\")\n",
    "events_pdf = pd.read_parquet(\"./canonical_dataset/events.parquet\")\n",
    "print(f\"‚úÖ Loaded {len(events_pdf):,} events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read State Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to read watermark (last processed timestamp)\n",
    "try:\n",
    "    watermark_data = spark.read.text(WATERMARK_PATH).first()[0]\n",
    "    last_sim_seconds = int(watermark_data)\n",
    "    is_first_run = False\n",
    "    last_day = int((last_sim_seconds - DATASET_EPOCH) / 86400)\n",
    "    print(f\"üìå Watermark: {last_sim_seconds} (day {last_day})\")\n",
    "except:\n",
    "    last_sim_seconds = int(DATASET_EPOCH)\n",
    "    is_first_run = True\n",
    "    print(f\"üìå No watermark - first run\")\n",
    "\n",
    "# Try to read simulation start time\n",
    "try:\n",
    "    sim_start_data = spark.read.text(SIM_START_PATH).first()[0]\n",
    "    sim_start_time = datetime.fromisoformat(sim_start_data)\n",
    "    print(f\"‚è∞ Sim started: {sim_start_time.isoformat()}\")\n",
    "except:\n",
    "    sim_start_time = NOW\n",
    "    print(f\"‚è∞ Establishing sim start: {sim_start_time.isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate New Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_first_run:\n",
    "    # First run: day 0 ‚Üí START_DAY + current time\n",
    "    current_tod = (NOW.hour * 3600) + (NOW.minute * 60) + NOW.second\n",
    "    new_end_seconds = int(DATASET_EPOCH + (START_DAY * 86400) + current_tod)\n",
    "    print(f\"\\nüé¨ FIRST RUN: day 0 ‚Üí day {START_DAY} @ {NOW.strftime('%H:%M:%S')}\")\n",
    "else:\n",
    "    # Subsequent runs: apply speed multiplier\n",
    "    elapsed_real = (NOW - sim_start_time).total_seconds()\n",
    "    elapsed_sim = int(elapsed_real * SPEED_MULTIPLIER)\n",
    "    \n",
    "    sim_start_tod = (sim_start_time.hour * 3600) + (sim_start_time.minute * 60) + sim_start_time.second\n",
    "    start_position = int(DATASET_EPOCH + (START_DAY * 86400) + sim_start_tod)\n",
    "    new_end_seconds = start_position + elapsed_sim\n",
    "    \n",
    "    # Cap at dataset end\n",
    "    max_seconds = int(DATASET_EPOCH + (90 * 86400))\n",
    "    new_end_seconds = min(new_end_seconds, max_seconds)\n",
    "    \n",
    "    print(f\"\\n‚ö° SPEED MODE:\")\n",
    "    print(f\"   Real elapsed: {elapsed_real:.0f}s ({elapsed_real/60:.1f} min)\")\n",
    "    print(f\"   Sim elapsed: {elapsed_sim}s ({elapsed_sim/3600:.1f} hours)\")\n",
    "\n",
    "start_day = int((last_sim_seconds - DATASET_EPOCH) / 86400)\n",
    "end_day = int((new_end_seconds - DATASET_EPOCH) / 86400)\n",
    "print(f\"   Processing: day {start_day} ‚Üí day {end_day}\")\n",
    "\n",
    "if new_end_seconds <= last_sim_seconds:\n",
    "    print(f\"‚è∏Ô∏è  No new data\")\n",
    "    dbutils.notebook.exit(\"No new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter pandas dataframe\n",
    "new_events_pdf = events_pdf[\n",
    "    (events_pdf[\"ts_seconds\"] > last_sim_seconds) & \n",
    "    (events_pdf[\"ts_seconds\"] <= new_end_seconds)\n",
    "]\n",
    "\n",
    "event_count = len(new_events_pdf)\n",
    "print(f\"üì¶ Processing {event_count:,} events\")\n",
    "\n",
    "if event_count == 0:\n",
    "    print(f\"‚è∏Ô∏è  No events in window\")\n",
    "    dbutils.notebook.exit(\"No events\")\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "new_events = spark.createDataFrame(new_events_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time shift\n",
    "today_midnight = datetime(NOW.year, NOW.month, NOW.day)\n",
    "dataset_day_0 = today_midnight - timedelta(days=START_DAY)\n",
    "TIME_SHIFT = int((dataset_day_0 - datetime(2024, 1, 1)).total_seconds())\n",
    "\n",
    "# Transform\n",
    "final_df = new_events \\\n",
    "    .withColumn(\"event_type\",\n",
    "        F.when(F.col(\"event_type_id\") == 1, \"order_created\")\n",
    "         .when(F.col(\"event_type_id\") == 2, \"gk_started\")\n",
    "         .when(F.col(\"event_type_id\") == 3, \"gk_finished\")\n",
    "         .when(F.col(\"event_type_id\") == 4, \"gk_ready\")\n",
    "         .when(F.col(\"event_type_id\") == 5, \"driver_arrived\")\n",
    "         .when(F.col(\"event_type_id\") == 6, \"driver_picked_up\")\n",
    "         .when(F.col(\"event_type_id\") == 7, \"driver_ping\")\n",
    "         .when(F.col(\"event_type_id\") == 8, \"delivered\")\n",
    "    ) \\\n",
    "    .withColumn(\"ts\",\n",
    "        F.date_format(F.from_unixtime(F.col(\"ts_seconds\") + F.lit(TIME_SHIFT)), \"yyyy-MM-dd HH:mm:ss.SSS\")\n",
    "    ) \\\n",
    "    .withColumn(\"body\",\n",
    "        F.when(F.col(\"event_type\") == \"order_created\",\n",
    "            F.to_json(F.struct(\n",
    "                F.col(\"customer_lat\").cast(\"double\").alias(\"customer_lat\"),\n",
    "                F.col(\"customer_lon\").cast(\"double\").alias(\"customer_lon\"),\n",
    "                F.col(\"customer_addr\"),\n",
    "                F.from_json(F.col(\"items_json\"), \n",
    "                    \"array<struct<id:int,category_id:int,menu_id:int,brand_id:int,name:string,price:double,qty:int>>\"\n",
    "                ).alias(\"items\")\n",
    "            ))\n",
    "        )\n",
    "        .when(F.col(\"event_type\") == \"driver_picked_up\",\n",
    "            F.when(F.col(\"route_json\").isNotNull(),\n",
    "                F.to_json(F.struct(F.from_json(F.col(\"route_json\"), \"array<array<double>>\").alias(\"route_points\")))\n",
    "            ).otherwise(F.lit(\"{}\"))\n",
    "        )\n",
    "        .when(F.col(\"event_type\") == \"driver_ping\",\n",
    "            F.when(F.col(\"ping_lat\").isNotNull(),\n",
    "                F.to_json(F.struct(\n",
    "                    F.col(\"ping_progress\").cast(\"double\").alias(\"progress_pct\"),\n",
    "                    F.col(\"ping_lat\").cast(\"double\").alias(\"loc_lat\"),\n",
    "                    F.col(\"ping_lon\").cast(\"double\").alias(\"loc_lon\")\n",
    "                ))\n",
    "            ).otherwise(F.lit(\"{}\"))\n",
    "        )\n",
    "        .when(F.col(\"event_type\") == \"delivered\",\n",
    "            F.when(F.col(\"customer_lat\").isNotNull(),\n",
    "                F.to_json(F.struct(\n",
    "                    F.col(\"customer_lat\").cast(\"double\").alias(\"delivered_lat\"),\n",
    "                    F.col(\"customer_lon\").cast(\"double\").alias(\"delivered_lon\")\n",
    "                ))\n",
    "            ).otherwise(F.lit(\"{}\"))\n",
    "        )\n",
    "        .otherwise(F.lit(\"{}\"))\n",
    "    ) \\\n",
    "    .withColumn(\"event_id\", F.expr(\"uuid()\")) \\\n",
    "    .select(\"event_id\", \"event_type\", \"ts\", \"location_id\", \"order_id\", \"sequence\", \"body\")\n",
    "\n",
    "print(f\"‚úÖ Transformed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write & Update State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write\n",
    "final_df.write.mode(\"append\").json(VOLUME_PATH)\n",
    "print(f\"üíæ Wrote {event_count:,} events\")\n",
    "\n",
    "# Update watermark\n",
    "spark.createDataFrame([(str(new_end_seconds),)], [\"value\"]).write.mode(\"overwrite\").text(WATERMARK_PATH)\n",
    "print(f\"üìå Watermark: {new_end_seconds} (day {end_day})\")\n",
    "\n",
    "# Save sim start time (first run only)\n",
    "if is_first_run:\n",
    "    spark.createDataFrame([(sim_start_time.isoformat(),)], [\"value\"]).write.mode(\"overwrite\").text(SIM_START_PATH)\n",
    "    print(f\"‚è∞ Saved sim start time\")\n",
    "\n",
    "print(f\"\\n‚úÖ Complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
