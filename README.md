# ğŸ” Casper's Kitchens

Casperâ€™s Kitchens is a fully Databricks-native ghost kitchen and food-delivery platform built by the Developer Relations team. It brings together every layer of the Databricks platform â€” Lakeflow (ingestion, Spark Declarative Pipelines), AI & BI dashboards with Genie, Agent Bricks, and Apps powered by Lakebase (Postgres) â€” into a single, cohesive live demo.

Casperâ€™s is more than a showcase. Itâ€™s a living playground for simulation, demos, and creative misuse â€” designed to push the Databricks platform past its comfort zone.

Everything is built to be easy to:

1. ğŸš€ **Deploy** â€” spin up the entire environment in minutes.
2. ğŸ¬ **Demo** â€” run only the stages you need, powered by live streaming data.
3. ğŸ§‘â€ğŸ’» **Develop** â€” extend with new pipelines, agents, or apps effortlessly.

We build only with Databricks â€” by choice â€” so Casperâ€™s serves as a shared sandbox for learning, experimentation, and storytelling across the platform.

## Prerequisites

- Databricks CLI installed on your local machine.
- Authenticated to your Databricks workspace. (can do interactively `databricks auth login`)
- Access to the repository containing Casper's Kitchens.
- Permissions in the Databricks workspace to create new catalogs.

## ğŸš€ Deploy

Casperâ€™s Kitchens uses **Databricks Asset Bundles (DABs)** for one-command deployment.
Clone this repo, then run from the root directory:

```bash
databricks bundle deploy -t <target>
```

Each **target** represents a different flavor of Casperâ€™s (for example, full demo, complaints-only, free tier, etc.).
Use whichever fits your needs:

```bash
databricks bundle deploy -t default     # full version: Data generation, Lakeflow, Agents, Lakebase & Apps
databricks bundle deploy -t complaints  # complaints agent: Data generation, Lakeflow, Agents, Lakebase
databricks bundle deploy -t free        # Databricks Free Edition: Data generation, Lakeflow
```

This creates the main job **Casperâ€™s Initializer**, which orchestrates the full ecosystem, and places all assets in your workspace under
`/Workspace/Users/<your.email@databricks.com>/caspers-kitchens-demo`.

> ğŸ’¡ You can also deploy from the Databricks UI by cloning this repo as a [Git-based folder](https://docs.databricks.com/repos/) and clicking [Deploy Bundle](https://docs.databricks.com/aws/en/dev-tools/bundles/workspace-tutorial#deploy-the-bundle).

For more about how bundles and targets work, see [databricks.yml](./databricks.yml) or the [Databricks Bundles docs](https://docs.databricks.com/en/dev-tools/bundles/index.html).

## ğŸ¬ Run the Demo

![](./images/stages.gif)

Once deployed, run **any** target with the same command:

```bash
databricks bundle run caspers
```

Optionally, specify a catalog (default: `caspersdev`):

```bash
databricks bundle run caspers --params "CATALOG=mycatalog"
```

This spins up all the componentsâ€”data generator, pipelines, agents, and appsâ€”based on your selected target.

To clean up:

```bash
databricks bundle run cleanup (--params "CATALOG=mycatalog")
databricks bundle destroy
```

> ğŸ§© You can also run individual tasks or stages directly in the Databricks Jobs UI for finer control.

## ğŸ“Š Generated Event Types

The data generator produces the following realistic events for each order in the Volume `caspers.simulator.events`:

| Event | Description | Data Included |
|-------|-------------|---------------|
| `order_created` | Customer places order | Customer location (lat/lon), delivery address, ordered items with quantities |
| `gk_started` | Kitchen begins preparing food | Timestamp when prep begins |
| `gk_finished` | Kitchen completes food preparation | Timestamp when food is ready |
| `gk_ready` | Order ready for pickup | Timestamp when driver can collect |
| `driver_arrived` | Driver arrives at kitchen | Timestamp of driver arrival |
| `driver_picked_up` | Driver collects order | Full GPS route to customer, estimated delivery time |
| `driver_ping` | Driver location updates during delivery | Current GPS coordinates, delivery progress percentage |
| `delivered` | Order delivered to customer | Final delivery location coordinates |

Each event includes order ID, sequence number, timestamp, and location context. The system models realistic timing between events based on configurable service times, kitchen capacity, and real road network routing via OpenStreetMap data.

## ğŸ¯ Use Cases

- **ğŸ“š Learning Databricks**: Complete end-to-end platform experience
- **ğŸ“ Teaching**: Consistent narrative across different Databricks features  
- **ğŸ§ª CUJ Testing**: Run critical user journeys in realistic environment
- **ğŸ¨ UX Prototyping**: Fully loaded platform for design iteration
- **ğŸ¬ Demo Creation**: Unified narrative for new feature demonstrations

## License

Â© 2025 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License [https://databricks.com/db-license-source]. All included or referenced third party libraries are subject to the licenses set forth below.

| library                                | description             | license    | source                                              |
|----------------------------------------|-------------------------|------------|-----------------------------------------------------|
